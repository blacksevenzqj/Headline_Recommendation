{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"MyLearn\")\n",
    "sc = SparkContext(conf = conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helllo张无忌\n",
      "Helllo赵敏\n",
      "Helllo周芷若\n",
      "['Helllo张无忌', 'Helllo赵敏', 'Helllo周芷若']\n"
     ]
    }
   ],
   "source": [
    "# 1、map\n",
    "names = [\"张无忌\", \"赵敏\", \"周芷若\"]\n",
    "listRDD = sc.parallelize(names)\n",
    "\n",
    "temp = listRDD.map(lambda name : \"Helllo\" + name)\n",
    "\n",
    "temp.foreach(lambda strs : print(strs)) # foreach是在Executor中执行\n",
    "for strs in temp.collect():\n",
    "    print(strs)\n",
    "print(temp.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello张无忌\n",
      "Hello赵敏\n",
      "Hello周芷若\n",
      "['Hello张无忌', 'Hello赵敏', 'Hello周芷若']\n"
     ]
    }
   ],
   "source": [
    "# 2、flatMap：\n",
    "names = [\"张无忌\", \"赵敏\", \"周芷若\"]\n",
    "listRDD = sc.parallelize(names)\n",
    "\n",
    "# flatMap会自动将元组打开，元组中的每个元素添加进列表中；\n",
    "# temp = listRDD.flatMap(lambda name : (name, \"Hello\" + name)) \n",
    "\n",
    "temp = listRDD.flatMap(lambda name : [\"Hello\" + name]) # flatMap要求得到的列表类型\n",
    "\n",
    "temp.foreach(lambda strs : print(strs)) # foreach是在Executor中执行\n",
    "for strs in temp.collect():\n",
    "    print(strs)\n",
    "print(temp.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello张无忌\n",
      "Hello赵敏\n",
      "Hello宋青书\n",
      "Hello周芷若\n",
      "Hello刘德华\n",
      "Hello张学友\n",
      "['Hello张无忌', 'Hello赵敏', 'Hello宋青书', 'Hello周芷若', 'Hello刘德华', 'Hello张学友']\n"
     ]
    }
   ],
   "source": [
    "names = (\"张无忌 赵敏\", \"宋青书 周芷若\", \"刘德华\", \"张学友\")\n",
    "listRDD = sc.parallelize(names)\n",
    "\n",
    "temp = listRDD.flatMap(lambda name: name.split(\" \")).map(lambda name: \"Hello\" + name)\n",
    "\n",
    "temp.foreach(lambda strs : print(strs)) # foreach是在Executor中执行\n",
    "for strs in temp.collect():\n",
    "    print(strs)\n",
    "print(temp.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "张无忌 赵敏\n",
      "张学友\n",
      "['张无忌 赵敏', '张学友']\n"
     ]
    }
   ],
   "source": [
    "# 3、filter：\n",
    "names = (\"张无忌 赵敏\", \"宋青书 周芷若\", \"刘德华\", \"张学友\")\n",
    "listRDD = sc.parallelize(names)\n",
    "\n",
    "temp = listRDD.filter(lambda name: name.startswith(\"张\"))\n",
    "\n",
    "temp.foreach(lambda strs : print(strs)) # foreach是在Executor中执行\n",
    "for strs in temp.collect():\n",
    "    print(strs)\n",
    "print(temp.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('hello', <pyspark.resultiterable.ResultIterable object at 0x0000015BFEB201D0>), ('spark', <pyspark.resultiterable.ResultIterable object at 0x0000015BFEB20208>), ('world', <pyspark.resultiterable.ResultIterable object at 0x0000015BFEB208D0>)]\n",
      "[{'hello': [1, 1, 1]}, {'spark': [1]}, {'world': [1, 1]}]\n"
     ]
    }
   ],
   "source": [
    "# 4、groupByKey：\n",
    "data = [\"hello spark\", \"hello world\", \"hello world\"]\n",
    "rdd = sc.parallelize(data)\n",
    "mapRdd = rdd.flatMap(lambda line:line.split(\" \")).map(lambda x:(x,1))\n",
    "groupByRdd = mapRdd.groupByKey()\n",
    "print(groupByRdd.collect())\n",
    "print(groupByRdd.map(lambda x:{x[0]:list(x[1])}).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('hello', 3), ('spark', 1), ('world', 2)]\n"
     ]
    }
   ],
   "source": [
    "# 5、reduceByKey：\n",
    "data = [\"hello spark\", \"hello world\", \"hello world\"]\n",
    "rdd = sc.parallelize(data)\n",
    "mapRdd = rdd.flatMap(lambda line:line.split(\" \")).map(lambda x:(x,1))\n",
    "reduceByKeyRdd = mapRdd.reduceByKey(lambda a,b:a+b) # 意思是 相同key的value进行操作\n",
    "print(reduceByKeyRdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('world', 2), ('spark', 1), ('hello', 3)]\n",
      "[('hello', 3), ('world', 2), ('spark', 1)]\n"
     ]
    }
   ],
   "source": [
    "# 6、sortByKey：\n",
    "data = [\"hello spark\", \"hello world\", \"hello world\"]\n",
    "rdd = sc.parallelize(data)\n",
    "mapRDD = rdd.flatMap(lambda line: line.split(\" \")).map(lambda x: (x, 1))\n",
    "reduceByKeyRdd = mapRdd.reduceByKey(lambda a, b: a + b)\n",
    "print(reduceByKeyRdd.sortByKey(False).collect())\n",
    "print(reduceByKeyRdd.map(lambda x:(x[1],x[0])).sortByKey(False).map(lambda x:(x[1],x[0])).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 3, 4, 5]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = sc.parallelize([1,2,3])\n",
    "b = sc.parallelize([3,4,5])\n",
    "a.union(b).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4, 1, 3]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = sc.parallelize([1, 2, 3])\n",
    "b = sc.parallelize([3, 4, 2])\n",
    "a.union(b).distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('C', ('c1', 'c2')), ('C', ('c1', 'c3')), ('A', ('a1', 'a2')), ('D', ('d1', None)), ('F', ('f1', None)), ('F', ('f2', None)), ('E', (None, 'e1'))]\n"
     ]
    }
   ],
   "source": [
    "a = sc.parallelize([(\"A\", \"a1\"), (\"C\", \"c1\"), (\"D\", \"d1\"), (\"F\", \"f1\"), (\"F\", \"f2\")])\n",
    "b = sc.parallelize([(\"A\", \"a2\"), (\"C\", \"c2\"), (\"C\", \"c3\"), (\"E\", \"e1\")])\n",
    "print(a.fullOuterJoin(b).collect())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession \n",
    "spark = SparkSession.builder.appName('learn_ml').master('local[1]').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ml 模块 三个抽象类：\n",
    "转换器（Transformer）、评估器（Estimator）和管道（Pipeline）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pyspark.ml.feature.Binarizer(self, threshold=0.0, inputCol=None, outputCol=None)\n",
    "根据指定的阈值将连续变量转换为对应的二进制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([(0.5,),(1.0,),(1.5,)], ['values'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|values|\n",
      "+------+\n",
      "|   0.5|\n",
      "|   1.0|\n",
      "|   1.5|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "|values|features|\n",
      "+------+--------+\n",
      "|   0.5|     0.0|\n",
      "|   1.0|     1.0|\n",
      "|   1.5|     1.0|\n",
      "+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Binarizer\n",
    "binarizer = Binarizer(threshold=0.7, inputCol=\"values\", outputCol=\"features\")\n",
    "binarizer.transform(df).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|values|freqs|\n",
      "+------+-----+\n",
      "|   0.5|  0.0|\n",
      "|   1.0|  1.0|\n",
      "|   1.5|  1.0|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 通过setParams，更改配置\n",
    "binarizer.setParams(outputCol=\"freqs\").transform(df).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|values|vector|\n",
      "+------+------+\n",
      "|   0.5|   1.0|\n",
      "|   1.0|   1.0|\n",
      "|   1.5|   1.0|\n",
      "+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 通过params更改配置\n",
    "params = {binarizer.threshold: -0.5, binarizer.outputCol: \"vector\"}\n",
    "binarizer.transform(df, params).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存配置\n",
    "import os\n",
    "#temp_path = os.getcwd()\n",
    "temp_path = os.path.abspath('.')\n",
    "binarizerPath = \"file://{}/binarizer\".format(temp_path)\n",
    "binarizer.save(binarizerPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加载配置\n",
    "loadedBinarizer = Binarizer.load(binarizerPath)\n",
    "loadedBinarizer.getThreshold() == binarizer.getThreshold()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pyspark.ml.feature.Bucketizer(self, splits=None, inputCol=None, outputCol=None, handleInvalid=\"error\")\n",
    "与Binarizer类似，该方法根据阈值列表（分割的参数），将连续变量转换为多项值（连续变量离散化到指定的范围区间）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+\n",
      "|values|buckets|\n",
      "+------+-------+\n",
      "|   0.1|    0.0|\n",
      "|   0.4|    0.0|\n",
      "|   1.2|    1.0|\n",
      "|   1.5|    2.0|\n",
      "|   NaN|    3.0|\n",
      "|   NaN|    3.0|\n",
      "+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Bucketizer\n",
    "values = [(0.1,), (0.4,), (1.2,), (1.5,), (float(\"nan\"),), (float(\"nan\"),)]\n",
    "df = spark.createDataFrame(values, [\"values\"])\n",
    "# splits 为分类区间\n",
    "bucketizer = Bucketizer(splits=[-float(\"inf\"), 0.5, 1.4, float(\"inf\")],inputCol=\"values\", outputCol=\"buckets\")\n",
    "# 这里setHandleInvalid是对nan值进行处理，默认是error：有nan则报错；keep：将nan保留为新分类；skip：忽略nan值\n",
    "bucketed = bucketizer.setHandleInvalid(\"keep\").transform(df)\n",
    "bucketed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+\n",
      "|values|  b|\n",
      "+------+---+\n",
      "|   0.1|0.0|\n",
      "|   0.4|0.0|\n",
      "|   1.2|1.0|\n",
      "|   1.5|2.0|\n",
      "|   NaN|3.0|\n",
      "|   NaN|3.0|\n",
      "+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 更改配置\n",
    "bucketizer.setParams(outputCol=\"b\").transform(df).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pyspark.ml.feature.ChiSqSelector(self, numTopFeatures=50, featuresCol=\"features\", outputCol=None, labelCol=\"label\", selectorType=\"numTopFeatures\", percentile=0.1, fpr=0.05, fdr=0.05, fwe=0.05)\n",
    "对于分类目标变量（思考分类模型），此功能允许你选择预定义数量的特征（由numTopFeatures参数进行参数化），以便最好地说明目标的变化。该方法需要两部：需要.fit()——可以计算卡方检验，调用.fit()方法，将DataFrame作为参数传入返回一个ChiSqSelectorModel对象，然后可以使用该对象的.transform()方法来转换DataFrame。默认情况下，选择方法是numTopFeatures，默认顶级要素数设置为50。\n",
    "percentile 相识于num ，选取百分比的特征\n",
    "fpr 选择p-values低于阈值的所有特征，从而控制误差的选择率。\n",
    "fdr 使用  Benjamini-Hochberg procedure \n",
    "fwe 选择p-values低于阈值的所有特征。阈值按1 / numFeatures缩放"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----+----------------+\n",
      "|          features|label|selectedFeatures|\n",
      "+------------------+-----+----------------+\n",
      "|[0.0,0.0,18.0,1.0]|  1.0|      [18.0,1.0]|\n",
      "|[0.0,1.0,12.0,0.0]|  0.0|      [12.0,0.0]|\n",
      "|[1.0,0.0,15.0,0.1]|  0.0|      [15.0,0.1]|\n",
      "+------------------+-----+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import ChiSqSelector\n",
    "df = spark.createDataFrame(\n",
    "[(Vectors.dense([0.0, 0.0, 18.0, 1.0]), 1.0),\n",
    "(Vectors.dense([0.0, 1.0, 12.0, 0.0]), 0.0),\n",
    "(Vectors.dense([1.0, 0.0, 15.0, 0.1]), 0.0)],\n",
    "[\"features\", \"label\"])\n",
    "selector = ChiSqSelector(numTopFeatures=2, outputCol=\"selectedFeatures\")\n",
    "model = selector.fit(df)\n",
    "model.transform(df).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pyspark.ml.feature.CountVectorizer(self, minTF=1.0, minDF=1.0, vocabSize=1 << 18, binary=False, inputCol=None, outputCol=None)\n",
    "从文档集合中提取词汇表并生成向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------+-------------------------+\n",
      "|label|raw            |vectors                  |\n",
      "+-----+---------------+-------------------------+\n",
      "|0    |[a, b, c]      |(3,[0,1,2],[1.0,1.0,1.0])|\n",
      "|1    |[a, b, b, c, a]|(3,[0,1,2],[2.0,2.0,1.0])|\n",
      "+-----+---------------+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "df = spark.createDataFrame(\n",
    "[(0, [\"a\", \"b\", \"c\"]), (1, [\"a\", \"b\", \"b\", \"c\", \"a\"])],\n",
    "[\"label\", \"raw\"])\n",
    "cv = CountVectorizer(inputCol=\"raw\", outputCol=\"vectors\")\n",
    "model = cv.fit(df)\n",
    "model.transform(df).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'b', 'c']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(model.vocabulary) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存model\n",
    "import os\n",
    "#temp_path = os.getcwd()\n",
    "temp_path = os.path.abspath('.')\n",
    "modelPath = \"file://{}/count-vectorizer-model\".format(temp_path)\n",
    "model.save(modelPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加载model\n",
    "from pyspark.ml.feature import CountVectorizerModel\n",
    "loadedModel = CountVectorizerModel.load(modelPath)\n",
    "loadedModel.vocabulary == model.vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pyspark.ml.feature.ElementwiseProduct(scalingVec=None, inputCol=None, outputCol=None)\n",
    "使用提供的“权重”向量输出每个输入向量的阿达马乘积（即，逐元素乘积）。换句话说，它通过标量乘数缩放数据集的每一列。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+\n",
      "|       values|        eprod|\n",
      "+-------------+-------------+\n",
      "|[2.0,1.0,3.0]|[2.0,2.0,9.0]|\n",
      "+-------------+-------------+\n",
      "\n",
      "+-------------+--------------+\n",
      "|       values|         eprod|\n",
      "+-------------+--------------+\n",
      "|[2.0,1.0,3.0]|[4.0,3.0,15.0]|\n",
      "+-------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import ElementwiseProduct \n",
    "from pyspark.ml.linalg import Vectors\n",
    "df = spark.createDataFrame([(Vectors.dense([2.0, 1.0, 3.0]),)], [\"values\"])\n",
    "ep = ElementwiseProduct(scalingVec=Vectors.dense([1.0, 2.0, 3.0]),\n",
    "inputCol=\"values\", outputCol=\"eprod\")\n",
    "ep.transform(df).show()\n",
    "ep.setParams(scalingVec=Vectors.dense([2.0, 3.0, 5.0])).transform(df).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pyspark.ml.feature.Imputer(*args, **kwargs)\n",
    "用于完成缺失值的插补估计器，使用缺失值所在列的平均值或中值。 输入列应该是DoubleType或FloatType。 目前的Imputer不支持分类特征，可能会为分类特征创建不正确的值。\n",
    "请注意，平均值/中值是在过滤出缺失值之后计算的。 输入列中的所有Null值都被视为缺失，所以也被归类。 为了计算中位数，使用pyspark.sql.DataFrame.approxQuantile（），相对误差为0.001。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|  a|  b|\n",
      "+---+---+\n",
      "|1.0|NaN|\n",
      "|2.0|NaN|\n",
      "|NaN|3.0|\n",
      "|4.0|4.0|\n",
      "|5.0|5.0|\n",
      "+---+---+\n",
      "\n",
      "+---+---+\n",
      "|  a|  b|\n",
      "+---+---+\n",
      "|3.0|4.0|\n",
      "+---+---+\n",
      "\n",
      "+---+---+-----+-----+\n",
      "|  a|  b|out_a|out_b|\n",
      "+---+---+-----+-----+\n",
      "|1.0|NaN|  1.0|  4.0|\n",
      "|2.0|NaN|  2.0|  4.0|\n",
      "|NaN|3.0|  3.0|  3.0|\n",
      "|4.0|4.0|  4.0|  4.0|\n",
      "|5.0|5.0|  5.0|  5.0|\n",
      "+---+---+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Imputer\n",
    "df = spark.createDataFrame([(1.0, float(\"nan\")), (2.0, float(\"nan\")), (float(\"nan\"), 3.0),\n",
    "                             (4.0, 4.0), (5.0, 5.0)], [\"a\", \"b\"])\n",
    "imputer = Imputer(inputCols=[\"a\", \"b\"], outputCols=[\"out_a\", \"out_b\"])\n",
    "model = imputer.fit(df)\n",
    "df.show()\n",
    "model.surrogateDF.show()\n",
    "model.transform(df).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----+-----+\n",
      "|  a|  b|out_a|out_b|\n",
      "+---+---+-----+-----+\n",
      "|1.0|NaN|  1.0|  4.0|\n",
      "|2.0|NaN|  2.0|  4.0|\n",
      "|NaN|3.0|  2.0|  3.0|\n",
      "|4.0|4.0|  4.0|  4.0|\n",
      "|5.0|5.0|  5.0|  5.0|\n",
      "+---+---+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "imputer.setStrategy(\"median\").setMissingValue(float(\"nan\")).fit(df).transform(df).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pyspark.ml.feature.MaxAbsScaler(self, inputCol=None, outputCol=None)\n",
    "通过分割每个特征中的最大绝对值来单独重新缩放每个特征以范围[-1,1]。 它不会移动/居中数据，因此不会破坏任何稀疏性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "|    a|scaled|\n",
      "+-----+------+\n",
      "|[1.0]| [0.5]|\n",
      "|[2.0]| [1.0]|\n",
      "+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import MaxAbsScaler\n",
    "from pyspark.ml.linalg import Vectors\n",
    "df = spark.createDataFrame([(Vectors.dense([1.0]),), (Vectors.dense([2.0]),)], [\"a\"])\n",
    "maScaler = MaxAbsScaler(inputCol=\"a\", outputCol=\"scaled\")\n",
    "model = maScaler.fit(df)\n",
    "model.transform(df).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pyspark.ml.feature.MinMaxScaler(self, min=0.0, max=1.0, inputCol=None, outputCol=None)\n",
    "使用列汇总统计信息，将每个特征单独重新标定为一个常用范围[min，max]，这也称为最小 - 最大标准化或重新标定（注意由于零值可能会被转换为非零值，因此即使对于稀疏输入，转换器的输出也将是DenseVector）。 特征E的重新缩放的值被计算为，数据将被缩放到[0.0,1.0]范围内。\n",
    "Rescaled(e_i) = (e_i - E_min) / (E_max - E_min) * (max - min) + min\n",
    "For the case E_max == E_min, Rescaled(e_i) = 0.5 * (max + min)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0] [2.0]\n",
      "+-----+------+\n",
      "|    a|scaled|\n",
      "+-----+------+\n",
      "|[0.0]| [0.0]|\n",
      "|[2.0]| [1.0]|\n",
      "+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.ml.linalg import Vectors\n",
    "df = spark.createDataFrame([(Vectors.dense([0.0]),), (Vectors.dense([2.0]),)], [\"a\"])\n",
    "mmScaler = MinMaxScaler(inputCol=\"a\", outputCol=\"scaled\")\n",
    "model = mmScaler.fit(df)\n",
    "print(model.originalMin, model.originalMax)\n",
    "model.transform(df).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pyspark.ml.feature.NGram(n=2, inputCol=None, outputCol=None)\n",
    "一种功能转换器，用于将输入的字符串数组转换为n-gram数组。输入数组中的空值将被忽略。它返回一个n-gram数组，其中每个n-gram由一个以空格分隔的单词串表示。当输入为空时，返回一个空数组。当输入数组长度小于n（每n-gram的元素数）时，不返回n-gram。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------------+\n",
      "|    inputTokens|              nGrams|\n",
      "+---------------+--------------------+\n",
      "|[a, b, c, d, e]|[a b, b c, c d, d e]|\n",
      "+---------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import NGram\n",
    "from pyspark.sql import Row\n",
    "df = spark.createDataFrame([Row(inputTokens=[\"a\", \"b\", \"c\", \"d\", \"e\"])])\n",
    "ngram = NGram(n=2, inputCol=\"inputTokens\", outputCol=\"nGrams\")\n",
    "ngram.transform(df).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------------+\n",
      "|    inputTokens|            nGrams|\n",
      "+---------------+------------------+\n",
      "|[a, b, c, d, e]|[a b c d, b c d e]|\n",
      "+---------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 更改 n-gram 长度\n",
    "ngram.setParams(n=4).transform(df).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------------+\n",
      "|    inputTokens|            output|\n",
      "+---------------+------------------+\n",
      "|[a, b, c, d, e]|[a b c d, b c d e]|\n",
      "+---------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 临时修改输出列\n",
    "ngram.transform(df, {ngram.outputCol: \"output\"}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pyspark.ml.feature.Normalizer(self, p=2.0, inputCol=None, outputCol=None)\n",
    "使用给定的p范数标准化矢量以得到单位范数（默认为L2）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+----------+\n",
      "|     dense|             sparse|  features|\n",
      "+----------+-------------------+----------+\n",
      "|[3.0,-4.0]|(4,[1,3],[4.0,3.0])|[0.6,-0.8]|\n",
      "+----------+-------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Normalizer\n",
    "from pyspark.ml.linalg import Vectors\n",
    "svec = Vectors.sparse(4, {1: 4.0, 3: 3.0})\n",
    "df = spark.createDataFrame([(Vectors.dense([3.0, -4.0]), svec)], [\"dense\", \"sparse\"])\n",
    "normalizer = Normalizer(p=2.0, inputCol=\"dense\", outputCol=\"features\")\n",
    "normalizer.transform(df).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+-------------------+\n",
      "|     dense|             sparse|              freqs|\n",
      "+----------+-------------------+-------------------+\n",
      "|[3.0,-4.0]|(4,[1,3],[4.0,3.0])|(4,[1,3],[0.8,0.6])|\n",
      "+----------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "normalizer.setParams(inputCol=\"sparse\", outputCol=\"freqs\").transform(df).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pyspark.ml.feature.OneHotEncoderEstimator(inputCols=None, outputCols=None, handleInvalid='error', dropLast=True)\n",
    "(分类列编码为二进制向量列)\n",
    "一个热门的编码器，将一列类别索引映射到一列二进制向量，每行至多有一个单值，表示输入类别索引。 例如，对于5个类别，输入值2.0将映射到[0.0，0.0，1.0，0.0]的输出向量。 最后一个类别默认不包含（可通过dropLast进行配置），因为它使向量条目总和为1，因此线性相关。 所以一个4.0的输入值映射到[0.0，0.0，0.0，0.0]。这与scikit-learn的OneHotEncoder不同，后者保留所有类别。 输出向量是稀疏的。\n",
    "当handleInvalid配置为“keep”时，会添加一个指示无效值的额外“类别”作为最后一个类别。因此，当dropLast为true时，无效值将被编码为全零向量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+\n",
      "|input|       output|\n",
      "+-----+-------------+\n",
      "|  0.0|(2,[0],[1.0])|\n",
      "|  1.0|(2,[1],[1.0])|\n",
      "|  2.0|    (2,[],[])|\n",
      "+-----+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import OneHotEncoderEstimator\n",
    "from pyspark.ml.linalg import Vectors\n",
    "df = spark.createDataFrame([(0.0,), (1.0,), (2.0,)], [\"input\"])\n",
    "ohe = OneHotEncoderEstimator(inputCols=[\"input\"], outputCols=[\"output\"])\n",
    "model = ohe.fit(df)\n",
    "model.transform(df).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pyspark.ml.feature.PCA(self, k=None, inputCol=None, outputCol=None)\n",
    "PCA训练一个模型将向量投影到前k个主成分的较低维空间。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+----------------------------------------+\n",
      "|features             |pca_features                            |\n",
      "+---------------------+----------------------------------------+\n",
      "|(5,[1,3],[1.0,7.0])  |[1.6485728230883807,-4.013282700516296] |\n",
      "|[2.0,0.0,3.0,4.0,5.0]|[-4.645104331781534,-1.1167972663619026]|\n",
      "|[4.0,0.0,0.0,6.0,7.0]|[-6.428880535676489,-5.337951427775355] |\n",
      "+---------------------+----------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import PCA\n",
    "from pyspark.ml.linalg import Vectors\n",
    "data = [(Vectors.sparse(5, [(1, 1.0), (3, 7.0)]),),\n",
    "     (Vectors.dense([2.0, 0.0, 3.0, 4.0, 5.0]),),\n",
    "     (Vectors.dense([4.0, 0.0, 0.0, 6.0, 7.0]),)]\n",
    "df = spark.createDataFrame(data,[\"features\"])\n",
    "pca = PCA(k=2, inputCol=\"features\", outputCol=\"pca_features\")\n",
    "model = pca.fit(df)\n",
    "model.transform(df).show(truncate=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([0.7944, 0.2056])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.explainedVariance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pyspark.ml.feature.QuantileDiscretizer(self, numBuckets=2, inputCol=None, outputCol=None, relativeError=0.001, handleInvalid=\"error\")\n",
    "与Bucketizer方法类似，但QuantileDiscretizer采用具有连续特征的列，并输出具有分箱分类特征的列。可以使用numBuckets参数设置区域的数量。所使用的桶的数量可能小于该值，例如，如果输入的不同值太少而不能创建足够的不同分位数。nan会占用一个新的分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+\n",
      "|values|buckets|\n",
      "+------+-------+\n",
      "|   0.1|    0.0|\n",
      "|   0.4|    1.0|\n",
      "|   1.2|    1.0|\n",
      "|   1.5|    1.0|\n",
      "|   NaN|    2.0|\n",
      "|   NaN|    2.0|\n",
      "+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import QuantileDiscretizer\n",
    "values = [(0.1,), (0.4,), (1.2,), (1.5,), (float(\"nan\"),), (float(\"nan\"),)]\n",
    "df = spark.createDataFrame(values, [\"values\"])\n",
    "qds = QuantileDiscretizer(numBuckets=2,\n",
    "     inputCol=\"values\", outputCol=\"buckets\", relativeError=0.01, handleInvalid=\"error\")\n",
    "bucketizer = qds.fit(df)\n",
    "qds.setHandleInvalid(\"keep\").fit(df).transform(df).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pyspark.ml.feature.RegexTokenizer(minTokenLength=1, gaps=True, pattern='\\s+', inputCol=None, outputCol=None, toLowercase=True)\n",
    "基于java正则表达式的标记生成器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+\n",
      "|  text|    words|\n",
      "+------+---------+\n",
      "|A B  c|[a, b, c]|\n",
      "+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer\n",
    "df = spark.createDataFrame([(\"A B  c\",)], [\"text\"])\n",
    "reTokenizer = RegexTokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "reTokenizer.transform(df).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  pyspark.ml.feature.SQLTransformer(statement=None)\n",
    "实现由SQL语句定义的转换。目前我们只支持SQL语法，"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+\n",
      "| id| v1| v2|\n",
      "+---+---+---+\n",
      "|  0|1.0|3.0|\n",
      "|  2|2.0|5.0|\n",
      "+---+---+---+\n",
      "\n",
      "+---+---+---+---+----+\n",
      "| id| v1| v2| v3|  v4|\n",
      "+---+---+---+---+----+\n",
      "|  0|1.0|3.0|4.0| 3.0|\n",
      "|  2|2.0|5.0|7.0|10.0|\n",
      "+---+---+---+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import SQLTransformer\n",
    "df = spark.createDataFrame([(0, 1.0, 3.0), (2, 2.0, 5.0)], [\"id\", \"v1\", \"v2\"])\n",
    "sqlTrans = SQLTransformer(\n",
    "     statement=\"SELECT *, (v1 + v2) AS v3, (v1 * v2) AS v4 FROM __THIS__\")\n",
    "df.show()\n",
    "sqlTrans.transform(df).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pyspark.ml.feature.StandardScaler(self, withMean=False, withStd=True, inputCol=None, outputCol=None)\n",
    "(标准化列，使其拥有零均值和等于1的标准差)\n",
    "通过使用训练集中样本的列汇总统计消除平均值和缩放到单位方差来标准化特征。使用校正后的样本标准偏差计算“单位标准差”，该标准偏差计算为无偏样本方差的平方根。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0] [1.4142135623730951]\n",
      "+-----+-------------------+\n",
      "|    a|             scaled|\n",
      "+-----+-------------------+\n",
      "|[0.0]|              [0.0]|\n",
      "|[2.0]|[1.414213562373095]|\n",
      "+-----+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.linalg import Vectors\n",
    "df = spark.createDataFrame([(Vectors.dense([0.0]),), (Vectors.dense([2.0]),)], [\"a\"])\n",
    "standardScaler = StandardScaler(inputCol=\"a\", outputCol=\"scaled\")\n",
    "model = standardScaler.fit(df)\n",
    "print(model.mean, model.std)\n",
    "model.transform(df).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pyspark.ml.feature.StopWordsRemover(inputCol=None, outputCol=None, stopWords=None, caseSensitive=False)\n",
    "一个特征转换器，用于过滤掉输入中的停用词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+\n",
      "|     text| words|\n",
      "+---------+------+\n",
      "|[a, b, c]|[a, c]|\n",
      "+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "df = spark.createDataFrame([([\"a\", \"b\", \"c\"],)], [\"text\"])\n",
    "remover = StopWordsRemover(inputCol=\"text\", outputCol=\"words\", stopWords=[\"b\"])\n",
    "remover.transform(df).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pyspark.ml.feature.Tokenizer(inputCol=None, outputCol=None)\n",
    "一个标记生成器，它将输入字符串转换为小写，然后用空格分隔它。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+\n",
      "|    text|       words|\n",
      "+--------+------------+\n",
      "|ASD VA c|[asd, va, c]|\n",
      "+--------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Tokenizer\n",
    "df = spark.createDataFrame([(\"ASD VA c\",)], [\"text\"])\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "tokenizer.transform(df).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pyspark.ml.feature.VectorSlicer(inputCol=None, outputCol=None, indices=None, names=None)\n",
    "此类采用特征向量并输出具有原始特征的子阵列的新特征向量。 可以使用索引（setIndices（））或名称（setNames（））指定要素子集。必须至少选择一个功能。不允许使用重复的功能，因此所选索引和名称之间不能重叠。 输出向量将首先按所选索引（按给定顺序）排序要素，然后是所选名称（按给定顺序）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+----------+\n",
      "|features               |sliced    |\n",
      "+-----------------------+----------+\n",
      "|[-2.0,2.3,0.0,0.0,1.0] |[2.3,1.0] |\n",
      "|[0.0,0.0,0.0,0.0,0.0]  |[0.0,0.0] |\n",
      "|[0.6,-1.1,-3.0,4.5,3.3]|[-1.1,3.3]|\n",
      "+-----------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorSlicer\n",
    "from pyspark.ml.linalg import Vectors\n",
    "df = spark.createDataFrame([\n",
    "     (Vectors.dense([-2.0, 2.3, 0.0, 0.0, 1.0]),),\n",
    "     (Vectors.dense([0.0, 0.0, 0.0, 0.0, 0.0]),),\n",
    "     (Vectors.dense([0.6, -1.1, -3.0, 4.5, 3.3]),)], [\"features\"])\n",
    "vs = VectorSlicer(inputCol=\"features\", outputCol=\"sliced\", indices=[1, 4])\n",
    "vs.transform(df).show(truncate=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pyspark.ml.feature.VectorAssembler(inputCols=None, outputCol=None)\n",
    "将多个列合并到向量列中的要素转换器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+-------------+\n",
      "|  a|  b|  c|     features|\n",
      "+---+---+---+-------------+\n",
      "|  1|  0|  3|[1.0,0.0,3.0]|\n",
      "+---+---+---+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "df = spark.createDataFrame([(1, 0, 3)], [\"a\", \"b\", \"c\"])\n",
    "vecAssembler = VectorAssembler(inputCols=[\"a\", \"b\", \"c\"], outputCol=\"features\")\n",
    "vecAssembler.transform(df).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pyspark.ml.feature.Word2Vec(vectorSize=100, minCount=5, numPartitions=1, stepSize=0.025, maxIter=1, seed=None, inputCol=None, outputCol=None, windowSize=5, maxSentenceLength=1000)\n",
    "Word2Vec训练Map（String，Vector）模型，即将单词转换为代码以进行进一步的自然语言处理或机器学习过程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+\n",
      "|word|              vector|\n",
      "+----+--------------------+\n",
      "|   a|[0.09461779892444...|\n",
      "|   b|[1.15474212169647...|\n",
      "|   c|[-0.3794820010662...|\n",
      "+----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Word2Vec\n",
    "sent = (\"a b \" * 100 + \"a c \" * 10).split(\" \")\n",
    "doc = spark.createDataFrame([(sent,), (sent,)], [\"sentence\"])\n",
    "word2Vec = Word2Vec(vectorSize=5, seed=42, inputCol=\"sentence\", outputCol=\"model\")\n",
    "model = word2Vec.fit(doc)\n",
    "model.getVectors().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------------+\n",
      "|word|         similarity|\n",
      "+----+-------------------+\n",
      "|   b|0.25053444504737854|\n",
      "+----+-------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('b', 0.25053444504737854)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 找相似字符\n",
    "model.findSynonyms(\"a\", 1).show()\n",
    "model.findSynonymsArray(\"a\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+\n",
      "|word|similarity|\n",
      "+----+----------+\n",
      "|   b|     0.251|\n",
      "|   c|    -0.698|\n",
      "+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import format_number as fmt\n",
    "model.findSynonyms(\"a\", 2).select(\"word\", fmt(\"similarity\", 3).alias(\"similarity\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

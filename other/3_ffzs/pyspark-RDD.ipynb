{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RDD（Resilient Distributed Dataset）叫做弹性分布式数据集，是Spark中最基本的数据抽象，它代表一个不可变、可分区、里面的元素可并行计算的集合。RDD具有数据流模型的特点：自动容错、位置感知性调度和可伸缩性。RDD允许用户在执行多个查询时显式地将工作集缓存在内存中，后续的查询能够重用工作集，这极大地提升了查询速度。\n",
    "\n",
    "（1）一组分片（Partition），即数据集的基本组成单位。对于RDD来说，每个分片都会被一个计算任务处理，并决定并行计算的粒度。用户可以在创建RDD时指定RDD的分片个数，如果没有指定，那么就会采用默认值。默认值就是程序所分配到的CPU Core的数目。\n",
    "\n",
    "（2）一个计算每个分区的函数。Spark中RDD的计算是以分片为单位的，每个RDD都会实现compute函数以达到这个目的。compute函数会对迭代器进行复合，不需要保存每次计算的结果。\n",
    "\n",
    "（3）RDD之间的依赖关系。RDD的每次转换都会生成一个新的RDD，所以RDD之间就会形成类似于流水线一样的前后依赖关系。在部分分区数据丢失时，Spark可以通过这个依赖关系重新计算丢失的分区数据，而不是对RDD的所有分区进行重新计算。\n",
    "\n",
    "（4）一个Partitioner，即RDD的分片函数。当前Spark中实现了两种类型的分片函数，一个是基于哈希的HashPartitioner，另外一个是基于范围的RangePartitioner。只有对于于key-value的RDD，才会有Partitioner，非key-value的RDD的Parititioner的值是None。Partitioner函数不但决定了RDD本身的分片数量，也决定了parent RDD Shuffle输出时的分片数量。\n",
    "\n",
    "（5）一个列表，存储存取每个Partition的优先位置（preferred location）。对于一个HDFS文件来说，这个列表保存的就是每个Partition所在的块的位置。按照“移动数据不如移动计算”的理念，Spark在进行任务调度的时候，会尽可能地将计算任务分配到其所要处理数据块的存储位置。\n",
    "\n",
    "使用手册 \n",
    "http://spark.apache.org/docs/latest/api/python/pyspark.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pyspark.SparkContext()是spark应用的入口，也可以称为驱动\n",
    "from pyspark import SparkConf, SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().setAppName(\"sparkApp1\").setMaster(\"local\")\n",
    "sc = SparkContext.getOrCreate(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2, 3, 4, 6]\n",
      "[[0], [2], [3], [4], [6]]\n",
      "[0, 2, 4]\n",
      "[[], [0], [], [2], [4]]\n"
     ]
    }
   ],
   "source": [
    "#parallelize（c，numSlices=None）分发本地Python集合以形成RDD。如果输入表示性能范围，则建议使用xrange。\n",
    "#glom（）通过将每个分区内的所有元素合并到一个列表中返回一个RDD。\n",
    "rdd1 = sc.parallelize([0,2,3,4,6], 5)\n",
    "rdd2 = sc.parallelize(range(0, 6, 2), 5)\n",
    "print(rdd1.collect())\n",
    "print(rdd1.glom().collect())\n",
    "print(rdd2.collect())\n",
    "print(rdd2.glom().collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 9]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#runJob(rdd, partitionFunc, partitions=None, allowLocal=False）\n",
    "#在指定的分区集合上执行给定的分区，将结果作为元素的数组返回。如果没有指定分区，那么它将在所有分区上运行。\n",
    "sc.runJob(rdd1, lambda part: [x * x for x in part], [0, 2], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1528077753028\n",
      "ffzs\n"
     ]
    }
   ],
   "source": [
    "print(sc.startTime)\n",
    "print(sc.sparkUser())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rdd.glom()\n",
    "# glom()定义了将原rdd相同分区的元素放在一个列表中构成新的rdd的转换操作。\n",
    "# rdd.collect()\n",
    "# 返回由rdd元素组成的列表\n",
    "# rdd.collectAsMap()\n",
    "# 将键值对形式的RDD以字典的形式返回给master "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cache()\n",
    "# 将RDD持久化为MEMORY_ONLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 'aa', 1), ('b', 'bb', 1), ('c', 'cc', 1)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# map(f, preservesPartitioning=False)\n",
    "# 通过对这个RDD的每个元素应用一个函数来返回一个新的RDD\n",
    "rdd = sc.parallelize([\"b\", \"a\", \"c\"])\n",
    "sorted(rdd.map(lambda x:(x, x*2, 1)).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 2, 2, 3]\n",
      "[[(2, 2), (2, 2)], [(3, 3), (3, 3)], [(4, 4), (4, 4)]]\n",
      "[(2, 2), (2, 2), (3, 3), (3, 3), (4, 4), (4, 4)]\n"
     ]
    }
   ],
   "source": [
    "#flatMap(f, preservesPartitioning=False)\n",
    "#首先将一个函数应用到这个RDD的所有元素上，然后将结果全部展开，返回一个新的RDD\n",
    "rdd = sc.parallelize([2, 3, 4])\n",
    "print(sorted(rdd.flatMap(lambda x: range(1, x)).collect()))\n",
    "print(sorted(rdd.map(lambda x: [(x, x), (x, x)]).collect()))\n",
    "print(sorted(rdd.flatMap(lambda x: [(x, x), (x, x)]).collect()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 3), ('b', 1)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mapValues(f)\n",
    "#通过map函数对RDD中的每个key传递value，而不改变键;同时保留了原始的RDD分区。\n",
    "x = sc.parallelize([(\"a\", [\"apple\", \"banana\", \"lemon\"]), (\"b\", [\"grapes\"])])\n",
    "def f(x): return len(x)\n",
    "x.mapValues(f).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', 'x'), ('a', 'y'), ('a', 'z'), ('b', 'p'), ('b', 'r')]\n",
      "[('a', ['x', 'y', 'z']), ('b', ['p', 'r'])]\n"
     ]
    }
   ],
   "source": [
    "#flatMapValues(f)\n",
    "#通过flatMap函数传递键值对RDD中的每个值，而不改变键;这也保留了原始的RDD分区。\n",
    "x = sc.parallelize([(\"a\", [\"x\", \"y\", \"z\"]), (\"b\", [\"p\", \"r\"])])\n",
    "def f(x): return x\n",
    "print(x.flatMapValues(f).collect())\n",
    "print(x.mapValues(f).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 7]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mapPartitions(f, preservesPartitioning=False)\n",
    "# 与map不同，map是对每一个元素用函数作用；而mapPartitions是对每一个分区用一个函数去作用，每一个分区的元素先构成一个迭代器iterator，iterator是一个像列表，但里面的元素又保持分布式特点的一类对象;输入的参数就是这个iterator，然后对iterator进行运算，iterator支持的函数不是太多，sum,count等一些spark定义的基本函数应该都是支持的。但如果要进行更为复杂的一些个性化函数运算，可以就用不了。实践中发生可以通过[x for i in iterator]的方式，将iterator转换为列表，然后就可以进行各种操作。但是这样在分区内部或分组内部就失去了分布式运算的特点。\n",
    "# yield是生成的意思，但是在python中则是作为生成器理解，生成器的用处主要可以迭代，这样简化了很多运算模型。\n",
    "rdd = sc.parallelize([1, 2, 3, 4], 2)\n",
    "def f(iterator): yield sum(iterator)\n",
    "rdd.mapPartitions(f).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mapPartitionsWithIndex(f, preservesPartitioning=False)\n",
    "# 通过在这个RDD的每个分区上应用一个函数来返回一个新的RDD，同时跟踪原始分区的索引。为对索引进行操作提供可能\n",
    "rdd = sc.parallelize([1, 2, 3, 4], 4)\n",
    "def f(splitIndex, iterator): yield splitIndex\n",
    "rdd.mapPartitionsWithIndex(f).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# partitionBy(numPartitions, partitionFunc=<function portable_hash>)\n",
    "# 返回使用指定的分区器分区的RDD的副本\n",
    "# set().intersection 取交集\n",
    "pairs = sc.parallelize([1, 2, 3, 4, 2, 4, 1]).map(lambda x: (x, x))\n",
    "sets = pairs.partitionBy(2).glom().collect()\n",
    "len(set(sets[0]).intersection(set(sets[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1], [2, 3], [4, 5]]\n",
      "[[], [1], [4, 5], [2, 3], []]\n"
     ]
    }
   ],
   "source": [
    "# coalesce(numPartitions, shuffle=False)\n",
    "# 返回一个新的RDD，将RDD重新分区,减少分区不适用shuffle ，正加分区数的话要shuffle为true 同repartition\n",
    "print(sc.parallelize([1, 2, 3, 4, 5], 3).glom().collect())\n",
    "print(sc.parallelize([1, 2, 3, 4, 5], 3).coalesce(5,True).glom().collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "# repartition(numPartitions)\n",
    "# 重新分区，默认shuffle 减少分区用coalesce\n",
    "rdd = sc.parallelize([1,2,3,4,5,6,7], 4)\n",
    "print(len(rdd.repartition(2).glom().collect()))\n",
    "print(len(rdd.repartition(10).glom().collect()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1000), (1, 1001), (2, 1002), (3, 1003), (4, 1004)]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# zip(other)\n",
    "# 一个RDD作为key，另一个让RDD作为value\n",
    "x = sc.parallelize(range(0,5))\n",
    "y = sc.parallelize(range(1000, 1005))\n",
    "x.zip(y).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 0), ('b', 1), ('c', 2), ('d', 3)]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rdd.zipWithIndex()\n",
    "# RDD为key 排序位置索引作为value\n",
    "sc.parallelize([\"a\", \"b\", \"c\", \"d\"], 2).zipWithIndex().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'b', 'c', 'd', 'e']\n",
      "[['a'], ['b', 'c'], ['d', 'e']]\n",
      "[('a', 0), ('b', 1), ('c', 4), ('d', 2), ('e', 5)]\n"
     ]
    }
   ],
   "source": [
    "# zipWithUniqueId(）\n",
    "# 根据分区k 按公式k，n+k，2*n+k产生value，RDD为key\n",
    "rdd = sc.parallelize([\"a\", \"b\", \"c\", \"d\", \"e\"], 3)\n",
    "print(rdd.collect())\n",
    "print(rdd.glom().collect())\n",
    "print(rdd.zipWithUniqueId().collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0), (1, 1), (4, 2)]\n",
      "[(0, 0), (1, 1), (2, 2), (3, 3), (4, 4)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0, [[0], [0]]),\n",
       " (1, [[1], [1]]),\n",
       " (2, [[], [2]]),\n",
       " (3, [[], [3]]),\n",
       " (4, [[2], [4]])]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rdd.keyBy()\n",
    "# RDD通过函数创建元组\n",
    "x = sc.parallelize(range(0,3)).keyBy(lambda x: x*x)\n",
    "y = sc.parallelize(zip(range(0,5), range(0,5)))\n",
    "print(x.collect())\n",
    "print(y.collect())\n",
    "[(x, list(map(list, y))) for x, y in sorted(x.cogroup(y).collect())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# foreach(f)\n",
    "# 是一个公式作用于rdd所有元素，生成非rdd\n",
    "def fun(x): \n",
    "    print(x)\n",
    "sc.parallelize([1, 2, 3, 4, 5]).foreach(fun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# foreachPartition(f)\n",
    "# 使一个函数作用于RDD上每一个分区\n",
    "def fun(iterator):\n",
    "    for x in iterator:\n",
    "        print(x)\n",
    "sc.parallelize([1, 2, 3, 4, 5]).foreachPartition(fun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "1\n",
      "2\n",
      "3\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n1\\n2\\n3\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputData=sc.parallelize([1,2,3])\n",
    "def f(x):#定义一个将内容追加于文件末尾的函数\n",
    "    with open('./example.txt','a+') as fl:\n",
    "        print(x,file=fl)\n",
    "\n",
    "open('./example.txt','w').close()#操作之前先关闭之前可能存在的对该文件的写操作\n",
    "y=inputData.foreach(f)\n",
    "print(y)\n",
    "#结果为：None,因为函数f没有返回值\n",
    "#查看写文件的结果\n",
    "with open('./example.txt') as fl:\n",
    "    print(fl.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, [2, 8]), (1, [1, 1, 3, 5])]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# groupBy(f, numPartitions=None, partitionFunc=<function portable_hash>）\n",
    "# 根据函数符合条件与否进行分组返回分组项目的RDD\n",
    "rdd = sc.parallelize([1, 1, 2, 3, 5, 8])\n",
    "result = rdd.groupBy(lambda x: x % 2).collect()\n",
    "sorted([(x, sorted(y)) for (x, y) in result])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', 2), ('b', 1)]\n",
      "[('a', [1, 1]), ('b', [1])]\n"
     ]
    }
   ],
   "source": [
    "#groupByKey(numPartitions=None, partitionFunc=<function portable_hash>)\n",
    "#原rdd为键值对，groupByKey()则将原rdd的元素相同键的值编进一个sequence\n",
    "#如果您正在进行分组以执行每个密钥的聚合（例如总计或平均值），则使用reduceByKey或aggregateByKey将提供更好的性能。\n",
    "rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
    "print(sorted(rdd.groupByKey().mapValues(len).collect()))\n",
    "print(sorted(rdd.groupByKey().mapValues(list).collect()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', ([1], [2])), ('b', ([4], []))]\n",
      "([4], [])\n"
     ]
    }
   ],
   "source": [
    "# cogroup(other, numPartitions=None)\n",
    "# 对于self或other中的每个关键字k，返回一个包含一个元组的结果RDD，以及该关键字在自身和其他关键字中的值列表。\n",
    "x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
    "y = sc.parallelize([(\"a\", 2)])\n",
    "print([(x, tuple(map(list, y))) for x, y in sorted(list(x.cogroup(y).collect()))])\n",
    "print(tuple(map(list,list(x.cogroup(y).collect()[0][1]))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', ([5], [1], [2], [])), ('b', ([6], [4], [], [42]))]\n"
     ]
    }
   ],
   "source": [
    "# groupWith(other, *others)\n",
    "# cogroup的别名，但支持多个RDD\n",
    "w = sc.parallelize([(\"a\", 5), (\"b\", 6)])\n",
    "x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
    "y = sc.parallelize([(\"a\", 2)])\n",
    "z = sc.parallelize([(\"b\", 42)])\n",
    "print([(x, tuple(map(list, y))) for x, y in sorted(list(w.groupWith(x, y, z).collect()))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "abcde\n"
     ]
    }
   ],
   "source": [
    "# reduce(f)\n",
    "# reduce函数是将rdd中的每个元素两两之间按函数f进行操作，然后再结果再两两之间按f进行操作，一直进行下去，\n",
    "# 即所谓的shuffle过程。reduce得到的结果是普通的python对象，而不是rdd.\n",
    "# operator 操作函数 https://docs.python.org/3/library/operator.html\n",
    "from operator import *\n",
    "print(sc.parallelize([1, 2, 3, 4, 5]).reduce(add))\n",
    "print(sc.parallelize([\"a\", \"b\", \"c\", \"d\", \"e\"]).reduce(concat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 2), ('b', 1)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reduceByKey(func, numPartitions=None, partitionFunc=<function portable_hash>)\n",
    "# 按key分组 组内进行reduce处理\n",
    "from operator import *\n",
    "rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
    "sorted(rdd.reduceByKey(add).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 2, 'b': 1}\n",
      "[('a', 2), ('b', 1)]\n"
     ]
    }
   ],
   "source": [
    "# reduceByKeyLocally(func)\n",
    "# 其他与reduceByKey一样，只不过聚合后立即将键，值对以字典的形式传给到集群master，即输出为字典\n",
    "# 这还将在将结果发送到reducer之前在每个映射器上进行本地合并，类似于“合并器”中的MapReduce的\n",
    "from operator import *\n",
    "rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
    "print(rdd.reduceByKeyLocally(add))\n",
    "print(sorted(rdd.reduceByKeyLocally(add).items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-5"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# treeReduce(f, depth=2)\n",
    "# 分区间多次进行reduce\n",
    "# depth 树的深度（执行次数？）\n",
    "add = lambda x, y: x + y\n",
    "rdd = sc.parallelize([-5, -4, -3, -2, -1, 1, 2, 3, 4], 10)\n",
    "rdd.treeReduce(add, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 6]\n",
      "['a', 'b']\n"
     ]
    }
   ],
   "source": [
    "# rdd.keys()\n",
    "# 原rdd的元素为键值对，返回原rdd元素的键为元素的rdd\n",
    "# rdd.values()\n",
    "# 原rdd的元素为键值对，返回原rdd元素的值为元素的rdd\n",
    "w = sc.parallelize([(\"a\", 5), (\"b\", 6)])\n",
    "print(w.keys().collect())\n",
    "print(w.values().collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`aggregate函数`\n",
    "\n",
    "将每个分区里面的元素进行聚合，然后用combine函数将每个分区的结果和初始值(zeroValue)进行combine操作。这个函数最终返回的类型不需要和RDD中元素类型一致。\n",
    "\n",
    "seqOp操作会聚合各分区中的元素，然后combOp操作把所有分区的聚合结果再次聚合，两个操作的初始值都是zeroValue.  seqOp的操作是遍历分区中的所有元素(T)，第一个T跟zeroValue做操作，结果再作为与第二个T做操作的zeroValue，直到遍历完整个分区。combOp操作是把各分区聚合的结果，再聚合。aggregate函数返回一个跟RDD不同类型的值。因此，需要一个操作seqOp来把分区中的元素T合并成一个U，另外一个操作combOp把所有U聚合。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 4)\n",
      "(10, 4)\n",
      "(10, 28)\n"
     ]
    }
   ],
   "source": [
    "#aggregate(zeroValue, seqOp, combOp)\n",
    "seqOp = (lambda x, y : (x[0] + y, x[1] + 1))\n",
    "combOp = (lambda x, y : (x[0] + y[0], x[1] + y[1]))\n",
    "print(sc.parallelize([1, 2, 3, 4]).aggregate((0, 0), seqOp, combOp))\n",
    "print(sc.parallelize([1, 2, 3, 4],3).aggregate((0, 0), seqOp, combOp))\n",
    "# 三个分区多加了4个6 ?\n",
    "print(sc.parallelize([1, 2, 3, 4],3).aggregate((0, 6), seqOp, combOp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregateByKey(zeroValue, seqFunc, combFunc, numPartitions=None, partitionFunc=<function portable_hash>)\n",
    "# 跟aggregate逻辑相同，bykey顾名思义 按照key分区 ，而aggregate按区分配；\n",
    "# 但是zeroValue与aggregate中的用法很不一样，这里的zeroValue是一个值，它即可以跟这样键聚合，也可以跟那个键聚合，而且zeroValue必须与键内聚合时定义的形式一致。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-5"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# treeAggregate(zeroValue, seqOp, combOp, depth=2)\n",
    "# 与aggregate不同的地方是:在每个分区,会做两次或者多次combOp,避免将所有局部的值传给driver端.另外,经过测验初始值zeroValue不会参与combOp.\n",
    "# depth：树的深度\n",
    "add = lambda x, y: x + y\n",
    "rdd = sc.parallelize([-5, -4, -3, -2, -1, 1, 2, 3, 4], 10)\n",
    "rdd.treeAggregate(0, add, add, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fold(zeroValue, op)\n",
    "# partitionBy的简易版，初始一个值，分区内部执行函数和汇总函数为同一个函数\n",
    "from operator import add\n",
    "sc.parallelize([1, 2, 3, 4, 5]).fold(0, add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 2), ('b', 1)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# foldByKey(zeroValue, func, numPartitions=None, partitionFunc=)\n",
    "# 跟fold逻辑相同，只不过是按照key进行分组\n",
    "rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
    "from operator import add\n",
    "sorted(rdd.foldByKey(0, add).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', 1), ('a', 2), ('b', 1), ('b', 3), ('c', 5), ('c', 6)]\n",
      "[('a', [1, 2]), ('b', [1, 3]), ('c', [5, 6])]\n"
     ]
    }
   ],
   "source": [
    "# combineByKey(createCombiner, mergeValue, mergeCombiners, numPartitions=None, partitionFunc=)\n",
    "# 将RDD [（K，V）]转换为RDD [（K，C）]类型的结果，通过三个函数进行转换聚合的目的，\n",
    "# createcombiner函数 rdd值、类型转换\n",
    "# 根据key对值进行合并\n",
    "# 将合并列表，将连个c合并成一个\n",
    "x=sc.parallelize([('a',1),('a',2),('b',1),('b',3),('c',5),('c',6)])\n",
    "def to_list(a):\n",
    "    return [a]\n",
    "def append(a,b):\n",
    "    a.append(b)\n",
    "    return a\n",
    "def extend(a,b):\n",
    "    a.extend(b)\n",
    "    return a\n",
    "print(x.collect())\n",
    "print(x.combineByKey(to_list,append,extend).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]\n",
      "[('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]\n"
     ]
    }
   ],
   "source": [
    "# rdd.sortBy(keyfunc, ascending=True, numPartitions=None)\n",
    "# 根据key对应的函数进行排序\n",
    "tmp = [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]\n",
    "print(sc.parallelize(tmp).sortBy(lambda x: x[0]).collect())\n",
    "print(sc.parallelize(tmp).sortBy(lambda x: x[1]).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('1', 3)\n",
      "[('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]\n",
      "[('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]\n",
      "[('a', 3), ('fleece', 7), ('had', 2), ('lamb', 5), ('little', 4), ('Mary', 1), ('was', 8), ('white', 9), ('whose', 6)]\n"
     ]
    }
   ],
   "source": [
    "# sortByKey(ascending=True, numPartitions=None, keyfunc=<function RDD.<lambda>>)\n",
    "# 对此RDD进行排序，假定它由（键，值）对组成\n",
    "tmp = [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]\n",
    "print(sc.parallelize(tmp).sortByKey().first())\n",
    "print(sc.parallelize(tmp).sortByKey(True, 1).collect())\n",
    "print(sc.parallelize(tmp).sortByKey(True, 2).collect())\n",
    "tmp2 = [('Mary', 1), ('had', 2), ('a', 3), ('little', 4), ('lamb', 5)]\n",
    "tmp2.extend([('whose', 6), ('fleece', 7), ('was', 8), ('white', 9)])\n",
    "print(sc.parallelize(tmp2).sortByKey(True, 3, keyfunc=lambda k: k.lower()).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(count: 4, mean: 2.5, stdev: 1.118033988749895, max: 4.0, min: 1.0)\n"
     ]
    }
   ],
   "source": [
    "# stats()\n",
    "# 计算rdd中全体元素的均值、方差、最大值、最小值和个数的信息\n",
    "samp=sc.parallelize([1,2,3,4]).stats()\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rdd.count()\n",
    "# 计算rdd所有元素个数\n",
    "sc.parallelize([2, 3, 4]).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# countApprox(timeout, confidence=0.95)\n",
    "# 在限定时间内做出有可能的结果，即使任务没有完成\n",
    "rdd = sc.parallelize(range(10000), 10)\n",
    "rdd.countApprox(1000, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1060\n",
      "19\n"
     ]
    }
   ],
   "source": [
    "# countApproxDistinct(relativeSD=0.05)\n",
    "# 返回RDD中不同值数的近似值\n",
    "# relativeSD 相对准确度。较小的值创建需要更多空间的计数器。它必须大于0.000017。\n",
    "n = sc.parallelize(range(1000)).map(str).countApproxDistinct()\n",
    "print(n)\n",
    "n = sc.parallelize([i % 20 for i in range(1000)]).countApproxDistinct()\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', 2), ('b', 1)]\n",
      "defaultdict(<class 'int'>, {'a': 2, 'b': 1})\n"
     ]
    }
   ],
   "source": [
    "# countByKey()\n",
    "# 计算每个键的元素数量，并将结果作为字典返回给主数据。\n",
    "rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
    "print(sorted(rdd.countByKey().items()))\n",
    "print(rdd.countByKey())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2], [1, 2, 2]]\n",
      "[(1, 2), (2, 3)]\n"
     ]
    }
   ],
   "source": [
    "# countByValue()\n",
    "# 将此RDD中每个唯一值的计数返回为（值，计数）对的字典。\n",
    "print(sc.parallelize([1, 2, 1, 2, 2], 2).glom().collect())\n",
    "print(sorted(sc.parallelize([1, 2, 1, 2, 2], 2).countByValue().items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first() 返回第一个元素\n",
    "# max()返回最大值\n",
    "# take(num) 返回开始num个值\n",
    "# top(num, key=None) 计算rdd所有元素按降序排列后最顶部的几个元素\n",
    "# min() rdd中的最小值\n",
    "# mean() 计算rdd所有元素均值\n",
    "# variance() 方差\n",
    "# stdev() 标准差\n",
    "# sum() 和"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([0, 25, 50], [25, 26])\n",
      "([0, 5, 25, 50], [5, 20, 26])\n",
      "([0, 15, 30, 45, 60], [15, 15, 15, 6])\n",
      "(('a', 'b', 'c'), [2, 2])\n"
     ]
    }
   ],
   "source": [
    "# histogram(buckets）\n",
    "# 对rdd中的元素进行频数统计，统计区间有两种，一种是给出段数，一种是直接给出区间。返回为元组\n",
    "rdd = sc.parallelize(range(51))\n",
    "print(rdd.histogram(2))\n",
    "print(rdd.histogram([0, 5, 25, 50]))\n",
    "print(rdd.histogram([0, 15, 30, 45, 60]))\n",
    "rdd = sc.parallelize([\"ab\", \"ac\", \"b\", \"bd\", \"ef\"])\n",
    "print(rdd.histogram((\"a\", \"b\", \"c\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1', '2', '', '3']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pipe(command, env=None, checkCode=False)\n",
    "# 通过管道向后面环节输出command处理过的结果，具体功能就体现在command，command为linux命令。 \n",
    "# pipe函数中的'cat'为linux命令，表示打印内容。\n",
    "sc.parallelize(['1', '2', '', '3']).pipe('cat').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter(f)\n",
    "# 返回满足条件的新RDD\n",
    "rdd = sc.parallelize([1, 2, 3, 4, 5])\n",
    "rdd.filter(lambda x: x % 2 == 0).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# distinct(numPartitions=None)\n",
    "# 返回一个没有重复元素的新RDD，就是去重处理\n",
    "sorted(sc.parallelize([1, 1, 2, 3]).distinct().collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "# sorted(sc.parallelize([1, 1, 2, 3]).distinct().collect())\n",
    "# 返回此RDD的采样子集\n",
    "# withReplacement：是否重复采样\n",
    "# fraction：样本预期占RDD的大小，每一个元素被取到的概率一样，是一个【0,1】的数\n",
    "# seed 随机模式的种子\n",
    "rdd = sc.parallelize(range(100), 4)\n",
    "print(rdd.sample(False, 0.1, 81).count())\n",
    "print(rdd.sample(False, 0.2, 81).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', 0), ('b', 0), ('a', 1), ('a', 2), ('b', 1), ('b', 2), ('a', 3), ('a', 4), ('a', 5), ('a', 6)]\n",
      "209 98\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'ResultIterable' object has no attribute 'takeSample'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-9f7c9d528662>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'b'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'b'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtakeSample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ResultIterable' object has no attribute 'takeSample'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "# sampleByKey(withReplacement, fractions, seed=None)\n",
    "# 返回按键取样的RDD的子集（通过分层抽样）。用分数指定的不同键的变量采样率来创建这个RDD的样本，这是抽样速率图的关键。\n",
    "# 多个key的fractions 以字典方式传递\n",
    "fractions = {\"a\": 0.2, \"b\": 0.1}\n",
    "rdd = sc.parallelize(fractions.keys()).cartesian(sc.parallelize(range(0, 1000)))\n",
    "sample = dict(rdd.sampleByKey(False, fractions, 2).groupByKey().collect())\n",
    "print(rdd.take(10))\n",
    "print(len(sample['a']), len(sample['b']))\n",
    "print(sorted(sample['a'])[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sampleStdev()\n",
    "# 计算这个RDD元素的样本标准差（通过除以N-1而不是N）来修正估计标准差的偏差。\n",
    "sc.parallelize([1, 2, 3]).sampleStdev()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sampleVariance()\n",
    "# 计算这个RDD元素的样本方差（它纠正了通过除以N-1而不是N来估计方差的偏差）。\n",
    "sc.parallelize([1, 2, 3]).sampleVariance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 9, 9, 8, 0, 7, 0, 8, 3, 6, 7, 8]\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "# takeSample(withReplacement, num, seed=None)\n",
    "# 返回这个RDD的一个固定大小的采样子集。\n",
    "# 只有当结果数组被认为是很小的时候，才应该使用这个方法，因为所有的数据都被加载到驱动程序的内存中。\n",
    "rdd = sc.parallelize(range(0, 10))\n",
    "print(rdd.takeSample(True, 12, 1))\n",
    "print(len(rdd.takeSample(False, 5, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# toLocalIterator()\n",
    "# 返回包含这个RDD中所有元素的迭代器。迭代器将消耗与此RDD中最大分区相同的内存。\n",
    "rdd = sc.parallelize(range(10))\n",
    "[x for x in rdd.toLocalIterator()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 2, 3, 1, 1, 2, 3]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# union(other)\n",
    "# 返回这个RDD和另一个的结合。不去重\n",
    "rdd = sc.parallelize([1, 1, 2, 3])\n",
    "rdd.union(rdd).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 1, 3]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# intersection(other)\n",
    "# 返回这个RDD和另一个的交集。即使输入RDDs完成了，输出也不会包含任何重复的元素。\n",
    "# 该方法在内部执行洗牌。\n",
    "rdd1 = sc.parallelize([1, 10, 2, 3, 4, 5])\n",
    "rdd2 = sc.parallelize([1, 6, 2, 3, 7, 8])\n",
    "rdd1.intersection(rdd2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 1), ('b', 4), ('b', 5)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# subtract(other, numPartitions=None)\n",
    "# 返回自己有其他没有的元素的值\n",
    "x = sc.parallelize([(\"a\", 1), (\"b\", 4), (\"b\", 5), (\"a\", 3)])\n",
    "y = sc.parallelize([(\"a\", 3), (\"c\", None)])\n",
    "sorted(x.subtract(y).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('b', 4), ('b', 5)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# subtractByKey(other, numPartitions=None)\n",
    "# 返回每一个（键，值）对，在另一个没有成对的匹配键。\n",
    "x = sc.parallelize([(\"a\", 1), (\"b\", 4), (\"b\", 5), (\"a\", 2)])\n",
    "y = sc.parallelize([(\"a\", 3), (\"c\", None)])\n",
    "sorted(x.subtractByKey(y).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 1), (1, 2), (2, 1), (2, 2)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cartesian(other)\n",
    "# 返回这个RDD和另一个RDD的笛卡尔积，也就是所有成对的元素（a，b）的RDD，a为本身RDD，b为其他RDD\n",
    "rdd = sc.parallelize([1, 2])\n",
    "sorted(rdd.cartesian(rdd).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', (1, 2)), ('a', (1, 3))]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# join(other, numPartitions=None)\n",
    "# 返回一个包含所有成对元素的RDD，其中包含在self和other中匹配的键。每一对元素都将作为一个（k，（v1，v2））返回，其中（k，v1）为self（k，v2）为other。\n",
    "# 在集群中执行散列连接\n",
    "x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
    "y = sc.parallelize([(\"a\", 2), (\"a\", 3)])\n",
    "sorted(x.join(y).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', (2, 1)), ('b', (None, 4))]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rightOuterJoin(other, numPartitions=None)\n",
    "# 对于在otherRDD中的每一个(k, w)元素，生成的RDD中有ｋ键的生成(k, (v, w)),　如果没有ｋ键的话也要生成none补位(k,(None, w))\n",
    "x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
    "y = sc.parallelize([(\"a\", 2)])\n",
    "sorted(y.rightOuterJoin(x).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', (1, 2)), ('b', (4, None))]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# leftOuterJoin(other, numPartitions=None)\n",
    "# 就是用第二个rdd的key去第一个rdd中寻找，在value组合的时候还是第一个rdd的值在前，第二个rdd的值在后。其他与leftOuterJoin完全一样。\n",
    "x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
    "y = sc.parallelize([(\"a\", 2)])\n",
    "sorted(x.leftOuterJoin(y).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "192 308\n"
     ]
    }
   ],
   "source": [
    "# randomSplit(weights, seed=None)\n",
    "# 将RDD按照一定的比例随机分开\n",
    "rdd = sc.parallelize(range(500), 1)\n",
    "rdd1, rdd2 = rdd.randomSplit([2, 3], 17)\n",
    "print(len(rdd1.collect() + rdd2.collect()))\n",
    "print(rdd1.count(), rdd2.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf, SparkContext\n",
    "spark = SparkSession.builder.master('local[1]').appName('learn_ml').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0 = spark.read.csv('file:///home/ffzs/python-projects/learn_spark/mushrooms.csv', header=True, inferSchema=True, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df0.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "看看分类的类别"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**查看是否有na值**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df0.toPandas().isna().sum()\n",
    "df0.toPandas().isna().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "old_columns_names = df0.columns\n",
    "new_columns_names = [name+'-new' for name in old_columns_names]\n",
    "for i in range(len(old_columns_names)):\n",
    "    indexer = StringIndexer(inputCol=old_columns_names[i], outputCol=new_columns_names[i])\n",
    "    df0 = indexer.fit(df0).transform(df0)\n",
    "vecAss = VectorAssembler(inputCols=new_columns_names[1:], outputCol='features')\n",
    "df0 = vecAss.transform(df0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df0 = df0.withColumnRenamed(new_columns_names[0], 'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfi = df0.select(['label', 'features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df0.describe().toPandas().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------------------------------------------------------------------+\n",
      "|label|features                                                                      |\n",
      "+-----+------------------------------------------------------------------------------+\n",
      "|1.0  |(22,[1,3,4,7,8,9,10,19,20,21],[1.0,1.0,6.0,1.0,7.0,1.0,2.0,2.0,2.0,4.0])      |\n",
      "|0.0  |(22,[1,2,3,4,8,9,10,19,20,21],[1.0,3.0,1.0,4.0,7.0,1.0,3.0,1.0,3.0,1.0])      |\n",
      "|0.0  |(22,[0,1,2,3,4,8,9,10,19,20,21],[3.0,1.0,4.0,1.0,5.0,3.0,1.0,3.0,1.0,3.0,5.0])|\n",
      "|1.0  |(22,[2,3,4,7,8,9,10,19,20,21],[4.0,1.0,6.0,1.0,3.0,1.0,2.0,2.0,2.0,4.0])      |\n",
      "|0.0  |(22,[1,2,6,8,10,18,19,20,21],[1.0,1.0,1.0,7.0,2.0,1.0,1.0,4.0,1.0])           |\n",
      "+-----+------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfi.show(5, truncate=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label = df0.rdd.map(lambda row: row[0])\n",
    "# row = df0.rdd.map(lambda row: row[1:])\n",
    "# dfi = label.map(lambda m: 0.0 if m=='p' else 1.0).zip(row.map(lambda x: list(x))).toDF(schema=['label','feature'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = dfi.randomSplit([4.0, 1.0], 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# test_data.filter(test_data['label']==1).show(5, truncate=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 评估器\n",
    "**分类(classification)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LogisticRegression :逻辑回归,支持多项逻辑（softmax）和二项逻辑回归"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pyspark.ml.classification.LogisticRegression(self, featuresCol=\"features\", labelCol=\"label\", predictionCol=\"prediction\", maxIter=100, regParam=0.0, elasticNetParam=0.0, tol=1e-6, fitIntercept=True, threshold=0.5, thresholds=None, probabilityCol=\"probability\", rawPredictionCol=\"rawPrediction\", standardization=True, weightCol=None, aggregationDepth=2, family=\"auto\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`\n",
    "regParam: 正则化参数(>=0)\n",
    "elasticNetParam: ElasticNet混合参数，0-1之间，当alpha为0时,惩罚为L2正则化，当为1时为L1正则化\n",
    "fitIntercept: 是否拟合一个截距项\n",
    "Standardization： 是否在拟合数据之前对数据进行标准化\n",
    "aggregationDepth: 树聚合所建议的深度(>=2)\n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "blor = LogisticRegression(regParam=0.01)\n",
    "blorModel = blor.fit(train_data)\n",
    "result = blorModel.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9661954517516902"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.filter(result.label == result.prediction).count()/result.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                 FPR|                 TPR|\n",
      "+--------------------+--------------------+\n",
      "|                 0.0|                 0.0|\n",
      "|                 0.0|0.020466901183242726|\n",
      "|                 0.0| 0.04093380236648545|\n",
      "|5.934718100890207E-4|0.060761112887751836|\n",
      "|0.001186943620178...| 0.08058842340901823|\n",
      "|0.001483679525222552| 0.10073552926127279|\n",
      "|0.001780415430267...| 0.12088263511352734|\n",
      "|0.002373887240356083| 0.14070994563479372|\n",
      "|0.002670623145400...|  0.1608570514870483|\n",
      "|0.002670623145400...| 0.18132395267029103|\n",
      "|0.002670623145400...| 0.20179085385353374|\n",
      "|0.002670623145400...| 0.22225775503677647|\n",
      "|0.002670623145400...| 0.24272465622001918|\n",
      "|0.002670623145400...|  0.2631915574032619|\n",
      "|0.002670623145400...|  0.2836584585865046|\n",
      "|0.002670623145400...| 0.30412535976974736|\n",
      "|0.002670623145400...|  0.3245922609529901|\n",
      "|0.002670623145400...| 0.34505916213623283|\n",
      "|0.002670623145400...|  0.3655260633194755|\n",
      "|0.002670623145400...| 0.38599296450271825|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------------------+------------------+\n",
      "|              recall|         precision|\n",
      "+--------------------+------------------+\n",
      "|                 0.0|               1.0|\n",
      "|0.020466901183242726|               1.0|\n",
      "| 0.04093380236648545|               1.0|\n",
      "|0.060761112887751836|0.9895833333333334|\n",
      "| 0.08058842340901823|          0.984375|\n",
      "| 0.10073552926127279|          0.984375|\n",
      "| 0.12088263511352734|          0.984375|\n",
      "| 0.14070994563479372|0.9821428571428571|\n",
      "|  0.1608570514870483|       0.982421875|\n",
      "| 0.18132395267029103|          0.984375|\n",
      "| 0.20179085385353374|         0.9859375|\n",
      "| 0.22225775503677647|0.9872159090909091|\n",
      "| 0.24272465622001918|        0.98828125|\n",
      "|  0.2631915574032619|0.9891826923076923|\n",
      "|  0.2836584585865046|0.9899553571428571|\n",
      "| 0.30412535976974736|          0.990625|\n",
      "|  0.3245922609529901|      0.9912109375|\n",
      "| 0.34505916213623283|0.9917279411764706|\n",
      "|  0.3655260633194755|         0.9921875|\n",
      "| 0.38599296450271825|0.9925986842105263|\n",
      "+--------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "blorModel.\n",
    "blorModel.summary.pr.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 决策树\n",
    "pyspark.ml.classification.DecisionTreeClassifier(featuresCol='features', labelCol='label', predictionCol='prediction', probabilityCol='probability', rawPredictionCol='rawPrediction', maxDepth=5, maxBins=32, minInstancesPerNode=1, minInfoGain=0.0, maxMemoryInMB=256, cacheNodeIds=False, checkpointInterval=10, impurity='gini', seed=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`\n",
    "checkpointInterval：设置checkpoint区间(>=1)，或宕掉checkpoint(-1)，例如10意味着缓冲区(cache)将会每迭代10次获得一次checkpoint\n",
    "fit(datasset,params=None)\n",
    "impurity: 信息增益计算的准则，选项\"entropy\", \"gini\"\n",
    "maxBins：连续特征离散化的最大分箱，必须>=2 并且>=分类特征分类的数量\n",
    "maxDepth：树的最大深度\n",
    "minInfoGain：分割结点所需的最小的信息增益\n",
    "minInstancesPerNode：每个结点最小实例个数\n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier(maxDepth=5)\n",
    "dtModel = dt.fit(train_data)\n",
    "result = dtModel.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accuracy\n",
    "result.filter(result.label == result.prediction).count()/result.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 梯度增强树\n",
    "pyspark.ml.classification.GBTClassifier(featuresCol='features', labelCol='label', predictionCol='prediction', maxDepth=5, maxBins=32, minInstancesPerNode=1, minInfoGain=0.0, maxMemoryInMB=256, cacheNodeIds=False, checkpointInterval=10, lossType='logistic', maxIter=20, stepSize=0.1, seed=None, subsamplingRate=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`\n",
    "checkpointInterval: 同DecisionTreeClassifier\n",
    "fit(dataset,params=None)方法\n",
    "lossType: GBT要最小化的损失函数，选项：logistic\n",
    "maxBins: 同DecisionTreeClassifier\n",
    "maxDepth: 同DecisionTreeClassifier\n",
    "maxIter: 同DecisionTreeClassifier\n",
    "minInfoGain: 同DecisionTreeClassifier\n",
    "minInstancesPerNode：同DecisionTreeClassifier\n",
    "stepSize: 每次迭代优化的步长\n",
    "subsamplingRate: 同RandomForesetClassier\n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import GBTClassifier\n",
    "gbt = GBTClassifier(maxDepth=5)\n",
    "gbtModel = gbt.fit(train_data)\n",
    "result = gbtModel.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.filter(result.label == result.prediction).count()/result.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 随机森林\n",
    "pyspark.ml.classification.RandomForestClassifier(featuresCol='features', labelCol='label', predictionCol='prediction', probabilityCol='probability', rawPredictionCol='rawPrediction', maxDepth=5, maxBins=32, minInstancesPerNode=1, minInfoGain=0.0, maxMemoryInMB=256, cacheNodeIds=False, checkpointInterval=10, impurity='gini', numTrees=20, featureSubsetStrategy='auto', seed=None, subsamplingRate=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`\n",
    "checkpoint：同DecisionTreeClassifier\n",
    "featureSubsetStrategy：每棵树上要分割的特征数目，选项为\"auto\",\"all\", \"onethird\", \"sqrt\", \"log2\", \"(0.0-1.0],\"[1-n]\"\n",
    "fit(dataset,params=None)方法\n",
    "impurity: 同DecisionTreeClassifier\n",
    "maxBins:同DecisionTreeClassifier\n",
    "maxDepth：同DecisionTreeClassifier\n",
    "minInfoGain: 同DecisionTreeClassifier\n",
    "numTrees: 训练树的个数\n",
    "subsamplingRate: 用于训练每颗决策树的样本个数，区间(0,1]\n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "rf = RandomForestClassifier(numTrees=10, maxDepth=5)\n",
    "rfModel = rf.fit(train_data)\n",
    "# model.featureImportances\n",
    "result = rfModel.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.filter(result.label == result.prediction).count()/result.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 朴素贝叶斯\n",
    "pyspark.ml.classification.NaiveBayes(featuresCol='features', labelCol='label', predictionCol='prediction', probabilityCol='probability', rawPredictionCol='rawPrediction', smoothing=1.0, modelType='multinomial', thresholds=None, weightCol=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`\n",
    "modelType: 选项：multinomial（多项式）和bernoulli（伯努利）\n",
    "smoothing: 平滑参数，应该>=0，默认为1.0\n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import NaiveBayes\n",
    "nb = NaiveBayes()\n",
    "nbModel = nb.fit(train_data)\n",
    "result = nbModel.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9231714812538414"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.filter(result.label == result.prediction).count()/result.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LinearSVC 支持向量机\n",
    "pyspark.ml.classification.LinearSVC(featuresCol='features', labelCol='label', predictionCol='prediction', maxIter=100, regParam=0.0, tol=1e-06, rawPredictionCol='rawPrediction', fitIntercept=True, standardization=True, threshold=0.0, weightCol=None, aggregationDepth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LinearSVC\n",
    "svm = LinearSVC(maxIter=10, regPcaram=0.01)\n",
    "svmModel = svm.fit(train_data)\n",
    "result = svmModel.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9797172710510141"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accuracy\n",
    "result.filter(result.label == result.prediction).count()/result.count()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

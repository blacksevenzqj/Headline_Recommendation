{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分析并预处理raw_sample数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# 配置spark driver和pyspark运行时，所使用的python解释器路径\n",
    "PYSPARK_PYTHON = \"/root/miniconda2/envs/bigdata/bin/python\"\n",
    "os.environ[\"PYSPARK_PYTHON\"] = PYSPARK_PYTHON\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = PYSPARK_PYTHON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark配置信息\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "SPARK_APP_NAME = \"preprocessingRawSample\"\n",
    "SPARK_URL = \"spark://192.168.19.137:7077\"\n",
    "\n",
    "conf = SparkConf()    # 创建spark config对象\n",
    "config = (\n",
    "\t(\"spark.app.name\", SPARK_APP_NAME),    # 设置启动的spark的app名称，没有提供，将随机产生一个名称\n",
    "\t(\"spark.executor.memory\", \"2g\"),    # 设置该app启动时占用的内存用量，默认1g\n",
    "\t(\"spark.master\", SPARK_URL),    # spark master的地址\n",
    "    (\"spark.executor.cores\", \"2\")    # 设置spark executor使用的CPU核心数\n",
    "# \t('spark.sql.pivotMaxValues', '99999'),  # 当需要pivot DF，且值很多时，需要修改，默认是10000\n",
    ")\n",
    "# 查看更详细配置及说明：https://spark.apache.org/docs/latest/configuration.html\n",
    "# \n",
    "conf.setAll(config)\n",
    "\n",
    "# 利用config对象，创建spark session\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/root/bigdata/hadoop-2.9.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/root/bigdata/apache-hive-2.3.4-bin/lib/log4j-slf4j-impl-2.6.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\n",
      "Found 4 items\n",
      "-rw-r--r--   1 root supergroup    31286431 2018-11-30 02:52 /project1-ad-rs/datasets/ad_feature.csv\n",
      "-rw-r--r--   1 root supergroup 23728773580 2018-11-30 02:50 /project1-ad-rs/datasets/behavior_log.csv\n",
      "-rw-r--r--   1 root supergroup  1088060964 2018-11-30 02:53 /project1-ad-rs/datasets/raw_sample.csv\n",
      "-rw-r--r--   1 root supergroup    24056588 2018-11-30 02:52 /project1-ad-rs/datasets/user_profile.csv\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -ls /workspace/3.rs_project/project1/dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+-----------+------+---+\n",
      "|  user|time_stamp|adgroup_id|        pid|nonclk|clk|\n",
      "+------+----------+----------+-----------+------+---+\n",
      "|581738|1494137644|         1|430548_1007|     1|  0|\n",
      "|449818|1494638778|         3|430548_1007|     1|  0|\n",
      "|914836|1494650879|         4|430548_1007|     1|  0|\n",
      "|914836|1494651029|         5|430548_1007|     1|  0|\n",
      "|399907|1494302958|         8|430548_1007|     1|  0|\n",
      "|628137|1494524935|         9|430548_1007|     1|  0|\n",
      "|298139|1494462593|         9|430539_1007|     1|  0|\n",
      "|775475|1494561036|         9|430548_1007|     1|  0|\n",
      "|555266|1494307136|        11|430539_1007|     1|  0|\n",
      "|117840|1494036743|        11|430548_1007|     1|  0|\n",
      "|739815|1494115387|        11|430539_1007|     1|  0|\n",
      "|623911|1494625301|        11|430548_1007|     1|  0|\n",
      "|623911|1494451608|        11|430548_1007|     1|  0|\n",
      "|421590|1494034144|        11|430548_1007|     1|  0|\n",
      "|976358|1494156949|        13|430548_1007|     1|  0|\n",
      "|286630|1494218579|        13|430539_1007|     1|  0|\n",
      "|286630|1494289247|        13|430539_1007|     1|  0|\n",
      "|771431|1494153867|        13|430548_1007|     1|  0|\n",
      "|707120|1494220810|        13|430548_1007|     1|  0|\n",
      "|530454|1494293746|        13|430548_1007|     1|  0|\n",
      "+------+----------+----------+-----------+------+---+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- user: string (nullable = true)\n",
      " |-- time_stamp: string (nullable = true)\n",
      " |-- adgroup_id: string (nullable = true)\n",
      " |-- pid: string (nullable = true)\n",
      " |-- nonclk: string (nullable = true)\n",
      " |-- clk: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 从HDFS中加载样本数据信息\n",
    "df = spark.read.csv(\"hdfs://hadoop-master:9000/workspace/3.rs_project/project1/dataset/raw_sample.csv\", header=True)\n",
    "df.show()    # 展示数据，默认前20条\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 分析数据集字段的类型和格式\n",
    "1. 查看是否有空值\n",
    "2. 查看每列数据的类型\n",
    "3. 查看每列数据的类别情况"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "样本数据集总条目数： 26557961\n",
      "用户user总数： 1141729\n",
      "广告id adgroup_id总数： 846811\n",
      "广告展示位pid情况： [Row(pid='430548_1007', count=16472898), Row(pid='430539_1007', count=10085063)]\n",
      "广告点击数据情况clk： [Row(clk='0', count=25191905), Row(clk='1', count=1366056)]\n"
     ]
    }
   ],
   "source": [
    "print(\"样本数据集总条目数：\", df.count())\n",
    "# 约2600w\n",
    "print(\"用户user总数：\", df.groupBy(\"user\").count().count())\n",
    "# 约 114w，略多余日志数据中用户数\n",
    "print(\"广告id adgroup_id总数：\", df.groupBy(\"adgroup_id\").count().count())\n",
    "# 约85w\n",
    "print(\"广告展示位pid情况：\", df.groupBy(\"pid\").count().collect())\n",
    "# 只有两种广告展示位，占比约为六比四\n",
    "print(\"广告点击数据情况clk：\", df.groupBy(\"clk\").count().collect())\n",
    "# 点和不点比率约： 1:20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 使用dataframe.withColumn更改df列数据结构；使用dataframe.withColumnRenamed更改列名称"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user: string (nullable = true)\n",
      " |-- time_stamp: string (nullable = true)\n",
      " |-- adgroup_id: string (nullable = true)\n",
      " |-- pid: string (nullable = true)\n",
      " |-- nonclk: string (nullable = true)\n",
      " |-- clk: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- userId: integer (nullable = true)\n",
      " |-- timestamp: long (nullable = true)\n",
      " |-- adgroupId: integer (nullable = true)\n",
      " |-- pid: string (nullable = true)\n",
      " |-- nonclk: integer (nullable = true)\n",
      " |-- clk: integer (nullable = true)\n",
      "\n",
      "+------+----------+---------+-----------+------+---+\n",
      "|userId| timestamp|adgroupId|        pid|nonclk|clk|\n",
      "+------+----------+---------+-----------+------+---+\n",
      "|581738|1494137644|        1|430548_1007|     1|  0|\n",
      "|449818|1494638778|        3|430548_1007|     1|  0|\n",
      "|914836|1494650879|        4|430548_1007|     1|  0|\n",
      "|914836|1494651029|        5|430548_1007|     1|  0|\n",
      "|399907|1494302958|        8|430548_1007|     1|  0|\n",
      "|628137|1494524935|        9|430548_1007|     1|  0|\n",
      "|298139|1494462593|        9|430539_1007|     1|  0|\n",
      "|775475|1494561036|        9|430548_1007|     1|  0|\n",
      "|555266|1494307136|       11|430539_1007|     1|  0|\n",
      "|117840|1494036743|       11|430548_1007|     1|  0|\n",
      "|739815|1494115387|       11|430539_1007|     1|  0|\n",
      "|623911|1494625301|       11|430548_1007|     1|  0|\n",
      "|623911|1494451608|       11|430548_1007|     1|  0|\n",
      "|421590|1494034144|       11|430548_1007|     1|  0|\n",
      "|976358|1494156949|       13|430548_1007|     1|  0|\n",
      "|286630|1494218579|       13|430539_1007|     1|  0|\n",
      "|286630|1494289247|       13|430539_1007|     1|  0|\n",
      "|771431|1494153867|       13|430548_1007|     1|  0|\n",
      "|707120|1494220810|       13|430548_1007|     1|  0|\n",
      "|530454|1494293746|       13|430548_1007|     1|  0|\n",
      "+------+----------+---------+-----------+------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 更改表结构，转换为对应的数据类型\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, FloatType, LongType, StringType\n",
    "\n",
    "# 打印df结构信息\n",
    "df.printSchema()   \n",
    "# 更改df表结构：更改列类型和列名称\n",
    "raw_sample_df = df.\\\n",
    "    withColumn(\"user\", df.user.cast(IntegerType())).withColumnRenamed(\"user\", \"userId\").\\\n",
    "    withColumn(\"time_stamp\", df.time_stamp.cast(LongType())).withColumnRenamed(\"time_stamp\", \"timestamp\").\\\n",
    "    withColumn(\"adgroup_id\", df.adgroup_id.cast(IntegerType())).withColumnRenamed(\"adgroup_id\", \"adgroupId\").\\\n",
    "    withColumn(\"pid\", df.pid.cast(StringType())).\\\n",
    "    withColumn(\"nonclk\", df.nonclk.cast(IntegerType())).\\\n",
    "    withColumn(\"clk\", df.clk.cast(IntegerType()))\n",
    "raw_sample_df.printSchema()\n",
    "raw_sample_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 特征选取（Feature Selection）\n",
    "\n",
    "特征选择就是选择那些靠谱的Feature，去掉冗余的Feature，对于搜索广告，Query关键词和广告的匹配程度很重要；但对于展示广告，广告本身的历史表现，往往是最重要的Feature。\n",
    "\n",
    "根据经验，该数据集中，只有广告展示位pid对比较重要，且数据不同数据之间的占比约为6:4，因此pid可以作为一个关键特征\n",
    "\n",
    "nonclk和clk在这里是作为目标值，不做为特征"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 热独编码 OneHotEncode\n",
    "\n",
    "热独编码是一种经典编码，是使用N位状态寄存器(如0和1)来对N个状态进行编码，每个状态都由他独立的寄存器位，并且在任意时候，其中只有一位有效。\n",
    "\n",
    "假设有三组特征，分别表示年龄，城市，设备；\n",
    "\n",
    "[\"男\", \"女\"]  [0,1]\n",
    "\n",
    "[\"北京\", \"上海\", \"广州\"]  [0,1,2]\n",
    "\n",
    "[\"苹果\", \"小米\", \"华为\", \"微软\"]  [0,1,2,3]\n",
    "\n",
    "传统变化： 对每一组特征，使用枚举类型，从0开始；\n",
    "\n",
    "[\"男“，”上海“，”小米“]=[ 0,1,1]\n",
    "\n",
    "[\"女“，”北京“，”苹果“] =[1,0,0]\n",
    "\n",
    "传统变化后的数据不是连续的，而是随机分配的，不容易应用在分类器中\n",
    "\n",
    "而经过热独编码，数据会变成稀疏的，方便分类器处理：\n",
    "\n",
    "[\"男“，”上海“，”小米“]=[ 1,0,0,1,0,0,1,0,0]\n",
    "\n",
    "[\"女“，”北京“，”苹果“] =[0,1,1,0,0,1,0,0,0]\n",
    "\n",
    "这样做保留了特征的多样性，但是也要注意如果数据过于稀疏(样本较少、维度过高)，其效果反而会变差"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spark中使用热独编码\n",
    "\n",
    "注意：热编码只能对字符串类型的列数据进行处理\n",
    "\n",
    "[StringIndexer](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html?highlight=stringindexer#pyspark.ml.feature.StringIndexer)：对指定字符串列数据进行特征处理，如将性别数据“男”、“女”转化为0和1\n",
    "\n",
    "[OneHotEncoder](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html?highlight=onehotencoder#pyspark.ml.feature.OneHotEncoder)：对特征列数据，进行热编码，通常需结合StringIndexer一起使用\n",
    "\n",
    "[Pipeline](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html?highlight=pipeline#pyspark.ml.Pipeline)：让数据按顺序依次被处理，将前一次的处理结果作为下一次的输入\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+---------+-----------+------+---+-----------+-------------+\n",
      "|userId| timestamp|adgroupId|        pid|nonclk|clk|pid_feature|    pid_value|\n",
      "+------+----------+---------+-----------+------+---+-----------+-------------+\n",
      "|581738|1494137644|        1|430548_1007|     1|  0|        0.0|(2,[0],[1.0])|\n",
      "|449818|1494638778|        3|430548_1007|     1|  0|        0.0|(2,[0],[1.0])|\n",
      "|914836|1494650879|        4|430548_1007|     1|  0|        0.0|(2,[0],[1.0])|\n",
      "|914836|1494651029|        5|430548_1007|     1|  0|        0.0|(2,[0],[1.0])|\n",
      "|399907|1494302958|        8|430548_1007|     1|  0|        0.0|(2,[0],[1.0])|\n",
      "|628137|1494524935|        9|430548_1007|     1|  0|        0.0|(2,[0],[1.0])|\n",
      "|298139|1494462593|        9|430539_1007|     1|  0|        1.0|(2,[1],[1.0])|\n",
      "|775475|1494561036|        9|430548_1007|     1|  0|        0.0|(2,[0],[1.0])|\n",
      "|555266|1494307136|       11|430539_1007|     1|  0|        1.0|(2,[1],[1.0])|\n",
      "|117840|1494036743|       11|430548_1007|     1|  0|        0.0|(2,[0],[1.0])|\n",
      "|739815|1494115387|       11|430539_1007|     1|  0|        1.0|(2,[1],[1.0])|\n",
      "|623911|1494625301|       11|430548_1007|     1|  0|        0.0|(2,[0],[1.0])|\n",
      "|623911|1494451608|       11|430548_1007|     1|  0|        0.0|(2,[0],[1.0])|\n",
      "|421590|1494034144|       11|430548_1007|     1|  0|        0.0|(2,[0],[1.0])|\n",
      "|976358|1494156949|       13|430548_1007|     1|  0|        0.0|(2,[0],[1.0])|\n",
      "|286630|1494218579|       13|430539_1007|     1|  0|        1.0|(2,[1],[1.0])|\n",
      "|286630|1494289247|       13|430539_1007|     1|  0|        1.0|(2,[1],[1.0])|\n",
      "|771431|1494153867|       13|430548_1007|     1|  0|        0.0|(2,[0],[1.0])|\n",
      "|707120|1494220810|       13|430548_1007|     1|  0|        0.0|(2,[0],[1.0])|\n",
      "|530454|1494293746|       13|430548_1007|     1|  0|        0.0|(2,[0],[1.0])|\n",
      "+------+----------+---------+-----------+------+---+-----------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''特征处理'''\n",
    "'''\n",
    "pid 资源位。该特征属于分类特征，只有两类取值，因此考虑进行热编码处理即可，分为是否在资源位1、是否在资源位2 两个特征\n",
    "'''\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# StringIndexer对指定字符串列进行特征处理\n",
    "stringindexer = StringIndexer(inputCol='pid', outputCol='pid_feature')\n",
    "\n",
    "# 对处理出来的特征处理列进行，热独编码\n",
    "encoder = OneHotEncoder(dropLast=False, inputCol='pid_feature', outputCol='pid_value')\n",
    "# 利用管道对每一个数据进行热独编码处理\n",
    "pipeline = Pipeline(stages=[stringindexer, encoder])\n",
    "pipeline_model = pipeline.fit(raw_sample_df)\n",
    "new_df = pipeline_model.transform(raw_sample_df)\n",
    "new_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 返回字段pid_value是一个稀疏向量类型数据\n",
    "\n",
    "#### [pyspark.ml.linalg.SparseVector](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html?highlight=sparse#pyspark.ml.linalg.SparseVector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4,[1,3],[3.0,4.0])\n",
      "[0. 3. 0. 4.]\n",
      "*********\n",
      "Row(pid_value=SparseVector(2, {0: 1.0}))\n",
      "[1. 0.]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.linalg import SparseVector\n",
    "# 参数：维度、索引列表、值列表\n",
    "print(SparseVector(4, [1, 3], [3.0, 4.0]))\n",
    "print(SparseVector(4, [1, 3], [3.0, 4.0]).toArray())\n",
    "print(\"*********\")\n",
    "print(new_df.select(\"pid_value\").first())\n",
    "print(new_df.select(\"pid_value\").first().pid_value.toArray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+---------+-----------+------+---+-----------+-------------+\n",
      "|userId| timestamp|adgroupId|        pid|nonclk|clk|pid_feature|    pid_value|\n",
      "+------+----------+---------+-----------+------+---+-----------+-------------+\n",
      "|177002|1494691186|   593001|430548_1007|     1|  0|        0.0|(2,[0],[1.0])|\n",
      "|243671|1494691186|   600195|430548_1007|     1|  0|        0.0|(2,[0],[1.0])|\n",
      "|488527|1494691184|   494312|430548_1007|     1|  0|        0.0|(2,[0],[1.0])|\n",
      "|488527|1494691184|   431082|430548_1007|     1|  0|        0.0|(2,[0],[1.0])|\n",
      "| 17054|1494691184|   742741|430548_1007|     1|  0|        0.0|(2,[0],[1.0])|\n",
      "| 17054|1494691184|   756665|430548_1007|     1|  0|        0.0|(2,[0],[1.0])|\n",
      "|488527|1494691184|   687854|430548_1007|     1|  0|        0.0|(2,[0],[1.0])|\n",
      "|839493|1494691183|   561681|430548_1007|     1|  0|        0.0|(2,[0],[1.0])|\n",
      "|704223|1494691183|   624504|430539_1007|     1|  0|        1.0|(2,[1],[1.0])|\n",
      "|839493|1494691183|   582235|430548_1007|     1|  0|        0.0|(2,[0],[1.0])|\n",
      "|704223|1494691183|   675674|430539_1007|     1|  0|        1.0|(2,[1],[1.0])|\n",
      "|628998|1494691180|   618965|430548_1007|     1|  0|        0.0|(2,[0],[1.0])|\n",
      "|674444|1494691179|   427579|430548_1007|     1|  0|        0.0|(2,[0],[1.0])|\n",
      "|627200|1494691179|   782038|430548_1007|     1|  0|        0.0|(2,[0],[1.0])|\n",
      "|627200|1494691179|   420769|430548_1007|     1|  0|        0.0|(2,[0],[1.0])|\n",
      "|674444|1494691179|   588664|430548_1007|     1|  0|        0.0|(2,[0],[1.0])|\n",
      "|738335|1494691179|   451004|430539_1007|     1|  0|        1.0|(2,[1],[1.0])|\n",
      "|627200|1494691179|   817569|430548_1007|     1|  0|        0.0|(2,[0],[1.0])|\n",
      "|322244|1494691179|   820018|430548_1007|     1|  0|        0.0|(2,[0],[1.0])|\n",
      "|322244|1494691179|   735220|430548_1007|     1|  0|        0.0|(2,[0],[1.0])|\n",
      "+------+----------+---------+-----------+------+---+-----------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 查看最大时间\n",
    "new_df.sort(\"timestamp\", ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "该时间之前的数据为训练样本，该时间以后的数据为测试样本： 2017-05-12 23:59:46\n"
     ]
    }
   ],
   "source": [
    "# 本样本数据集共计8天数据\n",
    "# 前七天为训练数据、最后一天为测试数据\n",
    "\n",
    "from datetime import datetime\n",
    "datetime.fromtimestamp(1494691186)\n",
    "print(\"该时间之前的数据为训练样本，该时间以后的数据为测试样本：\", datetime.fromtimestamp(1494691186-24*60*60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练样本个数：\n",
      "23249291\n",
      "测试样本个数：\n",
      "3308670\n"
     ]
    }
   ],
   "source": [
    "# 训练样本：\n",
    "train_sample = raw_sample_df.filter(raw_sample_df.timestamp<=(1494691186-24*60*60))\n",
    "print(\"训练样本个数：\")\n",
    "print(train_sample.count())\n",
    "# 测试样本\n",
    "test_sample = raw_sample_df.filter(raw_sample_df.timestamp>(1494691186-24*60*60))\n",
    "print(\"测试样本个数：\")\n",
    "print(test_sample.count())\n",
    "\n",
    "# 注意：还需要加入广告基本特征和用户基本特征才能做程一份完整的样本数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

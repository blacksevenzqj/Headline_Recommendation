{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预处理behavior_log数据集\n",
    "\n",
    "检查数据格式，以及对数据进行简单的统计运算(不同用户不同行为的次数)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# 配置spark driver和pyspark运行时，所使用的python解释器路径\n",
    "PYSPARK_PYTHON = \"/root/miniconda2/envs/bigdata/bin/python\"\n",
    "# 当存在多个版本时，不指定很可能会导致出错\n",
    "os.environ[\"PYSPARK_PYTHON\"] = PYSPARK_PYTHON\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = PYSPARK_PYTHON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark配置信息\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "SPARK_APP_NAME = \"preprocessingBehaviorLog\"\n",
    "SPARK_URL = \"spark://192.168.19.137:7077\"\n",
    "\n",
    "conf = SparkConf()    # 创建spark config对象\n",
    "config = (\n",
    "    (\"spark.app.name\", SPARK_APP_NAME),    # 设置启动的spark的app名称，没有提供，将随机产生一个名称\n",
    "    (\"spark.executor.memory\", \"2g\"),    # 设置该app启动时占用的内存用量，默认1g\n",
    "    (\"spark.master\", SPARK_URL),    # spark master的地址\n",
    "    (\"spark.executor.cores\", \"2\"),    # 设置spark executor使用的CPU核心数\n",
    "    # 以下三项配置，可以控制执行器数量\n",
    "    # (\"spark.dynamicAllocation.enabled\", True),\n",
    "    # (\"spark.dynamicAllocation.initialExecutors\", 1),    # 1个执行器\n",
    "    # (\"spark.shuffle.service.enabled\", True)\n",
    "    # ('spark.sql.pivotMaxValues', '99999'),  # 当需要pivot DF，且值很多时，需要修改，默认是10000\n",
    ")\n",
    "# 查看更详细配置及说明：https://spark.apache.org/docs/latest/configuration.html\n",
    "\n",
    "conf.setAll(config)\n",
    "\n",
    "# 利用config对象，创建spark session\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 items\r\n",
      "drwxr-xr-x   - root supergroup           0 2019-03-26 20:22 /workspace/3.rs_project/project1/dataset/ad_feature.csv\r\n",
      "-rw-r--r--   1 root supergroup 23728773580 2019-03-26 20:35 /workspace/3.rs_project/project1/dataset/behavior_log.csv\r\n",
      "drwxr-xr-x   - root supergroup           0 2019-03-26 20:36 /workspace/3.rs_project/project1/dataset/raw_sample.csv\r\n",
      "drwxr-xr-x   - root supergroup           0 2019-03-26 20:36 /workspace/3.rs_project/project1/dataset/user_profile.csv\r\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -ls /workspace/3.rs_project/project1/dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [spark.read.csv](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=read%20csv#pyspark.sql.DataFrameReader.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----+-----+------+\n",
      "|  user|time_stamp|btag| cate| brand|\n",
      "+------+----------+----+-----+------+\n",
      "|558157|1493741625|  pv| 6250| 91286|\n",
      "|558157|1493741626|  pv| 6250| 91286|\n",
      "|558157|1493741627|  pv| 6250| 91286|\n",
      "|728690|1493776998|  pv|11800| 62353|\n",
      "|332634|1493809895|  pv| 1101|365477|\n",
      "|857237|1493816945|  pv| 1043|110616|\n",
      "|619381|1493774638|  pv|  385|428950|\n",
      "|467042|1493772641|  pv| 8237|301299|\n",
      "|467042|1493772644|  pv| 8237|301299|\n",
      "|991528|1493780710|  pv| 7270|274795|\n",
      "|991528|1493780712|  pv| 7270|274795|\n",
      "|991528|1493780712|  pv| 7270|274795|\n",
      "|991528|1493780712|  pv| 7270|274795|\n",
      "|991528|1493780714|  pv| 7270|274795|\n",
      "|991528|1493780765|  pv| 7270|274795|\n",
      "|991528|1493780714|  pv| 7270|274795|\n",
      "|991528|1493780765|  pv| 7270|274795|\n",
      "|991528|1493780764|  pv| 7270|274795|\n",
      "|991528|1493780633|  pv| 7270|274795|\n",
      "|991528|1493780764|  pv| 7270|274795|\n",
      "+------+----------+----+-----+------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- user: string (nullable = true)\n",
      " |-- time_stamp: string (nullable = true)\n",
      " |-- btag: string (nullable = true)\n",
      " |-- cate: string (nullable = true)\n",
      " |-- brand: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 从hdfs加载CSV文件为DataFrame\n",
    "df = spark.read.csv(\"hdfs://hadoop-master:9000/workspace/3.rs_project/project1/dataset/behavior_log.csv\", header=True)\n",
    "df.show()    # 查看dataframe，默认显示前20条\n",
    "# 大致查看一下数据类型\n",
    "df.printSchema()    # 打印当前dataframe的结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType\n",
    "# 构建结构对象\n",
    "schema = StructType([\n",
    "    StructField(\"userId\", IntegerType()),\n",
    "    StructField(\"timestamp\", LongType()),\n",
    "    StructField(\"btag\", StringType()),\n",
    "    StructField(\"cateId\", IntegerType()),\n",
    "    StructField(\"brandId\", IntegerType())\n",
    "])\n",
    "# 从hdfs加载数据为dataframe，并设置结构\n",
    "behavior_log_df = spark.read.csv(\"hdfs://hadoop-master:9000/workspace/3.rs_project/project1/dataset/behavior_log.csv\", header=True, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----+------+-------+\n",
      "|userId| timestamp|btag|cateId|brandId|\n",
      "+------+----------+----+------+-------+\n",
      "|558157|1493741625|  pv|  6250|  91286|\n",
      "|558157|1493741626|  pv|  6250|  91286|\n",
      "|558157|1493741627|  pv|  6250|  91286|\n",
      "|728690|1493776998|  pv| 11800|  62353|\n",
      "|332634|1493809895|  pv|  1101| 365477|\n",
      "|857237|1493816945|  pv|  1043| 110616|\n",
      "|619381|1493774638|  pv|   385| 428950|\n",
      "|467042|1493772641|  pv|  8237| 301299|\n",
      "|467042|1493772644|  pv|  8237| 301299|\n",
      "|991528|1493780710|  pv|  7270| 274795|\n",
      "|991528|1493780712|  pv|  7270| 274795|\n",
      "|991528|1493780712|  pv|  7270| 274795|\n",
      "|991528|1493780712|  pv|  7270| 274795|\n",
      "|991528|1493780714|  pv|  7270| 274795|\n",
      "|991528|1493780765|  pv|  7270| 274795|\n",
      "|991528|1493780714|  pv|  7270| 274795|\n",
      "|991528|1493780765|  pv|  7270| 274795|\n",
      "|991528|1493780764|  pv|  7270| 274795|\n",
      "|991528|1493780633|  pv|  7270| 274795|\n",
      "|991528|1493780764|  pv|  7270| 274795|\n",
      "+------+----------+----+------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "723268134"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "behavior_log_df.show()\n",
    "behavior_log_df.count()  #   7亿"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 分析数据集字段的类型和格式\n",
    "1. 查看是否有空值\n",
    "2. 查看每列数据的类型\n",
    "3. 查看每列数据的类别情况"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "查看user的数据情况： 1136340\n"
     ]
    }
   ],
   "source": [
    "print(\"查看userId的数据情况：\", behavior_log_df.groupBy(\"userId\").count().count())\n",
    "# 约113w用户\n",
    "#注意：behavior_log_df.groupBy(\"userId\").count()  返回的是一个dataframe，这里的count计算的是每一个分组的个数，但当前还没有进行计算\n",
    "# 当调用df.count()时才开始进行计算，这里的count计算的是dataframe的条目数，也就是共有多少个分组"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "查看btag的数据情况： [Row(btag='buy', count=9115919), Row(btag='fav', count=9301837), Row(btag='cart', count=15946033), Row(btag='pv', count=688904345)]\n"
     ]
    }
   ],
   "source": [
    "print(\"查看btag的数据情况：\", behavior_log_df.groupBy(\"btag\").count().collect())    # collect会把计算结果全部加载到内存，谨慎使用\n",
    "# 只有四种类型数据：pv、fav、cart、buy\n",
    "# 这里由于类型只有四个，所以直接使用collect，把数据全部加载出来"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "查看cateId的数据情况： 12968\n"
     ]
    }
   ],
   "source": [
    "print(\"查看cateId的数据情况：\", behavior_log_df.groupBy(\"cateId\").count().count())\n",
    "# 约12968类别id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "查看brandId的数据情况： 460561\n"
     ]
    }
   ],
   "source": [
    "print(\"查看brandId的数据情况：\", behavior_log_df.groupBy(\"brandId\").count().count())\n",
    "# 约460561品牌id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "判断数据是否有空值： 723268134 723268134\n"
     ]
    }
   ],
   "source": [
    "print(\"判断数据是否有空值：\", behavior_log_df.count(), behavior_log_df.dropna().count())\n",
    "# 约7亿条目723268134 723268134\n",
    "# 本数据集无空值条目，可放心处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pyspark.sql.GroupedData.pivot\n",
    "\n",
    "pivot透视操作，把某列里的字段值转换成行并进行聚合运算\n",
    "\n",
    "如果透视的字段中的不同属性值超过10000个，则需要设置spark.sql.pivotMaxValues，否则计算过程中会出现错误。[文档介绍](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=pivot#pyspark.sql.GroupedData.pivot)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- userId: integer (nullable = true)\n",
      " |-- cateId: integer (nullable = true)\n",
      " |-- pv: long (nullable = true)\n",
      " |-- fav: long (nullable = true)\n",
      " |-- cart: long (nullable = true)\n",
      " |-- buy: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 统计每个用户对各类商品的pv、fav、cart、buy数量\n",
    "cate_count_df = behavior_log_df.groupBy(behavior_log_df.userId, behavior_log_df.cateId).pivot(\"btag\",[\"pv\",\"fav\",\"cart\",\"buy\"]).count()\n",
    "cate_count_df.printSchema()    # 此时还没有开始计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+---+----+----+----+\n",
      "| userId|cateId| pv| fav|cart| buy|\n",
      "+-------+------+---+----+----+----+\n",
      "| 881748| 10650|  1|null|null|null|\n",
      "| 140219|  4520|  6|null|null|null|\n",
      "| 418392|  4267|172|   1|null|   1|\n",
      "| 570671|  6300| 38|null|null|null|\n",
      "| 537700|  4292|227|null|  21|null|\n",
      "| 366815|  6140|  7|null|null|null|\n",
      "| 542614|  4520|178|   7|   7|   2|\n",
      "| 288128|  6027|  1|null|null|null|\n",
      "|1082528|  8923|  3|   1|null|null|\n",
      "|1006609|  4520|  3|null|null|null|\n",
      "| 409886|  6244|  4|   1|null|null|\n",
      "| 209047|  4517|  1|null|null|null|\n",
      "|1016137|  6092| 10|null|   1|   1|\n",
      "|  33955|  6621|  5|null|null|null|\n",
      "| 396941|  1478| 12|   1|null|null|\n",
      "| 855789|  6428| 10|   1|   1|null|\n",
      "| 391454|  4267| 11|null|null|   1|\n",
      "| 509772|   311|  1|null|null|null|\n",
      "|  15102|  6735|  1|null|null|null|\n",
      "| 860128|  6735| 45|null|   1|   1|\n",
      "+-------+------+---+----+----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cate_count_df = behavior_log_df.groupBy(behavior_log_df.userId, behavior_log_df.cateId).pivot(\"btag\", [\"pv\",\"fav\",\"cart\",\"buy\"]).count()\n",
    "cate_count_df.show()    # 如果调用show，那么将开始执行计算，且注意这里由于pivot操作复杂，所以会比较耗时\n",
    "# 113w * 1.2w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 统计每个用户对各个品牌的pv、fav、cart、buy数量\n",
    "brand_count_df = behavior_log_df.groupBy(behavior_log_df.userId, behavior_log_df.brandId).pivot(\"btag\",[\"pv\",\"fav\",\"cart\",\"buy\"]).count()\n",
    "# brand_count_df.show()    # 同上\n",
    "# 113w * 46w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [dataframe.write.csv](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=write%20csv#pyspark.sql.DataFrameWriter.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 由于运算时间比较长，所以这里先将结果存储起来，供后续其他操作使用\n",
    "# 写入数据时才开始计算\n",
    "cate_count_df.write.csv(\"hdfs://hadoop-master:9000/workspace/3.rs_project/project1/trained_result/preprocessing-datasets/cate_count.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "brand_count_df.write.csv(\"hdfs://hadoop-master:9000/workspace/3.rs_project/project1/trained_result/preprocessing-datasets/brand_count.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "drwxr-xr-x   - root supergroup          0 2019-03-21 11:21 /workspace/3.rs_project/project1/trained_result/preprocessing-datasets/brand_count.csv\r\n",
      "drwxr-xr-x   - root supergroup          0 2019-03-21 11:22 /workspace/3.rs_project/project1/trained_result/preprocessing-datasets/cate_count.csv\r\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -ls /workspace/3.rs_project/project1/trained_result/preprocessing-datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

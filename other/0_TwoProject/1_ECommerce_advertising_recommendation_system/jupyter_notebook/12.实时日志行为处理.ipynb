{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.实时日志行为处理\n",
    "\n",
    "根据实时的用户日志行为做出相应的反馈：实时更新特征、实时更新召回集\n",
    "\n",
    "例，用户的浏览、收藏、加购物车、购买等行为记录到日志后，可能包含如下信息：\n",
    "- 时间\n",
    "- 地点\n",
    "- 用户Id\n",
    "- 商品Id\n",
    "- 类别Id\n",
    "- 品牌Id\n",
    "- 商品价格\n",
    "\n",
    "以上信息，就目前我们持有的数据而言，能产生实时影响大概只有分为两种类：\n",
    "- 对用户的基本信息产生影响的数据：地点(根据当前低点定位来判断用户的消费环境/等级)、购买行为的商品价格\n",
    "- 对用户召回结果产生影响的数据：商品的类别、品牌\n",
    "\n",
    "因此现在假设日志格式为：\"时间,地点,用户ID,商品ID,类别ID,品牌ID,商品价格\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark配置信息\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "SPARK_APP_NAME = \"processingOnlineData\"\n",
    "SPARK_URL = \"yarn\"\n",
    "\n",
    "conf = SparkConf()    # 创建spark config对象\n",
    "config = (\n",
    "\t(\"spark.app.name\", SPARK_APP_NAME),    # 设置启动的spark的app名称，没有提供，将随机产生一个名称\n",
    "\t(\"spark.executor.memory\", \"2g\"),    # 设置该app启动时占用的内存用量，默认1g\n",
    "    (\"spark.executor.cores\", \"2\"),   # 设置spark executor使用的CPU核心数\n",
    "    (\"spark.executor.instances\", 1)    # 设置spark executor数量，yarn时起作用\n",
    ")\n",
    "# 查看更详细配置及说明：https://spark.apache.org/docs/latest/configuration.html\n",
    "# \n",
    "conf.setAll(config)\n",
    "# 利用config对象，创建spark session\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 注意：初次安装并运行时，由于使用了kafka，所以会自动下载一系列的依赖jar包，会耗费一定时间\n",
    "\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "ssc = StreamingContext(spark.sparkContext, 2)\n",
    "\n",
    "kafkaParams = {\"metadata.broker.list\": \"192.168.19.137:9092\"}\n",
    "dstream = KafkaUtils.createDirectStream(ssc, [\"mytopic\"], kafkaParams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 items\r\n",
      "drwxr-xr-x   - root supergroup          0 2018-11-07 11:48 /models/CTRModel_AllOneHot.obj\r\n",
      "drwxr-xr-x   - root supergroup          0 2018-11-07 20:40 /models/CTRModel_Normal.obj\r\n",
      "drwxr-xr-x   - root supergroup          0 2018-11-09 01:04 /models/userCateRatingALSModel.obj\r\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -ls /models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### 获取广告和类别的对应关系\n",
    "# 从HDFS中加载广告基本信息数据，返回spark dafaframe对象\n",
    "df = spark.read.csv(\"hdfs://hadoop-master:9000/workspace/3.rs_project/project1/dataset/ad_feature.csv\", header=True)\n",
    "\n",
    "# 注意：由于本数据集中存在NULL字样的数据，无法直接设置schema，只能先将NULL类型的数据处理掉，然后进行类型转换\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, FloatType\n",
    "\n",
    "# 替换掉NULL字符串，替换掉\n",
    "df = df.replace(\"NULL\", \"-1\")\n",
    "\n",
    "# 更改df表结构：更改列类型和列名称\n",
    "ad_feature_df = df.\\\n",
    "    withColumn(\"adgroup_id\", df.adgroup_id.cast(IntegerType())).withColumnRenamed(\"adgroup_id\", \"adgroupId\").\\\n",
    "    withColumn(\"cate_id\", df.cate_id.cast(IntegerType())).withColumnRenamed(\"cate_id\", \"cateId\").\\\n",
    "    withColumn(\"campaign_id\", df.campaign_id.cast(IntegerType())).withColumnRenamed(\"campaign_id\", \"campaignId\").\\\n",
    "    withColumn(\"customer\", df.customer.cast(IntegerType())).withColumnRenamed(\"customer\", \"customerId\").\\\n",
    "    withColumn(\"brand\", df.brand.cast(IntegerType())).withColumnRenamed(\"brand\", \"brandId\").\\\n",
    "    withColumn(\"price\", df.price.cast(FloatType()))\n",
    "\n",
    "# 这里我们只需要adgroupId、和cateId\n",
    "_ = ad_feature_df.select(\"adgroupId\", \"cateId\")\n",
    "# 由于这里数据集其实很少，所以我们再直接转成Pandas dataframe来处理，把数据载入内存\n",
    "pdf = _.toPandas()\n",
    "\n",
    "\n",
    "# 手动释放一些内存\n",
    "del df\n",
    "del ad_feature_df\n",
    "del _\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def m(e):\n",
    "    # 当前设定日志数据格式：\"时间,地点,用户ID,商品ID,类别ID,品牌ID,商品价格\"\n",
    "    # 用逗号分割\n",
    "    return e[1].split(\",\")\n",
    "\n",
    "import redis\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "client1 = redis.StrictRedis(host=\"192.168.19.137\", port=6379, db=10)\n",
    "client2 = redis.StrictRedis(host=\"192.168.19.137\", port=6379, db=9)\n",
    "\n",
    "def f(rdd):\n",
    "    print(\"foreach\", rdd.collect())\n",
    "    for r in rdd.collect():\n",
    "        userId = r[2]\n",
    "        location_level = r[1]   # 取值范围1-4\n",
    "        \n",
    "        new_user_class_level = location_level if int(location_level) in [1,2,3,4] else None\n",
    "        data = json.loads(client1.hget(\"user_features\", userId))\n",
    "        data[\"new_user_class_level\"] = new_user_class_level    # 注意：该需求只是假设的一种情况，不一定合理\n",
    "        client1.hset(\"user_features\", userId, json.dumps(data))\n",
    "        \n",
    "        cateId = r[4]\n",
    "        ad_list = pdf.where(pdf.cateId==int(cateId)).dropna().adgroupId.astype(np.int64)\n",
    "        if ad_list.size > 0:\n",
    "            # 随机抽出当前类别50个广告，进行在线召回\n",
    "            ret = set(np.random.choice(ad_list, 50))\n",
    "            # 更新到redis 中\n",
    "            client2.sadd(userId, *ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dstream.map(m).foreachRDD(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "foreach []\n",
      "foreach []\n",
      "foreach []\n",
      "foreach []\n",
      "foreach []\n",
      "foreach []\n",
      "foreach []\n",
      "foreach []\n",
      "foreach []\n",
      "foreach []\n",
      "foreach []\n",
      "foreach []\n",
      "foreach [['时间', '4', '1', '1', '1000', '1000', '100']]\n",
      "foreach []\n",
      "foreach []\n",
      "foreach []\n",
      "foreach []\n",
      "foreach []\n",
      "foreach []\n",
      "foreach []\n",
      "foreach []\n",
      "foreach []\n",
      "foreach []\n",
      "foreach []\n",
      "foreach []\n",
      "foreach []\n",
      "foreach []\n",
      "foreach []\n",
      "foreach []\n",
      "foreach []\n",
      "foreach []\n",
      "foreach []\n",
      "foreach []\n",
      "foreach []\n",
      "foreach []\n",
      "foreach []\n",
      "foreach []\n",
      "foreach []\n",
      "foreach []\n",
      "foreach []\n"
     ]
    }
   ],
   "source": [
    "ssc.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

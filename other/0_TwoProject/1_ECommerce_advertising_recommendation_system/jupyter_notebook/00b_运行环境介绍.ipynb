{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 运行环境介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 操作系统\n",
    "\n",
    "推荐：CENTOS7，并建议配置好静态IP地址\n",
    "\n",
    "依赖配置均已Centos7为例，我所用的是虚拟机\n",
    "\n",
    "静态IP配置:`vi /etc/sysconfig/network-scripts/ifcfg-ens33`,通常是名叫ens33的网卡\n",
    "```\n",
    "TYPE=\"Ethernet\"\n",
    "PROXY_METHOD=\"none\"\n",
    "BROWSER_ONLY=\"no\"\n",
    "BOOTPROTO=\"static\"        # 改为static\n",
    "DEFROUTE=\"yes\"\n",
    "IPV4_FAILURE_FATAL=\"no\"\n",
    "IPV6INIT=\"yes\"\n",
    "IPV6_AUTOCONF=\"yes\"\n",
    "IPV6_DEFROUTE=\"yes\"\n",
    "IPV6_FAILURE_FATAL=\"no\"\n",
    "IPV6_ADDR_GEN_MODE=\"stable-privacy\"\n",
    "NAME=\"ens33\"\n",
    "UUID=\"718fadde-17ce-43ee-8c74-de8445c8c831\"\n",
    "DEVICE=\"ens33\"\n",
    "ONBOOT=\"yes\"\n",
    "IPADDR=192.168.199.88    # 添加该配置，设置当前网段的IP地址，注意不要与其他机器冲突\n",
    "GATEWAY=192.168.199.1    # 当前网络网管\n",
    "NETMASK=255.255.255.0    # 子网掩码\n",
    "DNS1=192.168.199.1    # DNS\n",
    "```\n",
    "\n",
    "另外，由于属于测试环境，一下测试全是处于防火墙关闭状态，否则需要主动放行相应的端口。\n",
    "\n",
    "关闭centos7自带防火墙firewall`service firewalld stop`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 依赖软件\n",
    "\n",
    "以下软件直接都是下载的binary版本，无需编译\n",
    "\n",
    "#### Java_JDK\n",
    "\n",
    "版本：1.8.0_191\n",
    "下载地址：[https://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html](https://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html)\n",
    "\n",
    "#### Hadoop\n",
    "\n",
    "版本：2.9.1\n",
    "\n",
    "下载地址：[https://www.apache.org/dyn/closer.cgi/hadoop/common/hadoop-2.9.1/hadoop-2.9.1.tar.gz](https://www.apache.org/dyn/closer.cgi/hadoop/common/hadoop-2.9.1/hadoop-2.9.1.tar.gz)\n",
    "\n",
    "#### Spark\n",
    "\n",
    "版本：2.2.2  （注意：version>2.2.2的话，pyspark无法实现和kafka进行交互）\n",
    "\n",
    "下载地址：[https://spark.apache.org/downloads.html](https://spark.apache.org/downloads.html)\n",
    "\n",
    "#### Kafka\n",
    "\n",
    "版本：2.0.0\n",
    "\n",
    "下载地址：[https://kafka.apache.org/downloads](https://kafka.apache.org/downloads)\n",
    "\n",
    "#### Flume\n",
    "\n",
    "版本：1.8.0\n",
    "\n",
    "下载地址：[https://flume.apache.org/download.html](https://flume.apache.org/download.html)\n",
    "\n",
    "安装：\n",
    "1. 全部`tar -xvf 解压缩到/root/bigdata/`路径下\n",
    "2. 全部`ln -s`设置软链接\n",
    "3. 将所有bin目录添加到环境变量PATH中\n",
    "\n",
    "#### Redis\n",
    "\n",
    "这里直接使用`yum -y install redis`安装的3.2版本\n",
    "\n",
    "如果需要使用高版本需要官网下载源码包，手动编译和安装，这里不做介绍"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "总用量 0\r\n",
      "drwxr-xr-x.  7 root       root       187 11月  9 18:32 apache-flume-1.8.0-bin\r\n",
      "lrwxrwxrwx.  1 root       root        36 11月  9 18:33 flume -> /root/bigdata/apache-flume-1.8.0-bin\r\n",
      "lrwxrwxrwx.  1 root       root        26 11月  3 00:55 hadoop -> /root/bigdata/hadoop-2.9.1\r\n",
      "drwxr-xr-x. 10 root       root       161 11月  3 01:01 hadoop-2.9.1\r\n",
      "lrwxrwxrwx.  1 root       root        26 11月  3 00:28 jdk -> /root/bigdata/jdk1.8.0_191\r\n",
      "drwxr-xr-x.  7         10        143 245 10月  6 20:55 jdk1.8.0_191\r\n",
      "lrwxrwxrwx.  1 root       root        30 11月  9 18:41 kafka -> /root/bigdata/kafka_2.12-2.0.0\r\n",
      "drwxr-xr-x.  7 root       root       101 11月  9 20:39 kafka_2.12-2.0.0\r\n",
      "lrwxrwxrwx.  1 root       root        39 11月  9 23:10 spark -> /root/bigdata/spark-2.2.2-bin-hadoop2.7\r\n",
      "drwxr-xr-x. 15      38823      38823 234 11月  9 23:52 spark-2.2.2-bin-hadoop2.7\r\n",
      "drwxr-xr-x. 15 1311767953 1876110778 268 11月  9 23:09 spark-2.3.0-bin-hadoop2.7\r\n"
     ]
    }
   ],
   "source": [
    "# 解压到指定目录，如/root/bigdata\n",
    "!ls /root/bigdata -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Python\n",
    "\n",
    "这里使用miniconda版本安装的Python，并利用conda 的虚拟环境创建的名为`bigdata`的环境来使用\n",
    "\n",
    "#### python第三方模块\n",
    "\n",
    "这里主要是pyspark\n",
    "\n",
    "切换到虚拟环境中：\n",
    "\n",
    "1. pyspark安装: \n",
    "```\n",
    "cd /root/bigdata/spark/python\n",
    "python setup.py install\n",
    "```\n",
    "或：`pip install pyspark==2.2.2` 但需要临时下载\n",
    "\n",
    "2. `pip install pandas numpy tables redis` 如果有其他用到，也直接安装\n",
    "\n",
    "3. `pip install jupyter`\n",
    "```\n",
    "jupyter启动：\n",
    "切换到jupyter工作目录，如我的：\n",
    "cd /root/notebook_dir/\n",
    "jupyter notebook --ip 0.0.0.0 --allow-root\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# .bashrc\r\n",
      "\r\n",
      "# User specific aliases and functions\r\n",
      "\r\n",
      "alias rm='rm -i'\r\n",
      "alias cp='cp -i'\r\n",
      "alias mv='mv -i'\r\n",
      "\r\n",
      "# Source global definitions\r\n",
      "if [ -f /etc/bashrc ]; then\r\n",
      "\t. /etc/bashrc\r\n",
      "fi\r\n",
      "\r\n",
      "# added by Miniconda2 installer\r\n",
      "export PATH=\"/root/miniconda2/bin:$PATH\"\r\n",
      "export JAVA_HOME=/root/bigdata/jdk\r\n",
      "export PATH=\"$PATH:/root/bigdata/hadoop/bin:/root/bigdata/jdk/bin:/root/bigdata/spark/bin:/root/bigdata/flume/bin\"\r\n",
      "export PYSPARK_PYTHON=\"/root/miniconda2/envs/bigdata/bin/python\"\r\n",
      "export PYSPARK_DRIVER_PYTHON=\"/root/miniconda2/envs/bigdata/bin/python\"\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "# 注意这里的几个环境变量，请注意都配置上：JAVA_HOME, PATH, PYSPARK_PYTHON, PYSPARK_DRIVER_PYTHON\n",
    "!cat /root/.bashrc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2to3\t\t\t  lzcat\t\t     spark-class\r\n",
      "2to3-3.6\t\t  lzcmp\t\t     spark-class2.cmd\r\n",
      "beeline\t\t\t  lzdiff\t     spark-class.cmd\r\n",
      "beeline.cmd\t\t  lzegrep\t     sparkR\r\n",
      "captoinfo\t\t  lzfgrep\t     sparkR2.cmd\r\n",
      "clear\t\t\t  lzgrep\t     sparkR.cmd\r\n",
      "c_rehash\t\t  lzless\t     spark-shell\r\n",
      "easy_install\t\t  lzma\t\t     spark-shell2.cmd\r\n",
      "f2py\t\t\t  lzmadec\t     spark-shell.cmd\r\n",
      "find-spark-home\t\t  lzmainfo\t     spark-sql\r\n",
      "find-spark-home.cmd\t  lzmore\t     spark-submit\r\n",
      "find_spark_home.py\t  ncursesw6-config   spark-submit2.cmd\r\n",
      "idle3\t\t\t  openssl\t     spark-submit.cmd\r\n",
      "idle3.6\t\t\t  pip\t\t     sqlite3\r\n",
      "infocmp\t\t\t  pt2to3\t     sqlite3_analyzer\r\n",
      "infotocap\t\t  ptdump\t     tabs\r\n",
      "iptest\t\t\t  ptrepack\t     tclsh\r\n",
      "iptest3\t\t\t  pttree\t     tclsh8.6\r\n",
      "ipython\t\t\t  pydoc\t\t     tic\r\n",
      "ipython3\t\t  pydoc3\t     toe\r\n",
      "jsonschema\t\t  pydoc3.6\t     tput\r\n",
      "jupyter\t\t\t  pygmentize\t     tset\r\n",
      "jupyter-bundlerextension  pyspark\t     unlzma\r\n",
      "jupyter-console\t\t  pyspark2.cmd\t     unxz\r\n",
      "jupyter-kernel\t\t  pyspark.cmd\t     wheel\r\n",
      "jupyter-kernelspec\t  python\t     wish\r\n",
      "jupyter-migrate\t\t  python3\t     wish8.6\r\n",
      "jupyter-nbconvert\t  python3.6\t     xz\r\n",
      "jupyter-nbextension\t  python3.6-config   xzcat\r\n",
      "jupyter-notebook\t  python3.6m\t     xzcmp\r\n",
      "jupyter-qtconsole\t  python3.6m-config  xzdec\r\n",
      "jupyter-run\t\t  python3-config     xzdiff\r\n",
      "jupyter-serverextension   pyvenv\t     xzegrep\r\n",
      "jupyter-troubleshoot\t  pyvenv-3.6\t     xzfgrep\r\n",
      "jupyter-trust\t\t  reset\t\t     xzgrep\r\n",
      "load-spark-env.cmd\t  run-example\t     xzless\r\n",
      "load-spark-env.sh\t  run-example.cmd    xzmore\r\n"
     ]
    }
   ],
   "source": [
    "!ls /root/miniconda2/envs/bigdata/bin    # Python版本3.6.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将`/root/bigdata/hadoop/bin`、`/root/bigdata/jdk/bin`、`/root/bigdata/spark/bin`添加到环境变量PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 启动配置\n",
    "\n",
    "#### hadoop启动配置\n",
    "\n",
    "这里是用的伪分布式模式启动，[参考配置文档](https://hadoop.apache.org/docs/r2.9.1/hadoop-project-dist/hadoop-common/SingleCluster.html)\n",
    "\n",
    "切换到`cd /root/bigdata/hadoop`\n",
    "\n",
    "`etc/hadoop/core-site.xml`\n",
    "```\n",
    "<configuration>\n",
    "    <property>\n",
    "        <name>fs.defaultFS</name>\n",
    "        <value>hdfs://0.0.0.0:9000</value>\n",
    "    </property>\n",
    "</configuration>\n",
    "```\n",
    "\n",
    "`etc/hadoop/hdfs-site.xml`\n",
    "```\n",
    "<configuration>\n",
    "\t<property>\n",
    "\t\t<name>dfs.replication</name>\n",
    "\t\t<value>1</value>\n",
    "\t</property>\n",
    "</configuration>\n",
    "```\n",
    "\n",
    "`etc/hadoop/hadoop-env.sh`，设置JAVA_HOME，如当前我这里是`/root/bigdata/jdk`\n",
    "\n",
    "`export JAVA_HOME=/root/bigdata/jdk`\n",
    "\n",
    "设置ssh localhost的免密登录\n",
    "```\n",
    "$ ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa\n",
    "$ cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys\n",
    "$ chmod 0600 ~/.ssh/authorized_keys\n",
    "```\n",
    "\n",
    "启动hadoop\n",
    "\n",
    "初次启动需先执行：\n",
    "```\n",
    "bin/hdfs namenode -format\n",
    "```\n",
    "\n",
    "使用`start-dfs.sh`启动\n",
    "```\n",
    "sbin/start-dfs.sh\n",
    "```\n",
    "使用`stop-dfs.sh`关闭\n",
    "```\n",
    "sbin/start-dfs.sh\n",
    "```\n",
    " \n",
    "启动后，可在端口50070的WEB界面中查看运行情况\n",
    "\n",
    "#### spark启动配置\n",
    "\n",
    "这里也采用伪分布式，一个Master和一个Worker\n",
    "\n",
    "切换到`cd /root/bigdata/spark`\n",
    "\n",
    "在`spark/conf/spark-env.sh`中配置`JAVA_HOME`和`SPARK_MASTER_HOST`，如：\n",
    "\n",
    "```\n",
    "export JAVA_HOME=/root/bigdata/jdk\n",
    "export SPARK_MASTER_HOST=192.168.199.88    # 192.168.199.88当前系统的IP地址， 便于外网访问所用\n",
    "```\n",
    "\n",
    "启动spark：`sbin/start-all.sh`\n",
    "\n",
    "关闭spark：`spark/sbin/stop-all.sh`\n",
    "\n",
    "启动后，可在端口8080的WEB界面中查看运行情况\n",
    "\n",
    "#### Kafka启动配置\n",
    "\n",
    "这里使用默认的单机运行模式\n",
    "\n",
    "切换到: `cd /root/bigdata/kafka`\n",
    "\n",
    "先启动自带的zookeeper: `bin/zookeeper-server-start.sh -daemon config/zookeeper.properties`\n",
    "\n",
    "再启动kafka: `bin/kafka-server-start.sh -daemon config/server.properties`\n",
    "\n",
    "#### Flume启动配置\n",
    "\n",
    "切换到'cd /root/bigdata/flume'\n",
    "\n",
    "设置配置文件：\n",
    "\n",
    "在`conf/flume-env.sh`里配置`JAVA_HOME`\n",
    "\n",
    "在`conf/flume-conf.properties`配置同日志文件到Kafka的配置\n",
    "```\n",
    "# Name the components on this agent\n",
    "a1.sources = r1\n",
    "a1.sinks = k1\n",
    "a1.channels = c1\n",
    "\n",
    "# Describe/configure the source\n",
    "a1.sources.r1.type = exec\n",
    "a1.sources.r1.command = tail -F /root/my.log\n",
    "a1.sources.r1.channels = c1\n",
    "\n",
    "# Use a channel which buffers events in memory\n",
    "a1.channels.c1.type = memory\n",
    "a1.channels.c1.capacity = 1000\n",
    "a1.channels.c1.transactionCapacity = 100\n",
    "\n",
    "a1.sinks.k1.channel = c1\n",
    "a1.sinks.k1.type = org.apache.flume.sink.kafka.KafkaSink\n",
    "a1.sinks.k1.kafka.topic = mytopic\n",
    "a1.sinks.k1.kafka.bootstrap.servers = localhost:9092\n",
    "a1.sinks.k1.kafka.flumeBatchSize = 20\n",
    "a1.sinks.k1.kafka.producer.acks = 1\n",
    "a1.sinks.k1.kafka.producer.linger.ms = 1\n",
    "a1.sinks.k1.kafka.producer.compression.type = snappy\n",
    "```\n",
    "\n",
    "启动单节点flume：`flume-ng agent -f /root/bigdata/flume/conf/flume-conf.properties -n a1` \n",
    " \n",
    "#### Redis启动配置\n",
    " \n",
    "由于可能需要外网访问，因此安装后，配置'/etc/redis/conf'，`bind 0.0.0.0`\n",
    "\n",
    "重启’service redis restart‘\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21858 SparkSubmit\n",
      "2916 Worker\n",
      "21956 CoarseGrainedExecutorBackend\n",
      "2310 NameNode\n",
      "23078 Application\n",
      "21959 CoarseGrainedExecutorBackend\n",
      "3916 QuorumPeerMain\n",
      "23183 Jps\n",
      "4538 Kafka\n",
      "2427 DataNode\n",
      "2651 SecondaryNameNode\n",
      "2812 Master\n",
      "21949 CoarseGrainedExecutorBackend\n",
      "21951 CoarseGrainedExecutorBackend\n"
     ]
    }
   ],
   "source": [
    "!jps     # 查看运行的java进程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 根据用户对品牌偏好打分训练基于ALS的矩阵分解模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 注意！注意！注意！\n",
    "\n",
    "本单元代码与前一单元代码基本一致，只不过这里是计算用户对品牌的相关偏好\n",
    "\n",
    "但是这里的数据体量为 113w * 46w的数据量，比用户对类别数据的体量大了46倍\n",
    "\n",
    "这里进行模型训练时，由于内存限制导致ALS模型训练异常，因此本单元代码仅限于在机器内存足够的前提下\n",
    "\n",
    "当前测试机器：4核心8线程 + 12GB  虚拟机\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark配置信息\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "SPARK_APP_NAME = \"createUserBrandRatingALSModel\"\n",
    "SPARK_URL = \"yarn\"\n",
    "\n",
    "conf = SparkConf()    # 创建spark config对象\n",
    "config = (\n",
    "\t(\"spark.app.name\", SPARK_APP_NAME),    # 设置启动的spark的app名称，没有提供，将随机产生一个名称\n",
    "\t(\"spark.executor.memory\", \"2g\"),    # 设置该app启动时占用的内存用量，默认1g\n",
    "\t(\"spark.master\", SPARK_URL),    # spark master的地址\n",
    "    (\"spark.executor.cores\", \"2\"),   # 设置spark executor使用的CPU核心数\n",
    "    (\"spark.executor.instances\", 1)    # 设置spark executor数量，yarn时起作用\n",
    ")\n",
    "# 查看更详细配置及说明：https://spark.apache.org/docs/latest/configuration.html\n",
    "# \n",
    "conf.setAll(config)\n",
    "\n",
    "# 利用config对象，创建spark session\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark ml的模型训练是基于内存的，如果数据过大，内存空间小，迭代次数过多的化，可能会造成内存溢出，报错\n",
    "# 设置Checkpoint的话，会把所有数据落盘，这样如果异常退出，下次重启后，可以接着上次的训练节点继续运行\n",
    "# 但该方法其实指标不治本，因为无法防止内存溢出，所以还是会报错\n",
    "# 如果数据量大，应考虑的是增加内存、或限制迭代次数和训练数据量级等\n",
    "spark.sparkContext.setCheckpointDir(\"hdfs://hadoop-master:9000/workspace/3.rs_project/project1/checkPoint/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "drwxr-xr-x   - root supergroup          0 2019-03-21 11:21 /workspace/3.rs_project/project1/trained_result/preprocessing-datasets/brand_count.csv\r\n",
      "drwxr-xr-x   - root supergroup          0 2019-03-21 11:22 /workspace/3.rs_project/project1/trained_result/preprocessing-datasets/cate_count.csv\r\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -ls /workspace/3.rs_project/project1/trained_result/preprocessing-datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"userId\", IntegerType()),\n",
    "    StructField(\"brandId\", IntegerType()),\n",
    "    StructField(\"pv\", IntegerType()),\n",
    "    StructField(\"fav\", IntegerType()),\n",
    "    StructField(\"cart\", IntegerType()),\n",
    "    StructField(\"buy\", IntegerType())\n",
    "])\n",
    "# 从hdfs加载预处理好的品牌的统计数据\n",
    "brand_count_df = spark.read.csv(\"hdfs://hadoop-master:9000/workspace/3.rs_project/project1/trained_result/preprocessing-datasets/brand_count.csv\", header=True, schema=schema)\n",
    "# brand_count_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_row(r):\n",
    "    # 处理每一行数据：r表示row对象\n",
    "    \n",
    "    # 偏好评分规则：\n",
    "\t#     m: 用户对应的行为次数\n",
    "    #     该偏好权重比例，次数上限仅供参考，具体数值应根据产品业务场景权衡\n",
    "\t#     pv: if m<=20: score=0.2*m; else score=4\n",
    "\t#     fav: if m<=20: score=0.4*m; else score=8\n",
    "\t#     cart: if m<=20: score=0.6*m; else score=12\n",
    "\t#     buy: if m<=20: score=1*m; else score=20\n",
    "    \n",
    "    # 注意这里要全部设为浮点数，spark运算时对类型比较敏感，要保持数据类型都一致\n",
    "\tpv_count = r.pv if r.pv else 0.0\n",
    "\tfav_count = r.fav if r.fav else 0.0\n",
    "\tcart_count = r.cart if r.cart else 0.0\n",
    "\tbuy_count = r.buy if r.buy else 0.0\n",
    "\n",
    "\tpv_score = 0.2*pv_count if pv_count<=20 else 4.0\n",
    "\tfav_score = 0.4*fav_count if fav_count<=20 else 8.0\n",
    "\tcart_score = 0.6*cart_count if cart_count<=20 else 12.0\n",
    "\tbuy_score = 1.0*buy_count if buy_count<=20 else 20.0\n",
    "\n",
    "\trating = pv_score + fav_score + cart_score + buy_score\n",
    "\t# 返回用户ID、品牌ID、用户对品牌的偏好打分\n",
    "\treturn r.userId, r.brandId, rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用户对品牌的打分数据\n",
    "brand_rating_df = brand_count_df.rdd.map(process_row).toDF([\"userId\", \"brandId\", \"rating\"])\n",
    "# brand_rating_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brand_rating_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 基于Spark的ALS隐因子模型进行CF评分预测\n",
    "\n",
    "ALS的意思是交替最小二乘法（Alternating Least Squares），是Spark中进行基于模型的协同过滤（model-based CF）的推荐系统算法，也是目前Spark内唯一一个推荐算法。\n",
    "\n",
    "同SVD，它也是一种矩阵分解技术，但理论上，ALS在海量数据的处理上要优于SVD。\n",
    "\n",
    "更多了解：[pyspark.ml.recommendation.ALS](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html?highlight=vectors#module-pyspark.ml.recommendation)\n",
    "\n",
    "注意：由于数据量巨大，因此这里不考虑基于内存的CF算法\n",
    "\n",
    "参考：[为什么Spark中只有ALS](https://www.cnblogs.com/mooba/p/6539142.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用pyspark中的ALS矩阵分解方法实现CF评分预测\n",
    "# 文档地址：https://spark.apache.org/docs/latest/api/python/pyspark.ml.html?highlight=vectors#module-pyspark.ml.recommendation\n",
    "from pyspark.ml.recommendation import ALS\n",
    "\n",
    "als = ALS(userCol='userId', itemCol='brandId', ratingCol='rating', checkpointInterval=2)\n",
    "# 利用打分数据，训练ALS模型\n",
    "# 此处训练时间较长\n",
    "model = als.fit(brand_rating_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.recommendForAllUsers(N) 给用户推荐TOP-N个物品\n",
    "model.recommendForAllUsers(3).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将模型进行存储\n",
    "model.save(\"hdfs://hadoop-master:9000/workspace/3.rs_project/project1/trained_result/models/userBrandRatingModel.obj\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hadoop fs -ls /models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试存储的模型\n",
    "from pyspark.ml.recommendation import ALSModel\n",
    "# 从hdfs加载模型\n",
    "my_model = ALSModel.load(\"hdfs://hadoop-master:9000/workspace/3.rs_project/project1/trained_result/models/userBrandRatingModel.obj\")\n",
    "my_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.recommendForAllUsers(N) 给用户推荐TOP-N个物品\n",
    "my_model.recommendForAllUsers(3).first()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.离线数据缓存之离线特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark配置信息\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "SPARK_APP_NAME = \"cacheOfflineFeatures\"\n",
    "SPARK_URL = \"yarn\"\n",
    "\n",
    "conf = SparkConf()    # 创建spark config对象\n",
    "config = (\n",
    "\t(\"spark.app.name\", SPARK_APP_NAME),    # 设置启动的spark的app名称，没有提供，将随机产生一个名称\n",
    "\t(\"spark.executor.memory\", \"2g\"),    # 设置该app启动时占用的内存用量，默认1g\n",
    "\t(\"spark.master\", SPARK_URL),    # spark master的地址\n",
    "    (\"spark.executor.cores\", \"2\"),   # 设置spark executor使用的CPU核心数\n",
    "    (\"spark.executor.instances\", 1)    # 设置spark executor数量，yarn时起作用\n",
    ")\n",
    "# 查看更详细配置及说明：https://spark.apache.org/docs/latest/configuration.html\n",
    "# \n",
    "conf.setAll(config)\n",
    "\n",
    "# 利用config对象，创建spark session\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"pid\", 广告资源位，属于场景特征，也就是说，每一种广告通常是可以防止在多种资源外下的\n",
    "# 因此这里对于pid，应该是由广告系统发起推荐请求时，向推荐系统明确要推荐的用户是谁，以及对应的资源位，或者说有哪些\n",
    "# 这样如果有多个资源位，那么每个资源位都会对应相应的一个推荐列表\n",
    "\n",
    "# 需要进行缓存的特征值\n",
    "    \n",
    "feature_cols_from_ad = [\n",
    "    \"price\"    # 来自广告基本信息中\n",
    "]\n",
    "\n",
    "# 用户特征\n",
    "feature_cols_from_user = [\n",
    "    \"cms_group_id\",\n",
    "    \"final_gender_code\",\n",
    "    \"age_level\",\n",
    "    \"shopping_level\",\n",
    "    \"occupation\",\n",
    "    \"pvalue_level\",\n",
    "    \"new_user_class_level\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''从HDFS中加载广告基本信息数据'''\n",
    "_ad_feature_df = spark.read.csv(\"hdfs://hadoop-master:9000/workspace/3.rs_project/project1/dataset/ad_feature.csv\", header=True)\n",
    "\n",
    "# 更改表结构，转换为对应的数据类型\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, FloatType\n",
    "\n",
    "# 替换掉NULL字符串\n",
    "_ad_feature_df = _ad_feature_df.replace(\"NULL\", \"-1\")\n",
    " \n",
    "# 更改df表结构：更改列类型和列名称\n",
    "ad_feature_df = _ad_feature_df.\\\n",
    "    withColumn(\"adgroup_id\", _ad_feature_df.adgroup_id.cast(IntegerType())).withColumnRenamed(\"adgroup_id\", \"adgroupId\").\\\n",
    "    withColumn(\"cate_id\", _ad_feature_df.cate_id.cast(IntegerType())).withColumnRenamed(\"cate_id\", \"cateId\").\\\n",
    "    withColumn(\"campaign_id\", _ad_feature_df.campaign_id.cast(IntegerType())).withColumnRenamed(\"campaign_id\", \"campaignId\").\\\n",
    "    withColumn(\"customer\", _ad_feature_df.customer.cast(IntegerType())).withColumnRenamed(\"customer\", \"customerId\").\\\n",
    "    withColumn(\"brand\", _ad_feature_df.brand.cast(IntegerType())).withColumnRenamed(\"brand\", \"brandId\").\\\n",
    "    withColumn(\"price\", _ad_feature_df.price.cast(FloatType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def foreachPartition(partition):\n",
    "    \n",
    "    import redis\n",
    "    import json\n",
    "    client = redis.StrictRedis(host=\"192.168.19.137\", port=6379, db=10)\n",
    "    \n",
    "    for r in partition:\n",
    "        data = {\n",
    "            \"price\": r.price\n",
    "        }\n",
    "        # 转成json字符串再保存，能保证数据再次倒出来时，能有效的转换成python类型\n",
    "        client.hset(\"ad_features\", r.adgroupId, json.dumps(data))\n",
    "        \n",
    "ad_feature_df.foreachPartition(foreachPartition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[userId: int, cms_segid: int, cms_group_id: int, final_gender_code: int, age_level: int, pvalue_level: int, shopping_level: int, occupation: int, new_user_class_level: int]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''从HDFS加载用户基本信息数据'''\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType, FloatType\n",
    "\n",
    "# 构建表结构schema对象\n",
    "schema = StructType([\n",
    "    StructField(\"userId\", IntegerType()),\n",
    "    StructField(\"cms_segid\", IntegerType()),\n",
    "    StructField(\"cms_group_id\", IntegerType()),\n",
    "    StructField(\"final_gender_code\", IntegerType()),\n",
    "    StructField(\"age_level\", IntegerType()),\n",
    "    StructField(\"pvalue_level\", IntegerType()),\n",
    "    StructField(\"shopping_level\", IntegerType()),\n",
    "    StructField(\"occupation\", IntegerType()),\n",
    "    StructField(\"new_user_class_level\", IntegerType())\n",
    "])\n",
    "# 利用schema从hdfs加载\n",
    "user_profile_df = spark.read.csv(\"hdfs://hadoop-master:9000/workspace/3.rs_project/project1/dataset/user_profile.csv\", header=True, schema=schema)\n",
    "user_profile_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def foreachPartition2(partition):\n",
    "    \n",
    "    import redis\n",
    "    import json\n",
    "    client = redis.StrictRedis(host=\"192.168.19.137\", port=6379, db=10)\n",
    "    \n",
    "    for r in partition:\n",
    "        data = {\n",
    "            \"cms_group_id\": r.cms_group_id,\n",
    "            \"final_gender_code\": r.final_gender_code,\n",
    "            \"age_level\": r.age_level,\n",
    "            \"shopping_level\": r.shopping_level,\n",
    "            \"occupation\": r.occupation,\n",
    "            \"pvalue_level\": r.pvalue_level,\n",
    "            \"new_user_class_level\": r.new_user_class_level\n",
    "        }\n",
    "        # 转成json字符串再保存，能保证数据再次倒出来时，能有效的转换成python类型\n",
    "        client.hset(\"user_features\", r.userId, json.dumps(data))\n",
    "        \n",
    "user_profile_df.foreachPartition(foreachPartition2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

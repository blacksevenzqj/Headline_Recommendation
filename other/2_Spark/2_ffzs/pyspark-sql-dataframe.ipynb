{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SparkSession是Spark 2.0引入的新概念。SparkSession为用户提供了统一的切入点，来让用户学习spark的各项功能。 在spark的早期版本中，SparkContext是spark的主要切入点，由于RDD是主要的API，我们通过sparkcontext来创建和操作RDD。对于每个其他的API，我们需要使用不同的context。例如，对于Streming，我们需要使用StreamingContext；对于sql，使用sqlContext；对于hive，使用hiveContext。但是随着DataSet和DataFrame的API逐渐成为标准的API，就需要为他们建立接入点。所以在spark2.0中，引入SparkSession作为DataSet和DataFrame API的切入点。SparkSession实质上是SQLContext和HiveContext的组合(未来可能还会加上StreamingContext)，所以在SQLContext和HiveContext上可用的API在SparkSession上同样是可以使用的。SparkSession内部封装了SparkContext，所以计算实际上是由SparkContext完成的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf, SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().setAppName(\"sparkApp1\").setMaster(\"local\")\n",
    "sc = SparkContext.getOrCreate(conf)\n",
    "# sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master('local').appName('sparkApp1')\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(_1='Alice', _2=1)]\n",
      "[Row(name='Alice', age=1)]\n"
     ]
    }
   ],
   "source": [
    "#createDataFrame(data, schema=None, samplingRatio=None, verifySchema=True)\n",
    "#可以由RDD, list, pandas.DataFrame 转化为DateFrame\n",
    "#当schema由列名的列表充当时，每一列的类型由data推断得出\n",
    "#当schema不存在，schema（列名和类型）将从RDD of Row，命名元组，字典中推断出来\n",
    "#当schema是pyspark.sql.types.DataType 或是数据类型的字符串，必须跟真是数据相匹配或者运行时出现异常\n",
    "#如果给定的模式不是pyspark.sql.types.StructType，他将被包装称 pyspark.sql.types.StructType\n",
    "# data from list\n",
    "l = [('Alice', 1)]\n",
    "print(spark.createDataFrame(l).collect())\n",
    "# \n",
    "print(spark.createDataFrame(l, ['name', 'age']).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ffzs/anaconda3/lib/python3.6/site-packages/pyspark/sql/session.py:340: UserWarning: inferring schema from dict is deprecated,please use pyspark.sql.Row instead\n",
      "  warnings.warn(\"inferring schema from dict is deprecated,\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(age=1, name='Alice')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  schema from dict\n",
    "d = [{'name': 'Alice', 'age': 1}]\n",
    "spark.createDataFrame(d).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(_1='Alice', _2=1)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(name='Alice', age=1)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data from rdd\n",
    "rdd = sc.parallelize(l)\n",
    "print(spark.createDataFrame(rdd).collect())\n",
    "df = spark.createDataFrame(rdd, ['name', 'age'])\n",
    "df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name='Alice', age=1)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "Person = Row('name', 'age')\n",
    "person = rdd.map(lambda r: Person(*r))\n",
    "df2 = spark.createDataFrame(person)\n",
    "df2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name='Alice', age=1)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "schema = StructType([\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True)\n",
    "])\n",
    "df3 = spark.createDataFrame(rdd, schema)\n",
    "df3.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(name='Alice', age=1)]\n",
      "[Row(0=1), Row(0=2)]\n"
     ]
    }
   ],
   "source": [
    "# from pandas.df\n",
    "import pandas\n",
    "print(spark.createDataFrame(df.toPandas()).collect())\n",
    "print(spark.createDataFrame(pandas.DataFrame([1, 2])).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(a='Alice', b=1)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.createDataFrame(rdd, \"a: string, b: int\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(value=1)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = rdd.map(lambda row: row[1])\n",
    "spark.createDataFrame(rdd, 'int').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.createDataFrame(rdd, \"boolean\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(id=1), Row(id=3), Row(id=5)]\n",
      "[Row(id=0), Row(id=1), Row(id=2)]\n"
     ]
    }
   ],
   "source": [
    "# range(start, end=None, step=1, numPartitions=None)\n",
    "# 使用单个名为id的pyspark.sql.types.LongType列创建一个DataFrame\n",
    "# 一个参数则为最终值\n",
    "print(spark.range(1, 7, 2).collect())\n",
    "print(spark.range(3).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sql(sqlQuery)\n",
    "# 返回使用sql语言操作结果的DataFrame\n",
    "# df.createOrReplaceTempView(\"table1\")\n",
    "# df2 = spark.sql(\"SELECT field1 AS f1, field2 as f2 from table1\")\n",
    "# df2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# table(tableName)\n",
    "df.createOrReplaceTempView(\"table1\")\n",
    "df2 = spark.table(\"table1\")\n",
    "sorted(df.collect()) == sorted(df2.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQLContext 使用基本同 SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(stringLengthString(test)='4')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# register(name, f, returnType=None)\n",
    "# 将函数注册为sql函数\n",
    "strlen = spark.udf.register(\"stringLengthString\", lambda x: len(x))\n",
    "spark.sql(\"SELECT stringLengthString('test')\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(stringLengthString(text)='3')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"SELECT 'foo' AS text\").select(strlen(\"text\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(stringLengthInt(test)=4)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "_ = spark.udf.register(\"stringLengthInt\", lambda x: len(x), IntegerType())\n",
    "spark.sql(\"SELECT stringLengthInt('test')\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(slen(test)=4)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# f是用户定义的函数\n",
    "# Spark使用给定用户定义函数的返回类型作为注册用户定义函数的返回类型。不应该指定returnType。在这种情况下，这个API的工作方式就像注册（name，f）一样\n",
    "# pyspark.sql.functions.udf 产生一个用户定义函数\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import udf\n",
    "slen = udf(lambda s: len(s), IntegerType())\n",
    "_ = spark.udf.register(\"slen\", slen)\n",
    "spark.sql(\"SELECT slen('test')\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# registerJavaFunction(name, javaClassName, returnType=None)\n",
    "# 定义一个用java编写的用户定义函数作为一个SQL函数\n",
    "# 除了名称和函数本身之外，还可以指定返回类型。当没有指定返回类型时，我们会通过反射来推断它。\n",
    "from pyspark.sql.types import IntegerType\n",
    "#spark.udf.registerJavaFunction(\n",
    "#    \"javaStringLength\", \"test.org.apache.spark.sql.JavaStringLength\", IntegerType())\n",
    "#spark.sql(\"SELECT javaStringLength('test')\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.udf.registerJavaFunction(\n",
    "#    \"javaStringLength2\", \"test.org.apache.spark.sql.JavaStringLength\")\n",
    "#spark.sql(\"SELECT javaStringLength2('test')\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# registerJavaUDAF(name, javaClassName)\n",
    "#　将java用户自己定义的函数作为sql函数\n",
    "#df = spark.udf.registerJavaUDAF(\"javaUDAF\", \"test.org.spark.MyDoubleAvg\")\n",
    "#df.createOrReplaceTempView(\"df\")\n",
    "#spark.sql(\"SELECT name, javaUFAD(id) as avg from df group by name\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('_c0', 'string'),\n",
       " ('_c1', 'string'),\n",
       " ('_c2', 'string'),\n",
       " ('_c3', 'string'),\n",
       " ('_c4', 'string'),\n",
       " ('_c5', 'string'),\n",
       " ('_c6', 'string'),\n",
       " ('_c7', 'string'),\n",
       " ('_c8', 'string'),\n",
       " ('_c9', 'string')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pyspark.sql.DaraFrameReader\n",
    "# csv(path, schema=None, sep=None, encoding=None, quote=None, escape=None, comment=None, header=None, inferSchema=None, ignoreLeadingWhiteSpace=None, ignoreTrailingWhiteSpace=None, nullValue=None, nanValue=None, positiveInf=None, negativeInf=None, dateFormat=None, timestampFormat=None, maxColumns=None, maxCharsPerColumn=None, \\\n",
    "# maxMalformedLogPerPartition=None, mode=None, columnNameOfCorruptRecord=None, multiLine=None, charToEscapeQuoteEscaping=None)\n",
    "# inferSchema inferSchema - 从数据中自动推断输入模式。它需要额外的数据传递。如果设置为None，则使用默认值false。\n",
    "# header 使用第一行作为列的名称。如果设置为None，则使用默认值false。\n",
    "df = spark.read.csv('HR.csv')\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('satisfaction_level', 'string'),\n",
       " ('last_evaluation', 'string'),\n",
       " ('number_project', 'string'),\n",
       " ('average_montly_hours', 'string'),\n",
       " ('time_spend_company', 'string'),\n",
       " ('Work_accident', 'string'),\n",
       " ('left', 'string'),\n",
       " ('promotion_last_5years', 'string'),\n",
       " ('sales', 'string'),\n",
       " ('salary', 'string')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = spark.read.csv('HR.csv', header=True)\n",
    "df1.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('satisfaction_level', 'double'),\n",
       " ('last_evaluation', 'double'),\n",
       " ('number_project', 'int'),\n",
       " ('average_montly_hours', 'int'),\n",
       " ('time_spend_company', 'int'),\n",
       " ('Work_accident', 'int'),\n",
       " ('left', 'int'),\n",
       " ('promotion_last_5years', 'int'),\n",
       " ('sales', 'string'),\n",
       " ('salary', 'string')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2= spark.read.csv('HR.csv', header=True, inferSchema=True)\n",
    "df2.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('_c0', 'string'),\n",
       " ('_c1', 'string'),\n",
       " ('_c2', 'string'),\n",
       " ('_c3', 'string'),\n",
       " ('_c4', 'string'),\n",
       " ('_c5', 'string'),\n",
       " ('_c6', 'string'),\n",
       " ('_c7', 'string'),\n",
       " ('_c8', 'string'),\n",
       " ('_c9', 'string')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.textFile(\"HR.csv\")\n",
    "df2 = spark.read.csv(rdd)\n",
    "df2.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('value', 'string')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# format(source)\n",
    "# 指定数据源格式\n",
    "# load(path=None, format=None, schema=None, **options)\n",
    "# 从数据源加载数据并将其作为 DataFrame 返回\n",
    "df = spark.read.format('text').load(\"book.txt\")\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# json(path, schema=None, primitivesAsString=None, prefersDecimal=None, allowComments=None, allowUnquotedFieldNames=None, allowSingleQuotes=None, allowNumericLeadingZero=None, allowBackslashEscapingAnyCharacter=None, mode=None, columnNameOfCorruptRecord=None, dateFormat=None, timestampFormat=None, multiLine=None, allowUnquotedControlChars=None）\n",
    "# 加载json数据作为dataframe\n",
    "# 用法基本同csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JDBC（Java DataBase Connectivity,java数据库连接）\n",
    "# 通过url访问jdbc table 构建 dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(value='1'), Row(value='2'), Row(value='3')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# text(paths, wholetext=False)\n",
    "# 加载文本文件并返回一个DataFrame，该DataFrame的架构以名为“value”的字符串列开始，如果存在任何分区列，则返回分区列。\n",
    "#　文本文件中的每一行都是生成的DataFrame中的新行。\n",
    "df = spark.read.text('example.txt')\n",
    "df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(value='1\\n2\\n3\\n')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.read.text('example.txt', wholetext=True)\n",
    "df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# orc(path)\n",
    "# hive 相关格式加载成dataframe目前ORC支持仅与Hive支持一起提供。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parquet(*paths)\n",
    "# Parquet是Hadoop上的一种支持列式存储文件格式。\n",
    "# mergeSchema：设置我们是否应合并从所有Parquet零件文件收集的模式。这将覆盖spark.sql.parquet.mergeSchema。缺省值在spark.sql.parquet.mergeSchema中指定。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# schema(schema)\n",
    "# 指定输入模式\n",
    "# 某些数据源（例如JSON）可以根据数据自动推断输入模式。通过在这里指定模式，底层数据源可以跳过模式推断步骤，从而加速数据加载。\n",
    "s = spark.read.schema(\"col0 INT, col1 DOUBLE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# table(tableNmae)\n",
    "# 以dataframe的形式返回指定的表"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## class pyspark.sql.DataFrameWriter(df)\n",
    "\n",
    "用于将DataFrame写入外部存储系统（例如文件系统，键值存储等）的接口。使用DataFrame.write（）来访问它\n",
    "read()的反向函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import os\n",
    "df.write.csv(os.path.join(tempfile.mkdtemp(), 'data'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/tmp/tmpao3i7i5b/data'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.join(tempfile.mkdtemp(), 'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"hdfs://0.0.0.0:9000/test/input\"\n",
    "df = spark.read.csv('HR.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hdfs://0.0.0.0:9000/test/input/HR'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.join(path,'HR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---------------+--------------+--------------------+------------------+-------------+----+--------------------+-----+------+\n",
      "|               _c0|            _c1|           _c2|                 _c3|               _c4|          _c5| _c6|                 _c7|  _c8|   _c9|\n",
      "+------------------+---------------+--------------+--------------------+------------------+-------------+----+--------------------+-----+------+\n",
      "|satisfaction_level|last_evaluation|number_project|average_montly_hours|time_spend_company|Work_accident|left|promotion_last_5y...|sales|salary|\n",
      "|              0.38|           0.53|             2|                 157|                 3|            0|   1|                   0|sales|   low|\n",
      "+------------------+---------------+--------------+--------------------+------------------+-------------+----+--------------------+-----+------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = spark.read.json(os.path.join(path,'people.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  24|    Jim|\n",
      "|  22| Justin|\n",
      "|  22|   Mike|\n",
      "|  25|   Dick|\n",
      "|  26|Johnson|\n",
      "|  22|   John|\n",
      "|  27|  Cindy|\n",
      "|  23|  Windy|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## class pyspark.sql.DataFrame(jdf,sql_ctx)\n",
    "分组到已命名列中的分布式数据集合。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---------------+--------------+--------------------+------------------+-------------+----+---------------------+-----+------+\n",
      "|satisfaction_level|last_evaluation|number_project|average_montly_hours|time_spend_company|Work_accident|left|promotion_last_5years|sales|salary|\n",
      "+------------------+---------------+--------------+--------------------+------------------+-------------+----+---------------------+-----+------+\n",
      "|              0.38|           0.53|             2|                 157|                 3|            0|   1|                    0|sales|   low|\n",
      "|               0.8|           0.86|             5|                 262|                 6|            0|   1|                    0|sales|medium|\n",
      "+------------------+---------------+--------------+--------------------+------------------+-------------+----+---------------------+-----+------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('satisfaction_level', 'double'),\n",
       " ('last_evaluation', 'double'),\n",
       " ('number_project', 'int'),\n",
       " ('average_montly_hours', 'int'),\n",
       " ('time_spend_company', 'int'),\n",
       " ('Work_accident', 'int'),\n",
       " ('left', 'int'),\n",
       " ('promotion_last_5years', 'int'),\n",
       " ('sales', 'string'),\n",
       " ('salary', 'string')]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df0 = spark.read.csv('HR.csv', header=True, inferSchema=True)\n",
    "df0.show(2)\n",
    "df0.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# change typoe\n",
    "# df = df.withColumn('average_montly_hours', df.average_montly_hours.cast('float'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(min(average_montly_hours)=96)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# agg(*exprs)\n",
    "# 在dataframe上集合不通过groups（是df.group.agg()的简写)\n",
    "df0.agg({\"average_montly_hours\":\"min\"}).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(max(average_montly_hours)=310)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "df0.agg(F.max(df0.average_montly_hours)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---------------+--------------+--------------------+------------------+-------------+----+---------------------+-----+------+------------------+---------------+--------------+--------------------+------------------+-------------+----+---------------------+-----+------+\n",
      "|satisfaction_level|last_evaluation|number_project|average_montly_hours|time_spend_company|Work_accident|left|promotion_last_5years|sales|salary|satisfaction_level|last_evaluation|number_project|average_montly_hours|time_spend_company|Work_accident|left|promotion_last_5years|sales|salary|\n",
      "+------------------+---------------+--------------+--------------------+------------------+-------------+----+---------------------+-----+------+------------------+---------------+--------------+--------------------+------------------+-------------+----+---------------------+-----+------+\n",
      "|              0.38|           0.53|             2|                 157|                 3|            0|   1|                    0|sales|   low|              0.43|           0.46|             2|                 157|                 3|            0|   1|                    0|sales|medium|\n",
      "+------------------+---------------+--------------+--------------------+------------------+-------------+----+---------------------+-----+------+------------------+---------------+--------------+--------------------+------------------+-------------+----+---------------------+-----+------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# alias()\n",
    "# 复制dataframe\n",
    "from pyspark.sql.functions import *\n",
    "df_as1 = df0.alias('df_as1')\n",
    "df_as2 = df0.alias('df_as2')\n",
    "joined_df = df_as1.join(df_as2, col(\"df_as1.average_montly_hours\") == col(\"df_as2.average_montly_hours\"), 'inner')\n",
    "joined_df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# approxQuantile(col, probabilities, relativeError)\n",
    "# df0.approxQuantile('left',(0.5,1) , 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df0.coalesce(1).rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|Col1|\n",
      "+----+\n",
      "|   a|\n",
      "|   b|\n",
      "|   c|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# colRegex(colName)\n",
    "# 根据正则选取列名\n",
    "df = spark.createDataFrame([(\"a\", 1), (\"b\", 2), (\"c\",  3)], [\"Col1\", \"Col2\"])\n",
    "df.select(df.colRegex(\"`(Col2)?+.+`\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['satisfaction_level',\n",
       " 'last_evaluation',\n",
       " 'number_project',\n",
       " 'average_montly_hours',\n",
       " 'time_spend_company',\n",
       " 'Work_accident',\n",
       " 'left',\n",
       " 'promotion_last_5years',\n",
       " 'sales',\n",
       " 'salary']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# columns返回列名 lsit\n",
    "df0.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14482217493938632"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#corr(col1, col2, method=None)\n",
    "#计算一个DataFrame的两列的相关性作为一个双值。目前只支持皮尔逊相关系数。DataFrame.corr（）和DataFrameStatFunctions.corr（）是彼此的别名。\n",
    "#皮尔逊相关:是用于度量两个变量X和Y之间的相关（线性相关），其值介于-1与1之间 越接近正负1,相关度越大\n",
    "df0.corr(\"time_spend_company\", \"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09006595461255852"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cov()\n",
    "# 协方差,观察两列相关性 大于0:正相关 等于0:不相关 小于0:负相关\n",
    "df0.cov(\"time_spend_company\", \"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14999"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count()\n",
    "# 返回这个DataFrame中的行数。\n",
    "df0.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---------------+--------------+--------------------+------------------+-------------+----+---------------------+-----+------+\n",
      "|satisfaction_level|last_evaluation|number_project|average_montly_hours|time_spend_company|Work_accident|left|promotion_last_5years|sales|salary|\n",
      "+------------------+---------------+--------------+--------------------+------------------+-------------+----+---------------------+-----+------+\n",
      "|              0.38|           0.53|             2|                 157|                 3|            0|   1|                    0|sales|   low|\n",
      "|               0.8|           0.86|             5|                 262|                 6|            0|   1|                    0|sales|medium|\n",
      "+------------------+---------------+--------------+--------------------+------------------+-------------+----+---------------------+-----+------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# createGlobalTempView(name)\n",
    "# 如果你想拥有一个临时的view不光可以在不同的Session中共享，而且在application的运行周期内可用，那么就需要创建一个全局的临时view。并记得使用的时候加上global_temp作为前缀，因为全局的临时view是绑定到系统保留的数据库global_temp上。\n",
    "df0.createGlobalTempView(\"HR\")\n",
    "df2 = spark.sql(\"select * from global_temp.HR\")\n",
    "df2.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df2.createGlobalTempView('HR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.dropGlobalTempView(\"HR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# createOrReplaceGlobalTempView(name)\n",
    "# 使用给定的名称创建或替换全局临时视图。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# createTempView(name)\n",
    "# 用这个DataFrame创建一个本地临时视图。\n",
    "# 这个临时表的生命周期与用来创建这个DataFrame的SparkSession绑定在一起。如果视图名称已经存在于目录中，则抛出temptablealready存在异常。\n",
    "df0.createTempView(\"HR\")\n",
    "df2 = spark.sql(\"select * from HR\")\n",
    "sorted(df0.collect()) == sorted(df2.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 删除本地视图\n",
    "spark.catalog.dropTempView('HR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crossJoin(other)\n",
    "# 用另一个dataframe产生笛卡尔积\n",
    "# df.crossJoin(df2.select(\"height\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----+-----+\n",
      "|number_project|left|count|\n",
      "+--------------+----+-----+\n",
      "|          null|null|14999|\n",
      "|          null|   0|11428|\n",
      "|          null|   1| 3571|\n",
      "|             2|null| 2388|\n",
      "|             2|   0|  821|\n",
      "|             2|   1| 1567|\n",
      "|             3|null| 4055|\n",
      "|             3|   0| 3983|\n",
      "|             3|   1|   72|\n",
      "|             4|null| 4365|\n",
      "|             4|   0| 3956|\n",
      "|             4|   1|  409|\n",
      "|             5|null| 2761|\n",
      "|             5|   0| 2149|\n",
      "|             5|   1|  612|\n",
      "|             6|null| 1174|\n",
      "|             6|   0|  519|\n",
      "|             6|   1|  655|\n",
      "|             7|null|  256|\n",
      "|             7|   1|  256|\n",
      "+--------------+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# cube(*col)\n",
    "# 使用指定的列创建当前DataFrame的多维多维数据集，因此我们可以在它们上运行聚合。\n",
    "df0.cube(df0.number_project, df0.left).count().orderBy('number_project', 'left').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-------------------+\n",
      "|summary|    number_project|               left|\n",
      "+-------+------------------+-------------------+\n",
      "|  count|             14999|              14999|\n",
      "|   mean|  3.80305353690246| 0.2380825388359224|\n",
      "| stddev|1.2325923553183513|0.42592409938029885|\n",
      "|    min|                 2|                  0|\n",
      "|    max|                 7|                  1|\n",
      "+-------+------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# describe(*cols)\n",
    "# 计算数字和字符串列的基本统计信息。\n",
    "# 这包括count，mean，stddev，min和max。如果未给出列，则此函数将计算所有数字或字符串列的统计信息。\n",
    "df0.describe(['number_project', 'left']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11991"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# distinct()\n",
    "# 删除重复行\n",
    "df0.distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop(*cols)\n",
    "# 返回一个新的DataFrame掉落指定的列。如果模式不包含给定的列名（s），那么这是一个no-op。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+\n",
      "|age|height| name|\n",
      "+---+------+-----+\n",
      "|  5|    80|Alice|\n",
      "|  5|    80|  Bob|\n",
      "| 10|    80|Alice|\n",
      "+---+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# dropDuplicates(subset=None)\n",
    "# 返回一个新的DataFrame，去掉重复的行，可选地只考虑某些列。\n",
    "# 对于静态批处理DataFrame，它只是删除重复的行。对于流式DataFrame，它将把所有数据跨触发器保存为中间状态，以删除重复的行。您可以使用withWatermark()来限制重复数据的延迟，系统将相应地限制状态。此外，为了避免重复的可能性，将会删除比水印更早的数据。\n",
    "# 同drop_duplicates()\n",
    "from pyspark.sql import Row\n",
    "df = sc.parallelize([ \\\n",
    "    Row(name='Alice', age=5, height=80), \\\n",
    "    Row(name='Bob', age=5, height=80), \\\n",
    "    Row(name='Alice', age=10, height=80)]).toDF()\n",
    "df.dropDuplicates().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+\n",
      "|age|height| name|\n",
      "+---+------+-----+\n",
      "|  5|    80|Alice|\n",
      "+---+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.drop_duplicates(['name', 'height']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+-----+\n",
      "| age|height| name|\n",
      "+----+------+-----+\n",
      "|  10|    80|alice|\n",
      "|   5|  null|  bob|\n",
      "|null|  null|  tom|\n",
      "|null|  null| null|\n",
      "+----+------+-----+\n",
      "\n",
      "+---+------+-----+\n",
      "|age|height| name|\n",
      "+---+------+-----+\n",
      "| 10|    80|alice|\n",
      "+---+------+-----+\n",
      "\n",
      "+----+------+-----+\n",
      "| age|height| name|\n",
      "+----+------+-----+\n",
      "|  10|    80|alice|\n",
      "|   5|  null|  bob|\n",
      "|null|  null|  tom|\n",
      "+----+------+-----+\n",
      "\n",
      "+---+------+-----+\n",
      "|age|height| name|\n",
      "+---+------+-----+\n",
      "| 10|    80|alice|\n",
      "|  5|  null|  bob|\n",
      "+---+------+-----+\n",
      "\n",
      "+----+------+-----+\n",
      "| age|height| name|\n",
      "+----+------+-----+\n",
      "|  10|    80|alice|\n",
      "|   5|  null|  bob|\n",
      "|null|  null|  tom|\n",
      "+----+------+-----+\n",
      "\n",
      "+----+------+-----+\n",
      "| age|height| name|\n",
      "+----+------+-----+\n",
      "|  10|    80|alice|\n",
      "|   5|  null|  bob|\n",
      "|null|  null|  tom|\n",
      "+----+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# dropna(how='any', thresh=None, subset=None)\n",
    "#返回一个新的DataFrame来省略带有null值的行。\n",
    "#dataframe.dropna（）和dataFrameNafuntions.drop（）是彼此的别名。\n",
    "#how: any:有一个就删除 all:全是才删除\n",
    "#thresh: 设置超过多少个na删除行,会重置how\n",
    "#subset: 对选定列名\n",
    "df3 = spark.read.csv(os.path.join(path, \"null.csv\"), header=True, inferSchema=True)\n",
    "df3.show()\n",
    "df3.dropna().show()\n",
    "# drop全为null的行\n",
    "df3.dropna(\"all\").show()\n",
    "# drop多于2的行\n",
    "df3.na.drop(thresh=2).show()\n",
    "# drop掉name列为null的行\n",
    "df3.na.drop(subset=[\"name\"]).show()\n",
    "df3.na.drop(\"all\", subset=(\"name\", \"height\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+\n",
      "|age|height| name|\n",
      "+---+------+-----+\n",
      "| 10|    80|alice|\n",
      "|  5|    50|  bob|\n",
      "| 50|    50|  tom|\n",
      "| 50|    50| null|\n",
      "+---+------+-----+\n",
      "\n",
      "+----+------+-----+\n",
      "| age|height| name|\n",
      "+----+------+-----+\n",
      "|  10|    80|alice|\n",
      "|   5|  null|  bob|\n",
      "|null|  null|  tom|\n",
      "|null|  null| null|\n",
      "+----+------+-----+\n",
      "\n",
      "+---+------+------+\n",
      "|age|height|  name|\n",
      "+---+------+------+\n",
      "| 10|    80| alice|\n",
      "|  5|  null|   bob|\n",
      "| 50|  null|   tom|\n",
      "| 50|  null|unknow|\n",
      "+---+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# fillna(value, subset=None)\n",
    "# 替换na.fill（）的空值，别名。 DataFrame.fillna（）和DataFrameNaFunctions.fill（）是彼此的别名。\n",
    "# value:int，long，float，string，bool或dict。用来替换空值的值。如果值是字典，则子集将被忽略，并且值必须是从列名（字符串）到替换值的映射。\n",
    "# subset:可选的列名称要考虑。子集中指定的不具有匹配数据类型的列将被忽略。例如，如果value是一个字符串，并且子集包含一个非字符串列，则非字符串列将被忽略。\n",
    "df3.na.fill(50).show()\n",
    "df3.na.fill(False).show()\n",
    "df3.na.fill({'age':50, 'name': 'unknow'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(age=10, height=80, name='Alice')]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter(condition)\n",
    "# 使用给定的条件过滤行。\n",
    "# 同 where()\n",
    "df3.filter(df3.age > 3).collect()\n",
    "df.where(df.age >= 10).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(age=5, height=80, name='Alice')"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first()\n",
    "# 返回第一行\n",
    "df.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# foreach(f)\n",
    "# 对dataframe的每一行应用f函数\n",
    "# 是df.rdd.foreach()的缩写\n",
    "def f(person):\n",
    "    print(person.name)\n",
    "df.foreach(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# foreachPartition(f)\n",
    "# 用法同foreach()只不过是作用于每个分区\n",
    "# def f(people):\n",
    "#     for person in people:\n",
    "#         peint(person.name)\n",
    "# df.foreachPartition(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(avg(age)=6.666666666666667, avg(height)=80.0)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(name='Alice', avg(age)=6.666666666666667)]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# groupby()\n",
    "# 使用指定的列对DataFrame进行分组，因此我们可以对它们运行聚合。有关所有可用聚合函数，请参阅分组数据。\n",
    "# 同groupBy()\n",
    "print(df.groupBy().avg().collect())\n",
    "sorted(df.groupBy('name').agg({'age': 'mean'}).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name='Alice', avg(age)=6.666666666666667, avg(height)=80.0)]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(df.groupBy(df.name).avg().collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name='Alice', age=5, count=2), Row(name='Alice', age=10, count=1)]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(df.groupBy([\"name\", df.age]).count().collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(age=5, height=80, name='Alice'), Row(age=5, height=80, name='Alice')]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# head(n=None)\n",
    "# 返回前n行\n",
    "# 所有数据将被加载到内存,所以数组很小时使用\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intersect(other)\n",
    "# 返回一个新的dataframe包含相同的行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+-----+\n",
      "| age|height| name|\n",
      "+----+------+-----+\n",
      "|  10|    80|alice|\n",
      "|   5|  null|  bob|\n",
      "|null|  null|  tom|\n",
      "|null|  null| null|\n",
      "+----+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name=None, height=None),\n",
       " Row(name=None, height=80),\n",
       " Row(name='Alice', height=None),\n",
       " Row(name='Alice', height=None),\n",
       " Row(name='Alice', height=None),\n",
       " Row(name=None, height=None),\n",
       " Row(name=None, height=None)]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# join(other, on=None, how=None)\n",
    "# 使用给定的连接表达式与另一个DataFrame进行连接。\n",
    "# on: join条件\n",
    "# how: join的方式inner, cross, outer, full, full_outer, left, left_outer, right, right_outer, left_semi, and left_anti.\n",
    "df.join(df3, df.name == df3.name, 'outer').select(df.name, df3.height).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name=None, age=5),\n",
       " Row(name=None, age=None),\n",
       " Row(name=None, age=10),\n",
       " Row(name='Alice', age=None),\n",
       " Row(name=None, age=None),\n",
       " Row(name='Alice', age=None),\n",
       " Row(name='Alice', age=None)]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cond = [df.name == df3.name, df.age == df3.age]\n",
    "df.join(df3, cond, 'outer').select(df.name, df3.age).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.join(df3, 'name').select(df.name, df3.height).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(age=5, height=80, name='Alice')]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# limit(num)\n",
    "# 将结果技术限制为制定的数字\n",
    "df.limit(1).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(age=10, height=80, name='Alice'),\n",
       " Row(age=5, height=80, name='Alice'),\n",
       " Row(age=5, height=80, name='Alice')]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# orderBy()\n",
    "# 返回按指定列排序的新DataFrame。\n",
    "# ascending:布尔值或布尔值列表（默认值为True）。按升序排列与降序排列。指定多个排序顺序的列表。如果指定了列表，则列表的长度必须等于列的长度。\n",
    "df.sort(df.age.desc()).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(age=10, height=80, name='Alice'),\n",
       " Row(age=5, height=80, name='Alice'),\n",
       " Row(age=5, height=80, name='Alice')]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sort(\"age\", ascending=False).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(age=10, height=80, name='Alice'),\n",
       " Row(age=5, height=80, name='Alice'),\n",
       " Row(age=5, height=80, name='Alice')]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.orderBy(df.age.desc()).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(age=5, height=80, name='Alice'),\n",
       " Row(age=5, height=80, name='Alice'),\n",
       " Row(age=10, height=80, name='Alice')]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "df.sort(asc('age')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(age=10, height=80, name='Alice'),\n",
       " Row(age=5, height=80, name='Alice'),\n",
       " Row(age=5, height=80, name='Alice')]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.orderBy(desc(\"age\"), \"name\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(age=10, height=80, name='Alice'),\n",
       " Row(age=5, height=80, name='Alice'),\n",
       " Row(age=5, height=80, name='Bob')]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.orderBy([\"age\", \"name\"], ascending=[False, True]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- height: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# printSchema()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.988614055751865\n",
      "2.900274833137024\n"
     ]
    }
   ],
   "source": [
    "# randomSplit(weights, seed=None)\n",
    "# 根据比重随机分配dataframe\n",
    "# weights: 如果和不是1的话,会被标准话为1\n",
    "split = df0.randomSplit([1.0, 2.0, 3.0], 10)\n",
    "for item in split:\n",
    "    print(item.count()/split[0].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# repartition(numPartitions, *cols)\n",
    "# 返回由给定分区表达式分区的新DataFrame。生成的DataFrame是散列分区的。\n",
    "# numPartitions可以是一个int值，用于指定分区或列的目标数量。如果它是一个列，它将被用作第一个分区列。如果未指定，则使用默认的分区数量。\n",
    "df.repartition(10).rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+\n",
      "|age|height| name|\n",
      "+---+------+-----+\n",
      "|  5|    80|Alice|\n",
      "|  5|    80|  Bob|\n",
      "| 10|    80|Alice|\n",
      "|  5|    80|Alice|\n",
      "|  5|    80|  Bob|\n",
      "| 10|    80|Alice|\n",
      "+---+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = df.union(df)\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+\n",
      "|age|height| name|\n",
      "+---+------+-----+\n",
      "|  5|    80|Alice|\n",
      "|  5|    80|  Bob|\n",
      "|  5|    80|Alice|\n",
      "|  5|    80|  Bob|\n",
      "| 10|    80|Alice|\n",
      "| 10|    80|Alice|\n",
      "+---+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = df.union(df).repartition('age')\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+\n",
      "|age|height| name|\n",
      "+---+------+-----+\n",
      "|  5|    80|Alice|\n",
      "|  5|    80|  Bob|\n",
      "| 10|    80|Alice|\n",
      "|  5|    80|Alice|\n",
      "|  5|    80|  Bob|\n",
      "| 10|    80|Alice|\n",
      "+---+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = data.repartition(7, \"age\")\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+\n",
      "|age|height| name|\n",
      "+---+------+-----+\n",
      "|  5|    80|  Bob|\n",
      "|  5|    80|  Bob|\n",
      "| 10|    80|Alice|\n",
      "| 10|    80|Alice|\n",
      "|  5|    80|Alice|\n",
      "|  5|    80|Alice|\n",
      "+---+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = data.repartition(\"name\", \"age\")\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+-----+\n",
      "| age|height| name|\n",
      "+----+------+-----+\n",
      "|  10|    80|alice|\n",
      "|   5|  null|  bob|\n",
      "|null|  null|  tom|\n",
      "|null|  null| null|\n",
      "+----+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# replace(to_replace, value=<no value>, subset=None)\n",
    "# 返回一个新的DataFrame，用另一个值替换一个值。 DataFrame.replace（）和DataFrameNaFunctions.replace（）是彼此的别名。\n",
    "# 替换类型相同\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+-----+\n",
      "| age|height| name|\n",
      "+----+------+-----+\n",
      "|  10|    80|alice|\n",
      "|  10|  null|  bob|\n",
      "|null|  null|  tom|\n",
      "|null|  null| null|\n",
      "+----+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.na.replace(5, 10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+----+\n",
      "| age|height|name|\n",
      "+----+------+----+\n",
      "|  10|    80|null|\n",
      "|   5|  null| bob|\n",
      "|null|  null| tom|\n",
      "|null|  null|null|\n",
      "+----+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.replace('alice', None).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+----+\n",
      "| age|height|name|\n",
      "+----+------+----+\n",
      "|  10|    80|null|\n",
      "|   5|  null| bob|\n",
      "|null|  null| tom|\n",
      "|null|  null|null|\n",
      "+----+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.replace({'alice': None}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+----+\n",
      "| age|height|name|\n",
      "+----+------+----+\n",
      "|  10|    80|   a|\n",
      "|   5|  null|   b|\n",
      "|null|  null| tom|\n",
      "|null|  null|null|\n",
      "+----+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.replace(['alice', 'bob'], ['a', 'b'], 'name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5012334155610374"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample(withReplacement=None, fraction=None, seed=None)\n",
    "# 返回此DataFrame的采样子集。\n",
    "# withReplacement: 是否重复采样\n",
    "# fraction: 采样占比范围[0.0, 1.0],大概\n",
    "# 种子是必须的\n",
    "df0.sample(0.5, 3).count()/df0.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7402"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df0.sample(True, 0.5, 3).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|key|count|\n",
      "+---+-----+\n",
      "|  0|    3|\n",
      "|  1|    7|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sampleBy(col, fractions, seed=None)\n",
    "# 根据每层中给出的分数返回未更换的分层样本。\\\n",
    "from pyspark.sql.functions import col\n",
    "dataset = spark.range(0, 100).select((col(\"id\") % 3).alias(\"key\"))\n",
    "sampled = dataset.sampleBy(\"key\", fractions={0: 0.1, 1: 0.2}, seed=0)\n",
    "sampled.groupBy(\"key\").count().orderBy(\"key\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(age,LongType,true),StructField(height,LongType,true),StructField(name,StringType,true)))"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# schema\n",
    "# 以pyspark.sql.types.StructType的形式返回此DataFrame的模式。\n",
    "df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+\n",
      "|age|height| name|\n",
      "+---+------+-----+\n",
      "|  5|    80|Alice|\n",
      "|  5|    80|  Bob|\n",
      "| 10|    80|Alice|\n",
      "+---+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# select(*cols)\n",
    "# 投影一组表达式并返回一个新的DataFrame。\n",
    "df.select('*').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "| name|age|\n",
      "+-----+---+\n",
      "|Alice|  5|\n",
      "|  Bob|  5|\n",
      "|Alice| 10|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('name', 'age').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+\n",
      "| name| age|\n",
      "+-----+----+\n",
      "|Alice|15.0|\n",
      "|  Bob|15.0|\n",
      "|Alice|20.0|\n",
      "+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.name, (df.age + 10).alias('age').cast('float')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+\n",
      "|(age * 2)|abs(age)|\n",
      "+---------+--------+\n",
      "|       10|       5|\n",
      "|       10|       5|\n",
      "|       20|      10|\n",
      "+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# selectExpr(*expr)\n",
    "# 投影一组SQL表达式并返回一个新的DataFrame。\n",
    "# 这是接受SQL表达式的select（）的变体。\n",
    "df.selectExpr(\"age * 2\", \"abs(age)\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-------\n",
      " age    | 5     \n",
      " height | 80    \n",
      " name   | Alice \n",
      "-RECORD 1-------\n",
      " age    | 5     \n",
      " height | 80    \n",
      " name   | Bob   \n",
      "-RECORD 2-------\n",
      " age    | 10    \n",
      " height | 80    \n",
      " name   | Alice \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show(n=20, truncate=True, vertical=False)\n",
    "# 将前n行打印给控制台\n",
    "df.show(vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+\n",
      "|age|height| name|\n",
      "+---+------+-----+\n",
      "| 10|    80|Alice|\n",
      "|  5|    80|Alice|\n",
      "|  5|    80|  Bob|\n",
      "+---+------+-----+\n",
      "\n",
      "+---+------+-----+\n",
      "|age|height| name|\n",
      "+---+------+-----+\n",
      "| 10|    80|Alice|\n",
      "|  5|    80|Alice|\n",
      "|  5|    80|  Bob|\n",
      "+---+------+-----+\n",
      "\n",
      "+---+------+-----+\n",
      "|age|height| name|\n",
      "+---+------+-----+\n",
      "| 10|    80|Alice|\n",
      "|  5|    80|Alice|\n",
      "|  5|    80|  Bob|\n",
      "+---+------+-----+\n",
      "\n",
      "+---+------+-----+\n",
      "|age|height| name|\n",
      "+---+------+-----+\n",
      "|  5|    80|  Bob|\n",
      "|  5|    80|Alice|\n",
      "| 10|    80|Alice|\n",
      "+---+------+-----+\n",
      "\n",
      "+---+------+-----+\n",
      "|age|height| name|\n",
      "+---+------+-----+\n",
      "| 10|    80|Alice|\n",
      "|  5|    80|Alice|\n",
      "|  5|    80|  Bob|\n",
      "+---+------+-----+\n",
      "\n",
      "+---+------+-----+\n",
      "|age|height| name|\n",
      "+---+------+-----+\n",
      "| 10|    80|Alice|\n",
      "|  5|    80|Alice|\n",
      "|  5|    80|  Bob|\n",
      "+---+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sort(*cols, **kwargs)\n",
    "# 返回按指定列排序的新DataFrame。\n",
    "df.sort(df.age.desc()).show()\n",
    "df.sort(\"age\", ascending=False).show()\n",
    "df.orderBy(df.age.desc()).show()\n",
    "from pyspark.sql.functions import *\n",
    "df.sort(asc('age')).show()\n",
    "df.orderBy(desc('age'), 'name').show()\n",
    "df.orderBy(['age', 'name'], ascending=[0, 1]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+\n",
      "|age|height| name|\n",
      "+---+------+-----+\n",
      "| 10|    80|Alice|\n",
      "|  5|    80|Alice|\n",
      "|  5|    80|  Bob|\n",
      "+---+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sortWithinPartitions(*cols, **kwargs)\n",
    "# 根据分区排序\n",
    "df.sortWithinPartitions('age', ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StorageLevel(False, False, False, False, 1)"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# storageLevel\n",
    "# 获取存储级别\n",
    "df.storageLevel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['satisfaction_level', 'last_evaluation', 'number_project', 'average_montly_hours', 'time_spend_company', 'Work_accident', 'left', 'promotion_last_5years', 'sales', 'salary']\n",
      "+-------+-------------------+------------------+--------------------+\n",
      "|summary| satisfaction_level|time_spend_company|average_montly_hours|\n",
      "+-------+-------------------+------------------+--------------------+\n",
      "|  count|              14999|             14999|               14999|\n",
      "|   mean| 0.6128335222348166| 3.498233215547703|   201.0503366891126|\n",
      "| stddev|0.24863065106114257|1.4601362305354808|   49.94309937128406|\n",
      "|    min|               0.09|                 2|                  96|\n",
      "|    25%|               0.44|                 3|                 156|\n",
      "|    50%|               0.64|                 3|                 200|\n",
      "|    75%|               0.82|                 4|                 245|\n",
      "|    max|                1.0|                10|                 310|\n",
      "+-------+-------------------+------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# summary(*statistics)\n",
    "# 计算数字和字符串列的指定统计信息。可用统计数据为： - count - mean - stddev - min - max - 以百分比形式指定的任意近似百分比（例如，75％）\n",
    "# 如果未给出统计数据，则此函数会计算count，mean，stddev，min，近似四分位数（25％，50％和75％处的百分位数）以及最大值。\n",
    "print(df0.columns)\n",
    "df0.select('satisfaction_level', 'time_spend_company', 'average_montly_hours').summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "|   f1| f2|\n",
      "+-----+---+\n",
      "|Alice|  5|\n",
      "|  Bob|  5|\n",
      "|Alice| 10|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# toDF(*cols)\n",
    "# 返回一个带有新列名的dataframe\n",
    "df.select('name', 'age').toDF('f1', 'f2').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['{\"age\":5,\"height\":80,\"name\":\"Alice\"}', '{\"age\":5,\"height\":80,\"name\":\"Bob\"}']"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# toJSON(use_unicode=True)\n",
    "# 将DataFrame转换为字符串的RDD。\n",
    "# 每一行都被转换成JSON文档作为返回的RDD中的一个元素。\n",
    "df.toJSON().take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(age=5, height=80, name='Alice'),\n",
       " Row(age=5, height=80, name='Bob'),\n",
       " Row(age=10, height=80, name='Alice')]"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# toLocalIterator()\n",
    "# 返回包含此DataFrame中所有行的迭代器。迭代器将占用与此DataFrame中最大分区一样多的内存。\n",
    "list(df.toLocalIterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>height</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>80</td>\n",
       "      <td>Alice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>80</td>\n",
       "      <td>Bob</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>80</td>\n",
       "      <td>Alice</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  height   name\n",
       "0    5      80  Alice\n",
       "1    5      80    Bob\n",
       "2   10      80  Alice"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# toPandas()\n",
    "# 转pandasdataframe\n",
    "df.toPandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# union(other)\n",
    "# 两表格合并"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+\n",
      "|col0|col1|col2|\n",
      "+----+----+----+\n",
      "|   1|   2|   3|\n",
      "|   6|   4|   5|\n",
      "+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# unionByName(other)\n",
    "# 按照名字合并\n",
    "df1 = spark.createDataFrame([[1, 2, 3]], [\"col0\", \"col1\", \"col2\"])\n",
    "df2 = spark.createDataFrame([[4, 5, 6]], [\"col1\", \"col2\", \"col0\"])\n",
    "df1.unionByName(df2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(age=5, height=80, name='Alice', age2=7),\n",
       " Row(age=5, height=80, name='Bob', age2=7),\n",
       " Row(age=10, height=80, name='Alice', age2=12)]"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# withColumn(colName, col)\n",
    "# 通过添加列或替换具有相同名称的现有列来返回新的DataFrame。\n",
    "df.withColumn('age2', df.age + 2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+-----+\n",
      "|age2|height| name|\n",
      "+----+------+-----+\n",
      "|   5|    80|Alice|\n",
      "|   5|    80|  Bob|\n",
      "|  10|    80|Alice|\n",
      "+----+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# withColumnRenamed(existing, new)\n",
    "#通过重命名现有列来返回新的DataFrame。如果模式不包含给定的列名，则这是一个无操作。\n",
    "# 更换列名的操作\n",
    "df.withColumnRenamed('age', 'age2').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## class pyspark.sql.GroupedData(jgd, df)\n",
    "由DataFrame.groupBy（）创建的DataFrame上的一组聚合方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+\n",
      "| id|                  v|\n",
      "+---+-------------------+\n",
      "|  1|-0.7071067811865475|\n",
      "|  1| 0.7071067811865475|\n",
      "|  0|-0.8320502943378437|\n",
      "|  0|-0.2773500981126146|\n",
      "|  0| 1.1094003924504583|\n",
      "+---+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# apply(udf)\n",
    "# 使用pandas udf映射当前DataFrame的每个组，并将结果作为DataFrame返回。\n",
    "# 使用pyspark.sql.functions.pandas_udf() , 用法跟pandas.apply一样\n",
    "# 该函数不支持部分聚合，并且需要对DataFrame中的所有数据进行混洗。\n",
    "from pyspark.sql.functions import pandas_udf,PandasUDFType\n",
    "df = spark.createDataFrame(\n",
    "    [(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)],\n",
    "    (\"id\", \"v\"))\n",
    "@pandas_udf(\"id long, v double\", PandasUDFType.GROUPED_MAP)\n",
    "def normalize(pdf):\n",
    "    v = pdf.v\n",
    "    return pdf.assign(v=(v - v.mean())) / v.std()\n",
    "df.groupby(\"id\").apply(normalize).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(avg(left)=0.2380825388359224)]"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# avg(*cols)\n",
    "# 计算平均值 同mean()\n",
    "df0.groupBy().avg('left').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count()\n",
    "# 计数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['satisfaction_level',\n",
       " 'last_evaluation',\n",
       " 'number_project',\n",
       " 'average_montly_hours',\n",
       " 'time_spend_company',\n",
       " 'Work_accident',\n",
       " 'left',\n",
       " 'promotion_last_5years',\n",
       " 'sales',\n",
       " 'salary']"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pivot(pivot_col, values=None)\n",
    "# 透视功能,两个版本:1、制定不同值的列表 2、另一种不支持。后者简洁但是效率低，因为spark需要先计算内部不同值的列表\n",
    "df0.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+----+---------+\n",
      "|numbers|hours|left|promotion|\n",
      "+-------+-----+----+---------+\n",
      "|      2|  157|   1|        0|\n",
      "|      5|  262|   1|        0|\n",
      "+-------+-----+----+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = df0.select(\"number_project\", \"average_montly_hours\", \"left\", 'promotion_last_5years').toDF('numbers', 'hours', 'left', 'promotion')\n",
    "df1.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+------+\n",
      "|left|     3|     5|\n",
      "+----+------+------+\n",
      "|   1| 15767|149051|\n",
      "|   0|785126|436451|\n",
      "+----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.groupBy('left').pivot('numbers',['3', '5']).sum(\"hours\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## class pyspark.sql.Column(jc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|age2|\n",
      "+----+\n",
      "|   5|\n",
      "|   5|\n",
      "|  10|\n",
      "+----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# alias(*alias, **kwargs)\n",
    "# 使用新名称返回此列的别名（在返回多个列的表达式（例如爆炸）的情况下）。\n",
    "df.select(df.age.alias('age2')).show()\n",
    "df.select(df.age.alias('age3', metadata={'max': 99})).schema['age3'].metadata['max']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "# asc()\n",
    "# 返回升序\n",
    "# desc()\n",
    "# 返回降序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------------------+\n",
      "| name|((age >= 2) AND (age <= 4))|\n",
      "+-----+---------------------------+\n",
      "|Alice|                      false|\n",
      "|  Bob|                      false|\n",
      "|Alice|                      false|\n",
      "+-----+---------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# between(lowerBound, upperBound)\n",
    "# 一个布尔表达式，如果此表达式的值位于给定列之间，则该表达式的值为true。\n",
    "df.select(df.name, df.age.between(2, 4)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(age=5, height=80, name='Bob')]"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# contains(other)\n",
    "# 包含其他元素。基于字符串匹配返回一个布尔列。\n",
    "df.filter(df.name.contains('o')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(age=5, height=80, name='Bob')]"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# endswith(other)\n",
    "# 返回通过匹配结尾的\n",
    "df.filter(df.name.endswith('b')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|r.b|\n",
      "+---+\n",
      "|  b|\n",
      "+---+\n",
      "\n",
      "+---+\n",
      "|r.a|\n",
      "+---+\n",
      "|  1|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# getField(name)\n",
    "# 在StructField中通过名称获取字段的表达式。\n",
    "from pyspark.sql import Row\n",
    "df = spark.createDataFrame([Row(r=Row(a=1, b='b'))])\n",
    "df.select(df.r.getField('b')).show()\n",
    "df.select(df.r.a).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+\n",
      "|l[0]|d[key]|\n",
      "+----+------+\n",
      "|   1| value|\n",
      "+----+------+\n",
      "\n",
      "+----+------+\n",
      "|l[0]|d[key]|\n",
      "+----+------+\n",
      "|   1| value|\n",
      "+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# getIrem(key)\n",
    "# 一个表达式，用于从列表中获取序号位置处的项目，或者通过键名从字典中获取项目。\n",
    "df = spark.createDataFrame([([1, 2], {'key':'value'})], ['l', 'd'])\n",
    "df.select(df.l.getItem(0), df.d.getItem('key')).show()\n",
    "df.select(df.l[0], df.d['key']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(height=80, name='Tom')]"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# isNotNull()\n",
    "# 如果当前表达式不为null，则为true。\n",
    "from pyspark.sql import Row\n",
    "df = spark.createDataFrame([Row(name='Tom', height=80), Row(name='Alice', height=None)])\n",
    "df.filter(df.height.isNotNull()).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(height=None, name='Alice')]"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# isNull()\n",
    "# 如果为null这为真\n",
    "from pyspark.sql import Row\n",
    "df = spark.createDataFrame([Row(name='Tom', height=80), Row(name='Alice', height=None)])\n",
    "df.filter(df.height.isNull()).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|height| name|\n",
      "+------+-----+\n",
      "|    80|  Tom|\n",
      "|  null|Alice|\n",
      "+------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(height=80, name='Tom')]"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# isin(*cols)\n",
    "df.show()\n",
    "df[df.name.isin('Tom', 'Mike')].collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(height=80, name='Tom')]"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.height.isin([1, 2, 80])].collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(height=None, name='Alice')]"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# like(other)\n",
    "# SQL像表达式一样。基于SQL LIKE匹配返回一个布尔型列。\n",
    "df.filter(df.name.like('Al%')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------------------------+\n",
      "| name|CASE WHEN (age > 3) THEN 1 ELSE 0 END|\n",
      "+-----+-------------------------------------+\n",
      "|Alice|                                    1|\n",
      "|  Bob|                                    1|\n",
      "|Alice|                                    1|\n",
      "+-----+-------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# otherwise(value)\n",
    "# 评估条件列表并返回多个可能的结果表达式之一。如果未调用Column.otherwise（），则不匹配条件返回None。有关示例用法，请参阅pyspark.sql.functions.when（）。\n",
    "from pyspark.sql.functions import *\n",
    "df.select(df.name, when(df.age > 3, 1).otherwise(0)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+\n",
      "|age|height| name|\n",
      "+---+------+-----+\n",
      "|  5|    80|Alice|\n",
      "| 10|    80|Alice|\n",
      "+---+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# rlike(other)\n",
    "# SQL RLIKE表达式（LIKE with Regex）。基于正则表达式匹配返回一个布尔列。\n",
    "df.filter(df.name.rlike('ice$')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+\n",
      "|age|height| name|\n",
      "+---+------+-----+\n",
      "|  5|    80|Alice|\n",
      "| 10|    80|Alice|\n",
      "+---+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# startswith(other)\n",
    "# 字符串以。开头。基于字符串匹配返回一个布尔列。\n",
    "df.filter(df.name.startswith('Al')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|col|\n",
      "+---+\n",
      "| Al|\n",
      "| Bo|\n",
      "| Al|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# substr(startPos, length)\n",
    "# 返回一列，该列是该列的子字符串。\n",
    "df.select(df.name.substr(1,2).alias('col')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------------------------------------------------+\n",
      "| name|CASE WHEN (age > 4) THEN 1 WHEN (age < 3) THEN -1 ELSE 0 END|\n",
      "+-----+------------------------------------------------------------+\n",
      "|Alice|                                                           1|\n",
      "|  Bob|                                                           1|\n",
      "|Alice|                                                           1|\n",
      "+-----+------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# when(condition, value)\n",
    "# 评估条件列表并返回多个可能的结果表达式之一。如果Column.otherwise（）未被调用，则不匹配条件返回None。\n",
    "from pyspark.sql.functions import *\n",
    "df.select(df.name, when(df.age > 4, 1).when(df.age < 3, -1).otherwise(0)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"my_first_app_name\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "+---+-------+---+--------+\n",
      "| id|   name|age|eyeColor|\n",
      "+---+-------+---+--------+\n",
      "|123|  Katie| 19|   brown|\n",
      "|234|Michael| 22|   green|\n",
      "|345| Simone| 23|    blue|\n",
      "+---+-------+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1、从RDD创建\n",
    "# 1.1、显示指定字段类型\n",
    "# 生成以逗号分隔的数据\n",
    "stringCSVRDD = spark.sparkContext.parallelize([\n",
    "    (123, \"Katie\", 19, \"brown\"),\n",
    "    (234, \"Michael\", 22, \"green\"),\n",
    "    (345, \"Simone\", 23, \"blue\")\n",
    "])\n",
    "\n",
    "# 指定模式, StructField(name,dataType,nullable)\n",
    "# 其中：\n",
    "#   name: 该字段的名字，\n",
    "#   dataType：该字段的数据类型，\n",
    "#   nullable: 指示该字段的值是否为空\n",
    "from pyspark.sql.types import StructType, StructField, LongType, StringType  # 导入类型\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"id\", LongType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", LongType(), True),\n",
    "    StructField(\"eyeColor\", StringType(), True)\n",
    "])\n",
    "\n",
    "# 对RDD应用该模式并且创建DataFrame\n",
    "swimmers = spark.createDataFrame(stringCSVRDD,schema)\n",
    "\n",
    "# 利用DataFrame创建一个临时视图\n",
    "swimmers.registerTempTable(\"swimmers\")\n",
    "\n",
    "# 查看DataFrame的行数\n",
    "print(swimmers.count())\n",
    "\n",
    "tempDF = spark.sql(\"select * from swimmers\")\n",
    "tempDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+--------+\n",
      "| id|   name|age|eyccolor|\n",
      "+---+-------+---+--------+\n",
      "|123|  Katie| 19|   brown|\n",
      "|234|Michael| 22|   green|\n",
      "|345| Simone| 23|    blue|\n",
      "+---+-------+---+--------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1.2、自动类型推断字段类型\n",
    "data = [(123, \"Katie\", 19, \"brown\"),\n",
    "        (234, \"Michael\", 22, \"green\"),\n",
    "        (345, \"Simone\", 23, \"blue\")]\n",
    "df = spark.createDataFrame(data, schema=['id', 'name', 'age', 'eyccolor'])\n",
    "df.show()\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|       name|score|\n",
      "+-----------+-----+\n",
      "|    xuruyun|  100|\n",
      "|zhangxueyou|  109|\n",
      "|    wangfei|   90|\n",
      "|   liudehua|   80|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2、读取 外部文件\n",
    "# 2.1、JSON\n",
    "file = r\"E:\\code\\python_workSpace\\idea_space\\toutiao_project\\other\\myLearn\\data\\student.json\"\n",
    "df = spark.read.json(file)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+--------------------+------+----+-----+-----+------+------+-----+--------+\n",
      "|PassengerId|Pclass|                Name|   Sex| Age|SibSp|Parch|Ticket|  Fare|Cabin|Embarked|\n",
      "+-----------+------+--------------------+------+----+-----+-----+------+------+-----+--------+\n",
      "|        892|     3|    Kelly, Mr. James|  male|34.5|    0|    0|330911|7.8292| null|       Q|\n",
      "|        893|     3|Wilkes, Mrs. Jame...|female|47.0|    1|    0|363272|   7.0| null|       S|\n",
      "|        894|     2|Myles, Mr. Thomas...|  male|62.0|    0|    0|240276|9.6875| null|       Q|\n",
      "+-----------+------+--------------------+------+----+-----+-----+------+------+-----+--------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2.2、csv文件\n",
    "# 2.2.1、直接读取\n",
    "file_path = r\"E:\\code\\python_workSpace\\idea_space\\toutiao_project\\other\\myLearn\\data\\test.csv\"\n",
    "monthlySales = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "monthlySales.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(PassengerId=892, Pclass=3, Name='Kelly, Mr. James', Sex='male', Age=34.5, SibSp=0, Parch=0, Ticket='330911', Fare=7.8292, Cabin=None, Embarked='Q')\n",
      "male <class 'str'> None <class 'NoneType'>\n"
     ]
    }
   ],
   "source": [
    "# 空值转换为 NoneType 类型\n",
    "temp_row = monthlySales.take(1)\n",
    "print(temp_row[0])\n",
    "print(temp_row[0][\"Sex\"], type(temp_row[0][\"Sex\"]), temp_row[0][\"Cabin\"], type(temp_row[0][\"Cabin\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 2.2.2、从pandas.dataframe创建\n",
    "def readFile_inputData(train_name=None, test_name=None, index_col=None, dtype=None, parse_dates=None, encoding=\"UTF-8\", sep=','):\n",
    "    if parse_dates is not None and type(parse_dates) != list:\n",
    "        raise Exception('parse_dates Type is Error, must list')\n",
    "    if train_name is not None:\n",
    "        train = pd.read_csv(filepath_or_buffer=train_name, index_col=index_col, dtype=dtype, parse_dates=parse_dates, encoding=encoding, sep=sep)\n",
    "    if test_name is not None:\n",
    "        test = pd.read_csv(filepath_or_buffer=test_name, index_col=index_col, dtype=dtype, parse_dates=parse_dates, encoding=encoding, sep=sep)\n",
    "        return train, test\n",
    "    else:\n",
    "        return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>892</td>\n",
       "      <td>3</td>\n",
       "      <td>Kelly, Mr. James</td>\n",
       "      <td>male</td>\n",
       "      <td>34.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>330911</td>\n",
       "      <td>7.8292</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>893</td>\n",
       "      <td>3</td>\n",
       "      <td>Wilkes, Mrs. James (Ellen Needs)</td>\n",
       "      <td>female</td>\n",
       "      <td>47.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>363272</td>\n",
       "      <td>7.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>894</td>\n",
       "      <td>2</td>\n",
       "      <td>Myles, Mr. Thomas Francis</td>\n",
       "      <td>male</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>240276</td>\n",
       "      <td>9.6875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>895</td>\n",
       "      <td>3</td>\n",
       "      <td>Wirz, Mr. Albert</td>\n",
       "      <td>male</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>315154</td>\n",
       "      <td>8.6625</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>896</td>\n",
       "      <td>3</td>\n",
       "      <td>Hirvonen, Mrs. Alexander (Helga E Lindqvist)</td>\n",
       "      <td>female</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3101298</td>\n",
       "      <td>12.2875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Pclass                                          Name     Sex  \\\n",
       "0          892       3                              Kelly, Mr. James    male   \n",
       "1          893       3              Wilkes, Mrs. James (Ellen Needs)  female   \n",
       "2          894       2                     Myles, Mr. Thomas Francis    male   \n",
       "3          895       3                              Wirz, Mr. Albert    male   \n",
       "4          896       3  Hirvonen, Mrs. Alexander (Helga E Lindqvist)  female   \n",
       "\n",
       "    Age  SibSp  Parch   Ticket     Fare Cabin Embarked  \n",
       "0  34.5      0      0   330911   7.8292   NaN        Q  \n",
       "1  47.0      1      0   363272   7.0000   NaN        S  \n",
       "2  62.0      0      0   240276   9.6875   NaN        Q  \n",
       "3  27.0      0      0   315154   8.6625   NaN        S  \n",
       "4  22.0      1      1  3101298  12.2875   NaN        S  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_data = readFile_inputData(file_path)\n",
    "temp_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "male <class 'str'> nan <class 'float'>\n"
     ]
    }
   ],
   "source": [
    "print(temp_data.loc[0,\"Sex\"], type(temp_data.loc[0,\"Sex\"]), temp_data.loc[0,\"Cabin\"], type(temp_data.loc[0,\"Cabin\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "object <class 'float'>\n",
      "object <class 'str'>\n",
      "+-----------+------+--------------------+------+----+-----+-----+------+------+-----+--------+\n",
      "|PassengerId|Pclass|                Name|   Sex| Age|SibSp|Parch|Ticket|  Fare|Cabin|Embarked|\n",
      "+-----------+------+--------------------+------+----+-----+-----+------+------+-----+--------+\n",
      "|        892|     3|    Kelly, Mr. James|  male|34.5|    0|    0|330911|7.8292|  nan|       Q|\n",
      "|        893|     3|Wilkes, Mrs. Jame...|female|47.0|    1|    0|363272|   7.0|  nan|       S|\n",
      "|        894|     2|Myles, Mr. Thomas...|  male|62.0|    0|    0|240276|9.6875|  nan|       Q|\n",
      "+-----------+------+--------------------+------+----+-----+-----+------+------+-----+--------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 直接转换报错：TypeError: field Cabin: Can not merge type <class 'pyspark.sql.types.DoubleType'> and <class 'pyspark.sql.types.StringType'>\n",
    "# 因为 Cabin字段：PD中的DataFrame 空值NaN是float类型，有值的是String类型，也就是包含了2种数据类型，所以Spark报错。\n",
    "# spark_df = spark.createDataFrame(temp_data)\n",
    "\n",
    "print(temp_data[\"Cabin\"].dtype, type(temp_data.loc[0,\"Cabin\"]))\n",
    "temp_data[\"Cabin\"] = temp_data[\"Cabin\"].astype(str) # 所有数据都转换为str类型（浮点型NaN 变为 str类型的字符串nan）\n",
    "print(temp_data[\"Cabin\"].dtype, type(temp_data.loc[0,\"Cabin\"]))\n",
    "spark_df = spark.createDataFrame(temp_data)\n",
    "spark_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(PassengerId=892, Pclass=3, Name='Kelly, Mr. James', Sex='male', Age=34.5, SibSp=0, Parch=0, Ticket='330911', Fare=7.8292, Cabin='nan', Embarked='Q')\n",
      "male <class 'str'> nan <class 'str'>\n"
     ]
    }
   ],
   "source": [
    "# 字符串nan 没有变化\n",
    "temp1_row = spark_df.take(1)\n",
    "print(temp1_row[0])\n",
    "print(temp1_row[0][\"Sex\"], type(temp1_row[0][\"Sex\"]), temp1_row[0][\"Cabin\"], type(temp1_row[0][\"Cabin\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.3、从parquet读取\n",
    "file=r\"D:\\apps\\spark-2.2.0-bin-hadoop2.7\\examples\\src\\main\\resources\\users.parquet\"\n",
    "df=spark.read.parquet(file)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.4、从hive读取\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .enableHiveSupport() \\      \n",
    "        .master(\"172.31.100.170:7077\") \\\n",
    "        .appName(\"my_first_app_name\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "df=spark.sql(\"select * from hive_tb_name\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.5、从hdfs读取\n",
    "# 直接读取，不需要指定ip和port\n",
    "data= spark.read.csv('hdfs:///tmp/_da_exdata_path/data.csv', header=True)\n",
    "data.show()\n",
    "\n",
    "# 有些情况下是需要指定ip和端口的\n",
    "data= spark.read.csv('hdfs://localhost:9000/tmp/_da_exdata_path/data.csv', header=True)\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3、保存数据\n",
    "# 3.1、保存到CSV\n",
    "df = pd.DataFrame(np.random.random((4, 4)),columns=['a', 'b', 'c', 'd'])\n",
    "spark_df = spark.createDataFrame(df)\n",
    "file=r\"D:\\apps\\spark-2.2.0-bin-hadoop2.7\\examples\\src\\main\\resources\\test.csv\"\n",
    "spark_df.write.csv(path=file, header=True, sep=\",\", mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2、保存到parquet\n",
    "df = pd.DataFrame(np.random.random((4, 4)),columns=['a', 'b', 'c', 'd'])\n",
    "spark_df = spark.createDataFrame(df)\n",
    "# 写到parquet\n",
    "file=r\"D:\\apps\\spark-2.2.0-bin-hadoop2.7\\examples\\src\\main\\resources\\test.parquet\"\n",
    "spark_df.write.parquet(path=file,mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.3、保存到hive\n",
    "# 打开动态分区\n",
    "spark.sql(\"set hive.exec.dynamic.partition.mode = nonstrict\")\n",
    "spark.sql(\"set hive.exec.dynamic.partition=true\")\n",
    "\n",
    "# 使用普通的hive-sql写入分区表\n",
    "spark.sql(\"\"\"\n",
    "    insert overwrite table ai.da_aipurchase_dailysale_hive \n",
    "    partition (saledate) \n",
    "    select productid, propertyid, processcenterid, saleplatform, sku, poa, salecount, saledate \n",
    "    from szy_aipurchase_tmp_szy_dailysale distribute by saledate\n",
    "    \"\"\")\n",
    "\n",
    "# 或者使用每次重建分区表的方式\n",
    "jdbcDF.write.mode(\"overwrite\").partitionBy(\"saledate\").insertInto(\"ai.da_aipurchase_dailysale_hive\")\n",
    "jdbcDF.write.saveAsTable(\"ai.da_aipurchase_dailysale_hive\", None, \"append\", partitionBy='saledate')\n",
    "\n",
    "# 不写分区表，只是简单的导入到hive表\n",
    "jdbcDF.write.saveAsTable(\"ai.da_aipurchase_dailysale_for_ema_predict\", None, \"overwrite\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.4、保存到hdfs\n",
    "# 数据写到hdfs，而且以csv格式保存\n",
    "jdbcDF.write.mode(\"overwrite\").options(header=\"true\").csv(\"/home/ai/da/da_aipurchase_dailysale_for_ema_predict.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.5、保存到mysql\n",
    "# 会自动对齐字段，也就是说，spark_df 的列不一定要全部包含MySQL的表的全部列才行\n",
    "\n",
    "# overwrite 清空表再导入\n",
    "spark_df.write.mode(\"overwrite\").format(\"jdbc\").options(\n",
    "    url='jdbc:mysql://127.0.0.1',\n",
    "    user='root',\n",
    "    password='123456',\n",
    "    dbtable=\"test.test\",\n",
    "    batchsize=\"1000\",\n",
    ").save()\n",
    "\n",
    "# append 追加方式\n",
    "spark_df.write.mode(\"append\").format(\"jdbc\").options(\n",
    "    url='jdbc:mysql://127.0.0.1',\n",
    "    user='root',\n",
    "    password='123456',\n",
    "    dbtable=\"test.test\",\n",
    "    batchsize=\"1000\",\n",
    ").save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

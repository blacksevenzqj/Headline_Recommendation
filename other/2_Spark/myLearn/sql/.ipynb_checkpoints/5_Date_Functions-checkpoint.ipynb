{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName('my_first_app_name') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+\n",
      "| id|      date|\n",
      "+---+----------+\n",
      "|  0|2020-02-25|\n",
      "|  1|2020-02-25|\n",
      "|  2|2020-02-25|\n",
      "+---+----------+\n",
      "\n",
      "2020-02-25 <class 'datetime.date'>\n",
      "2020-02-25 18:17:14.903051 <class 'datetime.datetime'> 25 <class 'int'>\n",
      "2020-02-25 <class 'datetime.date'> 25 <class 'int'>\n"
     ]
    }
   ],
   "source": [
    "# Python时间函数 和 Pandas时间函数 看“Time_utils.py”\n",
    "# 1、获取当前日期\n",
    "from pyspark.sql.functions import current_date\n",
    "\n",
    "temp = spark.range(3).withColumn('date',current_date())\n",
    "temp.show()\n",
    "print(temp.select('id', 'date').take(1)[0][1], type(temp.select('id', 'date').take(1)[0][1])) # 就是 datetime.date\n",
    "\n",
    "temp2 = datetime.datetime.today()\n",
    "print(temp2, type(temp2), temp2.day, type(temp2.day))\n",
    "\n",
    "temp3 = datetime.date.today()\n",
    "print(temp3, type(temp3), temp3.day, type(temp3.day))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|      date|\n",
      "+----------+\n",
      "|2015/04/08|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2、日期格式转换\n",
    "from pyspark.sql.functions import date_format\n",
    "\n",
    "sdf = spark.createDataFrame([('2015-04-08',)], ['a'])\n",
    "sdf.select(date_format('a', 'yyyy/MM/dd').alias('date')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|      date|\n",
      "+----------+\n",
      "|1997-02-28|\n",
      "+----------+\n",
      "\n",
      "+-------------------+\n",
      "|                 dt|\n",
      "+-------------------+\n",
      "|1997-02-28 10:30:00|\n",
      "+-------------------+\n",
      "\n",
      "+-------------------+\n",
      "|                 dt|\n",
      "+-------------------+\n",
      "|1997-02-28 10:30:00|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3、字符转日期\n",
    "from pyspark.sql.functions import to_date, to_timestamp\n",
    "\n",
    "# 1.转日期\n",
    "sdf = spark.createDataFrame([('1997-02-28 10:30:00',)], ['t'])\n",
    "sdf.select(to_date(sdf.t).alias('date')).show()\n",
    "\n",
    "# 2.带时间的日期\n",
    "sdf = spark.createDataFrame([('1997-02-28 10:30:00',)], ['t'])\n",
    "sdf.select(to_timestamp(sdf.t).alias('dt')).show()\n",
    "\n",
    "# 还可以指定日期格式\n",
    "sdf = spark.createDataFrame([('1997-02-28 10:30:00',)], ['t'])\n",
    "sdf.select(to_timestamp(sdf.t, 'yyyy-MM-dd HH:mm:ss').alias('dt')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+\n",
      "|year|month|day|\n",
      "+----+-----+---+\n",
      "|2015|    4|  8|\n",
      "+----+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4、获取日期中的年月日\n",
    "from pyspark.sql.functions import year, month, dayofmonth\n",
    "\n",
    "sdf = spark.createDataFrame([('2015-04-08',)], ['a'])\n",
    "sdf.select(year('a').alias('year'), \n",
    "          month('a').alias('month'),\n",
    "          dayofmonth('a').alias('day')\n",
    "  ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+------+\n",
      "|hour|minute|second|\n",
      "+----+------+------+\n",
      "|  13|     8|    15|\n",
      "+----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 5、获取时分秒\n",
    "from pyspark.sql.functions import hour, minute, second\n",
    "\n",
    "df = spark.createDataFrame([('2015-04-08 13:08:15',)], ['a'])\n",
    "df.select(hour('a').alias('hour'),\n",
    "          minute('a').alias('minute'),\n",
    "          second('a').alias('second')\n",
    "          ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|quarter|\n",
      "+-------+\n",
      "|      2|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 6、获取日期对应的季度\n",
    "from pyspark.sql.functions import quarter\n",
    "\n",
    "sdf = spark.createDataFrame([('2015-04-08',)], ['a'])\n",
    "sdf.select(quarter('a').alias('quarter')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|     d-add|     d-sub|\n",
      "+----------+----------+\n",
      "|2015-04-09|2015-04-07|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 7、日期加减\n",
    "from pyspark.sql.functions import date_add, date_sub\n",
    "\n",
    "sdf = spark.createDataFrame([('2015-04-08',)], ['d'])\n",
    "sdf.select(date_add(sdf.d, 1).alias('d-add'),\n",
    "           date_sub(sdf.d, 1).alias('d-sub')\n",
    "    ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|         d|\n",
      "+----------+\n",
      "|2015-05-08|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 8、月份加减\n",
    "from pyspark.sql.functions import add_months\n",
    "\n",
    "df = spark.createDataFrame([('2015-04-08',)], ['d'])\n",
    "df.select(add_months(df.d, 1).alias('d')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|diff|\n",
      "+----+\n",
      "|  32|\n",
      "+----+\n",
      "\n",
      "+----------+\n",
      "|    months|\n",
      "+----------+\n",
      "|3.94959677|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 9、日期差,月份差\n",
    "from pyspark.sql.functions import datediff, months_between\n",
    "\n",
    "# 1.日期差\n",
    "sdf = spark.createDataFrame([('2015-04-08','2015-05-10')], ['d1', 'd2'])\n",
    "sdf.select(datediff(sdf.d2, sdf.d1).alias('diff')).show()\n",
    "\n",
    "# 2.月份差\n",
    "sdf = spark.createDataFrame([('1997-02-28 10:30:00', '1996-10-30')], ['t', 'd'])\n",
    "sdf.select(months_between(sdf.t, sdf.d).alias('months')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|      date|\n",
      "+----------+\n",
      "|2015-08-02|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 10、计算下一个指定星期的日期\n",
    "from pyspark.sql.functions import next_day\n",
    "\n",
    "# \"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\".\n",
    "sdf = spark.createDataFrame([('2015-07-27',)], ['d'])\n",
    "sdf.select(next_day(sdf.d, 'Sun').alias('date')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|      date|\n",
      "+----------+\n",
      "|1997-02-28|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 11、本月的最后一个日期\n",
    "from pyspark.sql.functions import last_day\n",
    "\n",
    "sdf = spark.createDataFrame([('1997-02-10',)], ['d'])\n",
    "sdf.select(last_day(sdf.d).alias('date')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName('my_first_app_name') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    color  length\n",
      "0   white       5\n",
      "1   green       5\n",
      "2  yellow       6\n",
      "3     red       3\n",
      "4   brown       5\n",
      "5    pink       4\n",
      "6   white       5\n",
      "+------+------+\n",
      "| color|length|\n",
      "+------+------+\n",
      "| white|     5|\n",
      "| green|     5|\n",
      "|yellow|     6|\n",
      "|   red|     3|\n",
      "| brown|     5|\n",
      "|  pink|     4|\n",
      "| white|     5|\n",
      "+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 从pandas dataframe创建spark dataframe\n",
    "colors = ['white','green','yellow','red','brown','pink', 'white']\n",
    "color_df = pd.DataFrame(colors,columns=['color'])\n",
    "color_df['length'] = color_df['color'].apply(len)\n",
    "print(color_df)\n",
    "\n",
    "color_sdf = spark.createDataFrame(color_df)\n",
    "color_sdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|length|count|\n",
      "+------+-----+\n",
      "|     6|    1|\n",
      "|     5|    4|\n",
      "|     3|    1|\n",
      "|     4|    1|\n",
      "+------+-----+\n",
      "\n",
      "+------+-----------+-----------+\n",
      "| color|max(length)|sum(length)|\n",
      "+------+-----------+-----------+\n",
      "| green|          5|          5|\n",
      "|yellow|          6|          6|\n",
      "| white|          5|         10|\n",
      "|  pink|          4|          4|\n",
      "|   red|          3|          3|\n",
      "| brown|          5|          5|\n",
      "+------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1、分组统计\n",
    "# 分组计算1\n",
    "color_sdf.groupBy('length').count().show()\n",
    "\n",
    "# 分组计算2：应用多函数\n",
    "import pyspark.sql.functions as func\n",
    "color_sdf.groupBy(\"color\").agg(func.max(\"length\"), func.sum(\"length\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---+------+------+------+-----------+\n",
      "|emp_id|    name|age|emp_id|salary|emp_id|departement|\n",
      "+------+--------+---+------+------+------+-----------+\n",
      "|     7|   James| 38|  null|  null|  null|       null|\n",
      "|     6| Vincent| 35|  null|  null|  null|       null|\n",
      "|     9|   Larry| 29|  null|  null|  null|       null|\n",
      "|     5|   Kevin| 26|  null|  null|  null|       null|\n",
      "|     1|    John| 25|     1|  1000|     1|       1000|\n",
      "|    10|Kimberly| 29|  null|  null|  null|       null|\n",
      "|     3|    Mike| 24|     3|  3000|     3|       3000|\n",
      "|    12|   Garry| 25|  null|  null|  null|       null|\n",
      "|     8|   Shane| 32|  null|  null|  null|       null|\n",
      "|    11|    Alex| 28|  null|  null|  null|       null|\n",
      "|     2|     Ray| 35|     2|  2000|     2|       2000|\n",
      "|     4|    Jane| 28|     4|  4000|     4|       4000|\n",
      "|    13|     Max| 31|  null|  null|  null|       null|\n",
      "+------+--------+---+------+------+------+-----------+\n",
      "\n",
      "+------+--------+---+------+-----------+\n",
      "|emp_id|    name|age|salary|departement|\n",
      "+------+--------+---+------+-----------+\n",
      "|     7|   James| 38|  null|       null|\n",
      "|     6| Vincent| 35|  null|       null|\n",
      "|     9|   Larry| 29|  null|       null|\n",
      "|     5|   Kevin| 26|  null|       null|\n",
      "|     1|    John| 25|  1000|       1000|\n",
      "|    10|Kimberly| 29|  null|       null|\n",
      "|     3|    Mike| 24|  3000|       3000|\n",
      "|    12|   Garry| 25|  null|       null|\n",
      "|     8|   Shane| 32|  null|       null|\n",
      "|    11|    Alex| 28|  null|       null|\n",
      "|     2|     Ray| 35|  2000|       2000|\n",
      "|     4|    Jane| 28|  4000|       4000|\n",
      "|    13|     Max| 31|  null|       null|\n",
      "+------+--------+---+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2、join操作\n",
    "# 1.生成测试数据\n",
    "employees = [(1, \"John\", 25), (2, \"Ray\", 35), (3,\"Mike\", 24), (4, \"Jane\", 28), \n",
    "             (5, \"Kevin\", 26), \n",
    "             (6, \"Vincent\", 35), (7,\"James\", 38), (8, \"Shane\", 32), \n",
    "             (9, \"Larry\", 29), (10, \"Kimberly\", 29),\n",
    "             (11, \"Alex\", 28), (12, \"Garry\", 25), (13, \"Max\",31)]\n",
    "employees=spark.createDataFrame(employees, schema=[\"emp_id\",\"name\",\"age\"])\n",
    "# employees.show()\n",
    "\n",
    "salary=[(1,1000),(2,2000),(3,3000),(4,4000)]\n",
    "salary=spark.createDataFrame(salary, schema=[\"emp_id\",\"salary\"])\n",
    "# salary.show()\n",
    "\n",
    "department=[(1,1000),(2,2000),(3,3000),(4,4000)]\n",
    "department=spark.createDataFrame(department, schema=[\"emp_id\",\"departement\"])\n",
    "# department.show()\n",
    "\n",
    "# 2.连接\n",
    "# join默认是内连接，最终结果会存在重复列名\n",
    "# 如果是pandas,重复列会用_x,_y等后缀标识出来，但spark不会\n",
    "# join会在最后的dataframe中存在重复列\n",
    "final_data = employees.join(salary, employees.emp_id == salary.emp_id, how='left')\\\n",
    "    .join(department, employees.emp_id==department.emp_id, how='left')\n",
    "final_data.show()\n",
    "\n",
    "# 3.如果两边的关联字段名相同，也可以省去很多麻烦\n",
    "final_data2 = employees.join(salary, on='emp_id', how='left')\\\n",
    "    .join(department, on='emp_id', how='left')\n",
    "final_data2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+---+------+-----------+\n",
      "|emp_id|name|age|salary|departement|\n",
      "+------+----+---+------+-----------+\n",
      "|     1|John| 25|  1000|       1000|\n",
      "|     3|Mike| 24|  3000|       3000|\n",
      "|     2| Ray| 35|  2000|       2000|\n",
      "|     4|Jane| 28|  4000|       4000|\n",
      "+------+----+---+------+-----------+\n",
      "\n",
      "+------+--------+---+------+-----------+\n",
      "|emp_id|    name|age|salary|departement|\n",
      "+------+--------+---+------+-----------+\n",
      "|     7|   James| 38|  2500|       2500|\n",
      "|     6| Vincent| 35|  2500|       2500|\n",
      "|     9|   Larry| 29|  2500|       2500|\n",
      "|     5|   Kevin| 26|  2500|       2500|\n",
      "|     1|    John| 25|  1000|       1000|\n",
      "|    10|Kimberly| 29|  2500|       2500|\n",
      "|     3|    Mike| 24|  3000|       3000|\n",
      "|    12|   Garry| 25|  2500|       2500|\n",
      "|     8|   Shane| 32|  2500|       2500|\n",
      "|    11|    Alex| 28|  2500|       2500|\n",
      "|     2|     Ray| 35|  2000|       2000|\n",
      "|     4|    Jane| 28|  4000|       4000|\n",
      "|    13|     Max| 31|  2500|       2500|\n",
      "+------+--------+---+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3. 缺失值处理（缺失值类型为：<class 'NoneType'>）\n",
    "# 1.删除有缺失值的行\n",
    "clean_data = final_data2.na.drop()\n",
    "clean_data.show()\n",
    "\n",
    "# 2.用均值替换缺失值\n",
    "import math\n",
    "from pyspark.sql import functions as func  # 导入spark内置函数\n",
    "# 计算缺失值，collect()函数将数据返回到driver端，为Row对象，[0]可以获取Row的值\n",
    "mean_salary = final_data2.select(func.mean('salary'), func.mean('departement')).collect()\n",
    "print(mean_salary)\n",
    "clean_data = final_data2.na.fill({'salary':mean_salary[0][0],\"departement\":mean_salary[0][1]})\n",
    "clean_data.show()\n",
    "\n",
    "# 3.如果一行至少2个缺失值才删除该行\n",
    "# final_data2.na.drop(thresh=1).show() # 不生效，不知原因\n",
    "\n",
    "# 4.填充缺失值\n",
    "# 对所有列用同一个值填充缺失值。注意：填充值必须和被填充列数据类型相同，否则不生效。\n",
    "# final_data2.na.fill('unknown').show()\n",
    "\n",
    "# 5.不同的列用不同的值填充。注意：填充值必须和被填充列数据类型相同，否则不生效。\n",
    "# final_data2.na.fill({'salary':'--', 'departement':'unknown'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-----+-----+\n",
      "|   r1|   r2|  r11|  r22|\n",
      "+-----+-----+-----+-----+\n",
      "|false| true|false|false|\n",
      "| true|false|false|false|\n",
      "+-----+-----+-----+-----+\n",
      "\n",
      "+-----+-----+-----+-----+\n",
      "|   r1|   r2|  r11|  r22|\n",
      "+-----+-----+-----+-----+\n",
      "|false|false|false| true|\n",
      "|false|false| true|false|\n",
      "+-----+-----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4、空值判断\n",
    "# 有两种空值判断，一种是浮点型空值nan（就是Numpy、Pandas中的np.nan），另一种是Python中普通的None\n",
    "from pyspark.sql.functions import isnull, isnan\n",
    "\n",
    "# 1.None的空值判断：isnull只能判断None\n",
    "df = spark.createDataFrame([(1, None), (None, 2)], (\"a\", \"b\"))\n",
    "df.select(isnull(\"a\").alias(\"r1\"), isnull(df.b).alias(\"r2\"), isnan(\"a\").alias(\"r11\"), isnan(df.b).alias(\"r22\")).show()\n",
    "\n",
    "# 2.浮点型nan的空值判断：isnan只能判断float('nan') 等价于 np.nan\n",
    "df = spark.createDataFrame([(1.0, float('nan')), (float('nan'), 2.0)], (\"a\", \"b\"))\n",
    "df.select(isnull(\"a\").alias(\"r1\"), isnull(df.b).alias(\"r2\"), isnan(\"a\").alias(\"r11\"), isnan(df.b).alias(\"r22\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   color  length\n",
      "0  False    True\n",
      "1  False    True\n",
      "2  False   False\n",
      "3  False   False\n",
      "4  False   False\n",
      "5  False   False\n",
      "6  False   False\n",
      "True True\n"
     ]
    }
   ],
   "source": [
    "# Pandas的series或Dataframe.isnull()方法都可以判断出来；np.isnan方法也都可以判断出来。\n",
    "color_df.loc[0,\"length\"] = np.nan\n",
    "color_df.loc[1,\"length\"] = None\n",
    "print(color_df.isnull())\n",
    "print(np.isnan(color_df.loc[0,\"length\"]), np.isnan(color_df.loc[1,\"length\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| r1| r2|\n",
      "+---+---+\n",
      "|1.0|1.0|\n",
      "|2.0|2.0|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "# pandas \n",
    "# where即if-else函数\n",
    "np.where(isnull(a),b,a) # 不清楚\n",
    "\n",
    "# combine_first方法\n",
    "# 如果a中值为空，就用b中的值填补\n",
    "a[:-2].combine_first(b[2:])\n",
    "\n",
    "# combine_first函数即对数据打补丁，用df2的数据填充df1中的缺失值\n",
    "df1.combine_first(df2)\n",
    "'''\n",
    "\n",
    "# pyspark\n",
    "import pyspark.sql.functions as func\n",
    "df = spark.createDataFrame([(1.0, float('nan')), (float('nan'), 2.0)], (\"a\", \"b\"))\n",
    "df.select(func.nanvl(\"a\", \"b\").alias(\"r1\"), func.nanvl(df.a, df.b).alias(\"r2\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500.0\n",
      "+---------+\n",
      "|deviation|\n",
      "+---------+\n",
      "|     null|\n",
      "|     null|\n",
      "|     null|\n",
      "|     null|\n",
      "|2250000.0|\n",
      "|     null|\n",
      "| 250000.0|\n",
      "|     null|\n",
      "|     null|\n",
      "|     null|\n",
      "| 250000.0|\n",
      "|2250000.0|\n",
      "|     null|\n",
      "+---------+\n",
      "\n",
      "1118\n",
      "+------+--------+---+------+--------------+\n",
      "|emp_id|    name|age|salary|updated_salary|\n",
      "+------+--------+---+------+--------------+\n",
      "|     7|   James| 38|  null|        2500.0|\n",
      "|     6| Vincent| 35|  null|        2500.0|\n",
      "|     9|   Larry| 29|  null|        2500.0|\n",
      "|     5|   Kevin| 26|  null|        2500.0|\n",
      "|     1|    John| 25|  1000|        1000.0|\n",
      "|    10|Kimberly| 29|  null|        2500.0|\n",
      "|     3|    Mike| 24|  3000|        3000.0|\n",
      "|    12|   Garry| 25|  null|        2500.0|\n",
      "|     8|   Shane| 32|  null|        2500.0|\n",
      "|    11|    Alex| 28|  null|        2500.0|\n",
      "|     2|     Ray| 35|  2000|        2000.0|\n",
      "|     4|    Jane| 28|  4000|        4000.0|\n",
      "|    13|     Max| 31|  null|        2500.0|\n",
      "+------+--------+---+------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 5、离群点\n",
    "import math\n",
    "# 需要提醒的是，列的计算都是放在select里面的\n",
    "\n",
    "# 5.1.先计算均值\n",
    "mean_salary = final_data2.select(func.mean('salary')).collect()[0][0]\n",
    "print(mean_salary)\n",
    "\n",
    "# 5.2.再计算方差\n",
    "devs = final_data2.select(((final_data2.salary-mean_salary)**2).alias('deviation'))\n",
    "devs.show()\n",
    "\n",
    "# 5.3.再计算标准差\n",
    "stddev = math.floor(math.sqrt(devs.groupBy().avg('deviation').first()[0]))\n",
    "print(stddev)\n",
    "\n",
    "# 5.4.用均值的两倍标准差替代离群值\n",
    "no_outlier = final_data2.select(\n",
    "    final_data2.emp_id, final_data2.name, final_data2.age, final_data2.salary,\n",
    "    # between：salary在区间中则返回True； when：当条件为True返回指定值，否则返回otherwise指定的值。\n",
    "    func.when(final_data2.salary.between(mean_salary-2*stddev, mean_salary+2*stddev), final_data2.salary).otherwise(mean_salary).alias(\"updated_salary\")\n",
    "    )\n",
    "no_outlier.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---+------+--------------+\n",
      "|emp_id|    name|age|salary|updated_salary|\n",
      "+------+--------+---+------+--------------+\n",
      "|     7|   James| 38|  null|        2500.0|\n",
      "|     6| Vincent| 35|  null|        2500.0|\n",
      "|     9|   Larry| 29|  null|        2500.0|\n",
      "|     5|   Kevin| 26|  null|        2500.0|\n",
      "|     1|    John| 25|  1000|        1000.0|\n",
      "|    10|Kimberly| 29|  null|        2500.0|\n",
      "|     3|    Mike| 24|  3000|        3000.0|\n",
      "|    12|   Garry| 25|  null|        2500.0|\n",
      "|     8|   Shane| 32|  null|        2500.0|\n",
      "|    11|    Alex| 28|  null|        2500.0|\n",
      "|     2|     Ray| 35|  2000|        2000.0|\n",
      "|     4|    Jane| 28|  4000|        4000.0|\n",
      "|    13|     Max| 31|  null|        2500.0|\n",
      "+------+--------+---+------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 5.5、func中有现成的常用统计函数，更加方便\n",
    "# 1.计算均值\n",
    "mean_salary = final_data2.select(func.mean('salary')).collect()[0][0]\n",
    "# 2.计算标准差\n",
    "stddev2 = final_data2.select(func.stddev('salary')).collect()[0][0]\n",
    "# 离群值替代就和上面的一致了\n",
    "no_outlier2 = final_data2.select(\n",
    "    final_data2.emp_id, final_data2.name, final_data2.age, final_data2.salary,\n",
    "    # between：salary在区间中则返回True； when：当条件为True返回指定值，否则返回otherwise指定的值。\n",
    "    func.when(final_data2.salary.between(mean_salary-2*stddev2, mean_salary+2*stddev2), final_data2.salary).otherwise(mean_salary).alias(\"updated_salary\")\n",
    "    )\n",
    "no_outlier2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+----------------+\n",
      "|FirstName|LastName|             Dob|\n",
      "+---------+--------+----------------+\n",
      "|   Thomas|   Hardy|     June 2,1840|\n",
      "|   Thomas|   Hardy|     June 2,1840|\n",
      "|   Thomas|       H|            null|\n",
      "|     Jane|  Austen|16 December 1775|\n",
      "|    Emily|    null|            null|\n",
      "+---------+--------+----------------+\n",
      "\n",
      "+---------+--------+----------------+\n",
      "|FirstName|LastName|             Dob|\n",
      "+---------+--------+----------------+\n",
      "|     Jane|  Austen|16 December 1775|\n",
      "|    Emily|    null|            null|\n",
      "|   Thomas|   Hardy|     June 2,1840|\n",
      "|   Thomas|       H|            null|\n",
      "+---------+--------+----------------+\n",
      "\n",
      "+---------+--------+----------------+\n",
      "|FirstName|LastName|             Dob|\n",
      "+---------+--------+----------------+\n",
      "|    Emily|    null|            null|\n",
      "|     Jane|  Austen|16 December 1775|\n",
      "|   Thomas|   Hardy|     June 2,1840|\n",
      "+---------+--------+----------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FirstName</th>\n",
       "      <th>LastName</th>\n",
       "      <th>Dob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Thomas</td>\n",
       "      <td>Hardy</td>\n",
       "      <td>June 2,1840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jane</td>\n",
       "      <td>Austen</td>\n",
       "      <td>16 December 1775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Emily</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  FirstName LastName               Dob\n",
       "0    Thomas    Hardy       June 2,1840\n",
       "3      Jane   Austen  16 December 1775\n",
       "4     Emily     None              None"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6、重复值\n",
    "# 重复值的处理，和pandas很像啊\n",
    "authors = [['Thomas','Hardy','June 2,1840'],\n",
    "            ['Thomas','Hardy','June 2,1840'],\n",
    "            ['Thomas','H',None],\n",
    "            ['Jane','Austen','16 December 1775'],\n",
    "            ['Emily',None,None]]\n",
    "\n",
    "sdf = spark.createDataFrame(authors, schema=[\"FirstName\",\"LastName\",\"Dob\"])\n",
    "sdf.show()\n",
    "\n",
    "# 删除重复值行（整行数据都重复才去重）\n",
    "sdf.dropDuplicates().show()\n",
    "\n",
    "# 指定列有重复值，则去重\n",
    "sdf.dropDuplicates(subset=['FirstName']).show()\n",
    "\n",
    "\n",
    "# pandas的方法\n",
    "df = pd.DataFrame(authors, columns=[\"FirstName\",\"LastName\",\"Dob\"])\n",
    "df.drop_duplicates(subset=['FirstName']) # 指定列有重复值，则去重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---+------+-----------+-----------+\n",
      "|emp_id|    name|age|salary|departement|   name_age|\n",
      "+------+--------+---+------+-----------+-----------+\n",
      "|     7|   James| 38|  null|       null|   James_38|\n",
      "|     6| Vincent| 35|  null|       null| Vincent_35|\n",
      "|     9|   Larry| 29|  null|       null|   Larry_29|\n",
      "|     5|   Kevin| 26|  null|       null|   Kevin_26|\n",
      "|     1|    John| 25|  1000|       1000|    John_25|\n",
      "|    10|Kimberly| 29|  null|       null|Kimberly_29|\n",
      "|     3|    Mike| 24|  3000|       3000|    Mike_24|\n",
      "|    12|   Garry| 25|  null|       null|   Garry_25|\n",
      "|     8|   Shane| 32|  null|       null|   Shane_32|\n",
      "|    11|    Alex| 28|  null|       null|    Alex_28|\n",
      "|     2|     Ray| 35|  2000|       2000|     Ray_35|\n",
      "|     4|    Jane| 28|  4000|       4000|    Jane_28|\n",
      "|    13|     Max| 31|  null|       null|     Max_31|\n",
      "+------+--------+---+------+-----------+-----------+\n",
      "\n",
      "+------+--------+---+------+-----------+-----------+---------------+\n",
      "|emp_id|    name|age|salary|departement|   name_age|age_incremented|\n",
      "+------+--------+---+------+-----------+-----------+---------------+\n",
      "|     7|   James| 38|  null|       null|   James_38|             39|\n",
      "|     6| Vincent| 35|  null|       null| Vincent_35|             36|\n",
      "|     9|   Larry| 29|  null|       null|   Larry_29|             30|\n",
      "|     5|   Kevin| 26|  null|       null|   Kevin_26|             27|\n",
      "|     1|    John| 25|  1000|       1000|    John_25|             26|\n",
      "|    10|Kimberly| 29|  null|       null|Kimberly_29|             30|\n",
      "|     3|    Mike| 24|  3000|       3000|    Mike_24|             25|\n",
      "|    12|   Garry| 25|  null|       null|   Garry_25|             26|\n",
      "|     8|   Shane| 32|  null|       null|   Shane_32|             33|\n",
      "|    11|    Alex| 28|  null|       null|    Alex_28|             29|\n",
      "|     2|     Ray| 35|  2000|       2000|     Ray_35|             36|\n",
      "|     4|    Jane| 28|  4000|       4000|    Jane_28|             29|\n",
      "|    13|     Max| 31|  null|       null|     Max_31|             32|\n",
      "+------+--------+---+------+-----------+-----------+---------------+\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-023908bffe43>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;31m# 3.某些列是自带一些常用的方法的\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[0msdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Initial'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLastName\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;31m# 4.顺便增加一新列\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df1' is not defined"
     ]
    }
   ],
   "source": [
    "# 7、生成新列\n",
    "# 数据转换，可以理解成列与列的运算\n",
    "# 注意自定义函数的调用方式\n",
    "\n",
    "# 0.创建udf自定义函数，对于简单的lambda函数不需要指定返回值类型\n",
    "from pyspark.sql.functions import udf\n",
    "concat_func = udf(lambda name ,age : name + '_' + str(age))\n",
    "\n",
    "# 1.应用自定义函数\n",
    "concat_df = final_data2.withColumn(\"name_age\", \n",
    "                                   concat_func(final_data.name, final_data.age))\n",
    "concat_df.show()\n",
    "\n",
    "# 2.通过列生成另一列\n",
    "data_new = concat_df.withColumn(\"age_incremented\", concat_df.age+1)\n",
    "data_new.show()\n",
    "\n",
    "# 3.某些列是自带一些常用的方法的\n",
    "sdf.withColumn('Initial', sdf.LastName.substr(1,1)).show()\n",
    "\n",
    "# 4.顺便增加一新列\n",
    "from pyspark.sql.functions import lit\n",
    "sdf.withColumn('newCol', lit(0)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|length(color)|\n",
      "+-------------+\n",
      "|            5|\n",
      "|            5|\n",
      "|            6|\n",
      "|            3|\n",
      "|            5|\n",
      "|            4|\n",
      "|            5|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 8、类eval操作\n",
    "# 传入一个操作字符串，然后转成python代码执行，就像python的eval一样。\n",
    "from pyspark.sql.functions import expr\n",
    "color_sdf.select(expr('length(color)')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|emp_id|salary|\n",
      "+------+------+\n",
      "|     1|  1000|\n",
      "|     2|  2000|\n",
      "|     3|  3000|\n",
      "|     4|  4000|\n",
      "+------+------+\n",
      "\n",
      "+--------+-----+\n",
      "|greatest|least|\n",
      "+--------+-----+\n",
      "|    1000|    1|\n",
      "|    2000|    2|\n",
      "|    3000|    3|\n",
      "|    4000|    4|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 9、行的最大最小值\n",
    "# 测试数据\n",
    "sdf = [(1,1000),(2,2000),(3,3000),(4,4000)]\n",
    "sdf = spark.createDataFrame(sdf, schema=[\"emp_id\",\"salary\"])\n",
    "sdf.show()\n",
    "\n",
    "# 求行的最大最小值：比较列之间的值大小（不是求一列的最大/最小值）\n",
    "from pyspark.sql.functions import greatest, least\n",
    "sdf.select(greatest('emp_id','salary').alias('greatest'),\n",
    "          least('emp_id','salary').alias('least')\n",
    "          ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|emp_id|\n",
      "+------+\n",
      "|     4|\n",
      "|     3|\n",
      "|     4|\n",
      "|     4|\n",
      "+------+\n",
      "\n",
      "+------+\n",
      "|emp_id|\n",
      "+------+\n",
      "|     5|\n",
      "|     3|\n",
      "|     5|\n",
      "|     5|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 10、when操作\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "# 1.case when age=2 then 3 else 4\n",
    "sdf.select(when(sdf['emp_id'] == 2, 3).otherwise(4).alias(\"emp_id\")).show()\n",
    "\n",
    "# # 2.case when age=2 when age=age+1 \n",
    "sdf.select(when(sdf.emp_id == 2, sdf.emp_id + 1).otherwise(5).alias(\"emp_id\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+\n",
      "|  a|  b|  c|\n",
      "+---+---+---+\n",
      "|  1|  2|  3|\n",
      "|  1|  2|  1|\n",
      "|  1|  2|  3|\n",
      "|  3|  6|  3|\n",
      "|  1|  2|  3|\n",
      "+---+---+---+\n",
      "\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o477.showString.\n: java.lang.UnsupportedOperationException: Cannot evaluate expression: lag(input[0, bigint, true], 1, 0)\r\n\tat org.apache.spark.sql.catalyst.expressions.Unevaluable$class.doGenCode(Expression.scala:261)\r\n\tat org.apache.spark.sql.catalyst.expressions.OffsetWindowFunction.doGenCode(windowExpressions.scala:337)\r\n\tat org.apache.spark.sql.catalyst.expressions.Expression$$anonfun$genCode$2.apply(Expression.scala:108)\r\n\tat org.apache.spark.sql.catalyst.expressions.Expression$$anonfun$genCode$2.apply(Expression.scala:105)\r\n\tat scala.Option.getOrElse(Option.scala:121)\r\n\tat org.apache.spark.sql.catalyst.expressions.Expression.genCode(Expression.scala:105)\r\n\tat org.apache.spark.sql.catalyst.expressions.Cast.doGenCode(Cast.scala:660)\r\n\tat org.apache.spark.sql.catalyst.expressions.Expression$$anonfun$genCode$2.apply(Expression.scala:108)\r\n\tat org.apache.spark.sql.catalyst.expressions.Expression$$anonfun$genCode$2.apply(Expression.scala:105)\r\n\tat scala.Option.getOrElse(Option.scala:121)\r\n\tat org.apache.spark.sql.catalyst.expressions.Expression.genCode(Expression.scala:105)\r\n\tat org.apache.spark.sql.catalyst.expressions.Cast.genCode(Cast.scala:655)\r\n\tat org.apache.spark.sql.catalyst.expressions.Alias.genCode(namedExpressions.scala:155)\r\n\tat org.apache.spark.sql.execution.ProjectExec$$anonfun$6.apply(basicPhysicalOperators.scala:60)\r\n\tat org.apache.spark.sql.execution.ProjectExec$$anonfun$6.apply(basicPhysicalOperators.scala:60)\r\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\r\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\r\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\r\n\tat org.apache.spark.sql.execution.ProjectExec.doConsume(basicPhysicalOperators.scala:60)\r\n\tat org.apache.spark.sql.execution.CodegenSupport$class.consume(WholeStageCodegenExec.scala:189)\r\n\tat org.apache.spark.sql.execution.InputAdapter.consume(WholeStageCodegenExec.scala:374)\r\n\tat org.apache.spark.sql.execution.InputAdapter.doProduce(WholeStageCodegenExec.scala:403)\r\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:90)\r\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:85)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\r\n\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:85)\r\n\tat org.apache.spark.sql.execution.InputAdapter.produce(WholeStageCodegenExec.scala:374)\r\n\tat org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:45)\r\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:90)\r\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:85)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\r\n\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:85)\r\n\tat org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:35)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doCodeGen(WholeStageCodegenExec.scala:544)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:598)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\r\n\tat org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:247)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:339)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\r\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3389)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)\r\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3369)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2550)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2764)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:254)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:291)\r\n\tat sun.reflect.GeneratedMethodAccessor67.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-4830550ccb7c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'a'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malias\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'lag'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;31m# df.select(lead('a', 1, 0).alias('lag')).show()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\soft\\anaconda\\anaconda_python3.6\\lib\\site-packages\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    378\u001b[0m         \"\"\"\n\u001b[0;32m    379\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 380\u001b[1;33m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    381\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    382\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\soft\\anaconda\\anaconda_python3.6\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1284\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1285\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1286\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1287\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1288\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\soft\\anaconda\\anaconda_python3.6\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\soft\\anaconda\\anaconda_python3.6\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o477.showString.\n: java.lang.UnsupportedOperationException: Cannot evaluate expression: lag(input[0, bigint, true], 1, 0)\r\n\tat org.apache.spark.sql.catalyst.expressions.Unevaluable$class.doGenCode(Expression.scala:261)\r\n\tat org.apache.spark.sql.catalyst.expressions.OffsetWindowFunction.doGenCode(windowExpressions.scala:337)\r\n\tat org.apache.spark.sql.catalyst.expressions.Expression$$anonfun$genCode$2.apply(Expression.scala:108)\r\n\tat org.apache.spark.sql.catalyst.expressions.Expression$$anonfun$genCode$2.apply(Expression.scala:105)\r\n\tat scala.Option.getOrElse(Option.scala:121)\r\n\tat org.apache.spark.sql.catalyst.expressions.Expression.genCode(Expression.scala:105)\r\n\tat org.apache.spark.sql.catalyst.expressions.Cast.doGenCode(Cast.scala:660)\r\n\tat org.apache.spark.sql.catalyst.expressions.Expression$$anonfun$genCode$2.apply(Expression.scala:108)\r\n\tat org.apache.spark.sql.catalyst.expressions.Expression$$anonfun$genCode$2.apply(Expression.scala:105)\r\n\tat scala.Option.getOrElse(Option.scala:121)\r\n\tat org.apache.spark.sql.catalyst.expressions.Expression.genCode(Expression.scala:105)\r\n\tat org.apache.spark.sql.catalyst.expressions.Cast.genCode(Cast.scala:655)\r\n\tat org.apache.spark.sql.catalyst.expressions.Alias.genCode(namedExpressions.scala:155)\r\n\tat org.apache.spark.sql.execution.ProjectExec$$anonfun$6.apply(basicPhysicalOperators.scala:60)\r\n\tat org.apache.spark.sql.execution.ProjectExec$$anonfun$6.apply(basicPhysicalOperators.scala:60)\r\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\r\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\r\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\r\n\tat org.apache.spark.sql.execution.ProjectExec.doConsume(basicPhysicalOperators.scala:60)\r\n\tat org.apache.spark.sql.execution.CodegenSupport$class.consume(WholeStageCodegenExec.scala:189)\r\n\tat org.apache.spark.sql.execution.InputAdapter.consume(WholeStageCodegenExec.scala:374)\r\n\tat org.apache.spark.sql.execution.InputAdapter.doProduce(WholeStageCodegenExec.scala:403)\r\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:90)\r\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:85)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\r\n\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:85)\r\n\tat org.apache.spark.sql.execution.InputAdapter.produce(WholeStageCodegenExec.scala:374)\r\n\tat org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:45)\r\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:90)\r\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:85)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\r\n\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:85)\r\n\tat org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:35)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doCodeGen(WholeStageCodegenExec.scala:544)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:598)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\r\n\tat org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:247)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:339)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\r\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3389)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)\r\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3369)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2550)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2764)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:254)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:291)\r\n\tat sun.reflect.GeneratedMethodAccessor67.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n"
     ]
    }
   ],
   "source": [
    "# 11、lag,lead平移\n",
    "# 很好用的函数啊，特别是在处理时间序列的时候，和pandas的shift很像。\n",
    "from pyspark.sql.functions import lag, lead\n",
    "df = spark.createDataFrame([(1, 2, 3) if i % 2 == 0 else (i, 2 * i, i % 4) \n",
    "                               for i in range(5)],\n",
    "                           [\"a\", \"b\", \"c\"])\n",
    "df.show()\n",
    "\n",
    "# 报错，不知原因\n",
    "# df.select(lag('a', 1, 0).alias('lag')).show()\n",
    "# df.select(lead('a', 1, 0).alias('lag')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

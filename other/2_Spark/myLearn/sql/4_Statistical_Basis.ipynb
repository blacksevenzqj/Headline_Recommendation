{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName('my_first_app_name') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    color  length\n",
      "0   white       5\n",
      "1   green       5\n",
      "2  yellow       6\n",
      "3     red       3\n",
      "4   brown       5\n",
      "5    pink       4\n",
      "6   white       5\n",
      "+------+------+\n",
      "| color|length|\n",
      "+------+------+\n",
      "| white|     5|\n",
      "| green|     5|\n",
      "|yellow|     6|\n",
      "|   red|     3|\n",
      "| brown|     5|\n",
      "|  pink|     4|\n",
      "| white|     5|\n",
      "+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 从pandas dataframe创建spark dataframe\n",
    "colors = ['white','green','yellow','red','brown','pink', 'white']\n",
    "color_df = pd.DataFrame(colors,columns=['color'])\n",
    "color_df['length'] = color_df['color'].apply(len)\n",
    "print(color_df)\n",
    "\n",
    "color_sdf = spark.createDataFrame(color_df)\n",
    "color_sdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+\n",
      "|            uniform|              normal|\n",
      "+-------------------+--------------------+\n",
      "| 0.1982919638208397| 0.06157382353970104|\n",
      "|0.12030715258495939|  1.0854146699817222|\n",
      "|0.44292918521277047| -0.4798519469521663|\n",
      "| 0.2731073068483362|-0.15116027592854422|\n",
      "|   0.87079354700073|-0.27674189870783683|\n",
      "|0.27149331793166864|-0.18575112254167045|\n",
      "| 0.6037143578435027|   0.734722467897308|\n",
      "+-------------------+--------------------+\n",
      "\n",
      "+---+-------------------+-------------------+\n",
      "| id|              rand1|              rand2|\n",
      "+---+-------------------+-------------------+\n",
      "|  0|0.41371264720975787|  0.714105256846827|\n",
      "|  1| 0.1982919638208397|0.19369846818250636|\n",
      "|  2|0.12030715258495939| 0.8122802274304282|\n",
      "|  3|0.44292918521277047|0.31429268272540556|\n",
      "|  4| 0.8898784253886249|0.15864611152259134|\n",
      "|  5| 0.2731073068483362| 0.3221262660507942|\n",
      "|  6|   0.87079354700073| 0.3134176192702436|\n",
      "|  7|0.27149331793166864| 0.3071632607232556|\n",
      "|  8| 0.6037143578435027| 0.8951550661334966|\n",
      "|  9| 0.1435668838975337|  0.337985995393818|\n",
      "+---+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1、随机数\n",
    "# 基于dataframe生成相同行数的随机数\n",
    "from pyspark.sql.functions import rand, randn  # 均匀分布和正太分布函数\n",
    "\n",
    "color_sdf.select(rand(seed=10).alias(\"uniform\"), \n",
    "                randn(seed=27).alias(\"normal\"))\\\n",
    "    .show()\n",
    "\n",
    "# 或者随机生成指定行数的dataframe\n",
    "df = spark.range(0, 10).withColumn('rand1', rand(seed=10)) \\\n",
    "                       .withColumn('rand2', rand(seed=27))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|  r|\n",
      "+---+\n",
      "|3.0|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2、四舍五入\n",
    "from pyspark.sql.functions import round\n",
    "df = spark.createDataFrame([(2.5,)], ['a'])\n",
    "\n",
    "df.select(round('a', 0).alias('r')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "| color|length|\n",
      "+------+------+\n",
      "| white|     5|\n",
      "| green|     5|\n",
      "|yellow|     6|\n",
      "|   red|     3|\n",
      "| brown|     5|\n",
      "|  pink|     4|\n",
      "| white|     5|\n",
      "+------+------+\n",
      "\n",
      "+-----+------+\n",
      "|color|length|\n",
      "+-----+------+\n",
      "|green|     5|\n",
      "| pink|     4|\n",
      "+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3、抽样\n",
    "color_sdf.show()\n",
    "\n",
    "# 抽样\n",
    "sample1 = color_sdf.sample(\n",
    "    withReplacement=False, # 无放回抽样\n",
    "    fraction=0.4,\n",
    "    seed=1000)  \n",
    "sample1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+---+\n",
      "|  a|  b|  c|  d|  e|\n",
      "+---+---+---+---+---+\n",
      "|  2|  0|8.0|  3|  8|\n",
      "|  7|  0|9.0|  3|  7|\n",
      "|  9|  6|NaN|  5|  2|\n",
      "|  0|  6|7.0|  2|  1|\n",
      "|  5|  9|5.0|  0|  0|\n",
      "+---+---+---+---+---+\n",
      "\n",
      "+-------+-----------------+-----------------+---+------------------+-----------------+\n",
      "|summary|                a|                b|  c|                 d|                e|\n",
      "+-------+-----------------+-----------------+---+------------------+-----------------+\n",
      "|  count|                5|                5|  5|                 5|                5|\n",
      "|   mean|              4.6|              4.2|NaN|               2.6|              3.6|\n",
      "| stddev|3.646916505762094|4.024922359499621|NaN|1.8165902124584952|3.646916505762094|\n",
      "|    min|                0|                0|5.0|                 0|                0|\n",
      "|    max|                9|                9|NaN|                 5|                8|\n",
      "+-------+-----------------+-----------------+---+------------------+-----------------+\n",
      "\n",
      "+-------+-----------------+\n",
      "|summary|                a|\n",
      "+-------+-----------------+\n",
      "|  count|                5|\n",
      "|   mean|              4.6|\n",
      "| stddev|3.646916505762094|\n",
      "|    min|                0|\n",
      "|    max|                9|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4、描述性统计\n",
    "# dataframe本身也有基本统计的方法，和pandas一致\n",
    "# 1.生成测试数据\n",
    "df = pd.DataFrame(np.random.rand(5,5),columns=['a','b','c','d','e']).\\\n",
    "    applymap(lambda x: int(x*10))\n",
    "df.iloc[2,2]=np.nan\n",
    "\n",
    "spark_df=spark.createDataFrame(df)\n",
    "spark_df.show()\n",
    "\n",
    "# 2.描述性统计信息：按列统计\n",
    "spark_df.describe().show()\n",
    "\n",
    "# 3.针对一个字段的统计信息\n",
    "spark_df.describe('a').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+\n",
      "|min(length)|max(length)|\n",
      "+-----------+-----------+\n",
      "|          3|          6|\n",
      "+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 5、最大值最小值\n",
    "from pyspark.sql.functions import min, max\n",
    "color_sdf.select(min('length'), max('length')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------------------+\n",
      "|             mean|            stddev|\n",
      "+-----------------+------------------+\n",
      "|4.714285714285714|0.9511897312113418|\n",
      "+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 6、均值、标准差\n",
    "from pyspark.sql.functions import mean, stddev  # 同样是在function里面\n",
    "color_sdf.select(mean('length').alias('mean'),\n",
    "                stddev('length').alias('stddev'))\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5999999999999998\n",
      "0.48\n",
      "0.6\n",
      "0.04087595596566437\n"
     ]
    }
   ],
   "source": [
    "# 7、协方差\n",
    "# 样本协方差\n",
    "print(spark_df.stat.cov('a','b')) # 计算给定列的样本协方差（由它们的名称指定）作为双精度值。 \n",
    "\n",
    "# 总体协方差 和 样本协方差\n",
    "from pyspark.sql.functions import covar_pop, covar_samp\n",
    "print(spark_df.agg(covar_pop(\"a\", \"b\").alias('new_col')).collect()[0][0])  # 返回col1和col2的总体协方差的新列。\n",
    "print(spark_df.agg(covar_samp(\"a\", \"b\").alias('new_col')).collect()[0][0])  # 返回col1和col2的样本协方差的新列。\n",
    "\n",
    "# 皮尔森相关系数\n",
    "print(spark_df.stat.corr('a', 'b'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "      <th>d</th>\n",
       "      <th>e</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.040876</td>\n",
       "      <td>0.156941</td>\n",
       "      <td>0.460381</td>\n",
       "      <td>0.003759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b</th>\n",
       "      <td>0.040876</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.943370</td>\n",
       "      <td>-0.430820</td>\n",
       "      <td>-0.981023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c</th>\n",
       "      <td>0.156941</td>\n",
       "      <td>-0.943370</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.966092</td>\n",
       "      <td>0.860565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d</th>\n",
       "      <td>0.460381</td>\n",
       "      <td>-0.430820</td>\n",
       "      <td>0.966092</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.384908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e</th>\n",
       "      <td>0.003759</td>\n",
       "      <td>-0.981023</td>\n",
       "      <td>0.860565</td>\n",
       "      <td>0.384908</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          a         b         c         d         e\n",
       "a  1.000000  0.040876  0.156941  0.460381  0.003759\n",
       "b  0.040876  1.000000 -0.943370 -0.430820 -0.981023\n",
       "c  0.156941 -0.943370  1.000000  0.966092  0.860565\n",
       "d  0.460381 -0.430820  0.966092  1.000000  0.384908\n",
       "e  0.003759 -0.981023  0.860565  0.384908  1.000000"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pandas的相关系数\n",
    "df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    name     item\n",
      "0  Alice     milk\n",
      "1    Bob    bread\n",
      "2   Mike   butter\n",
      "3  Alice   apples\n",
      "4    Bob  oranges\n",
      "5   Mike     milk\n",
      "6  Alice    bread\n",
      "7    Bob   butter\n",
      "8   Mike   apples\n",
      "9  Alice  oranges\n",
      "+-----+-------+\n",
      "| name|   item|\n",
      "+-----+-------+\n",
      "|Alice|   milk|\n",
      "|  Bob|  bread|\n",
      "| Mike| butter|\n",
      "|Alice| apples|\n",
      "|  Bob|oranges|\n",
      "| Mike|   milk|\n",
      "|Alice|  bread|\n",
      "|  Bob| butter|\n",
      "| Mike| apples|\n",
      "|Alice|oranges|\n",
      "+-----+-------+\n",
      "\n",
      "item   apples  bread  butter  milk  oranges  All\n",
      "name                                            \n",
      "Alice       1      1       0     1        1    4\n",
      "Bob         0      1       1     0        1    3\n",
      "Mike        1      0       1     1        0    3\n",
      "All         2      2       2     2        2   10\n",
      "+---------+------+-----+------+----+-------+\n",
      "|name_item|apples|bread|butter|milk|oranges|\n",
      "+---------+------+-----+------+----+-------+\n",
      "|      Bob|     0|    1|     1|   0|      1|\n",
      "|     Mike|     1|    0|     1|   1|      0|\n",
      "|    Alice|     1|    1|     0|   1|      1|\n",
      "+---------+------+-----+------+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 8、交叉表(列联表)\n",
    "# Create a DataFrame with two columns (name, item)\n",
    "names = [\"Alice\", \"Bob\", \"Mike\"]\n",
    "items = [\"milk\", \"bread\", \"butter\", \"apples\", \"oranges\"]\n",
    "df = pd.DataFrame([(names[i % 3], items[i % 5]) for i in range(10)], columns=['name', 'item'])\n",
    "print(df)\n",
    "sdf = spark.createDataFrame(df)\n",
    "sdf.show()\n",
    "\n",
    "print(pd.crosstab(df['name'], df['item'], margins=True)) # PD\n",
    "sdf.stat.crosstab(\"name\", \"item\").show() # Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+\n",
      "|  a|  b|  c|\n",
      "+---+---+---+\n",
      "|  1|  2|  3|\n",
      "|  1|  2|  1|\n",
      "|  1|  2|  3|\n",
      "|  3|  6|  3|\n",
      "|  1|  2|  3|\n",
      "|  5| 10|  1|\n",
      "|  1|  2|  3|\n",
      "|  7| 14|  3|\n",
      "|  1|  2|  3|\n",
      "|  9| 18|  1|\n",
      "+---+---+---+\n",
      "only showing top 10 rows\n",
      "\n",
      "+-----------+-----------+-----------+\n",
      "|a_freqItems|b_freqItems|c_freqItems|\n",
      "+-----------+-----------+-----------+\n",
      "|    [11, 1]|    [2, 22]|     [1, 3]|\n",
      "+-----------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 9、频繁项目元素\n",
    "# 找出现次数最多的元素(频数分布)\n",
    "df = spark.createDataFrame([(1, 2, 3) if i % 2 == 0 else (i, 2 * i, i % 4) for i in range(100)],\n",
    "                           [\"a\", \"b\", \"c\"])\n",
    "df.show(10)\n",
    "\n",
    "# 下面的代码找到 指定列 出现次数占总的40%以上频繁元素\n",
    "df.stat.freqItems([\"a\", \"b\", \"c\"], 0.4).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. 数学函数\n",
    "| log | 对数 |\n",
    "| log2 | 以2为底的对数 |\n",
    "| factorial | 阶乘 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+\n",
      "|  a|  b|  c|\n",
      "+---+---+---+\n",
      "|  1|  2|  3|\n",
      "|  1|  2|  1|\n",
      "|  1|  2|  3|\n",
      "|  3|  6|  3|\n",
      "|  1|  2|  3|\n",
      "|  5| 10|  1|\n",
      "|  1|  2|  3|\n",
      "|  7| 14|  3|\n",
      "|  1|  2|  3|\n",
      "|  9| 18|  1|\n",
      "+---+---+---+\n",
      "\n",
      "+-----------------+\n",
      "|count(DISTINCT a)|\n",
      "+-----------------+\n",
      "|                5|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 11、元素去重计数\n",
    "from pyspark.sql import functions as func\n",
    "sdf = spark.createDataFrame([(1, 2, 3) if i % 2 == 0 else (i, 2 * i, i % 4) for i in range(10)],\n",
    "                           [\"a\", \"b\", \"c\"])\n",
    "sdf.show()\n",
    "\n",
    "# agg(*exprs)\n",
    "# 在没有组的情况下汇总整个DataFrame（df.groupBy.agg（）的简写）。\n",
    "sdf.agg(func.countDistinct('a')).show() # 看不懂"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(max(a)=9)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf.agg({\"a\": \"max\"}).collect() # 等于 sdf.groupBy().agg({\"a\":\"max\"}).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(a=7, max(b)=14),\n",
       " Row(a=9, max(b)=18),\n",
       " Row(a=5, max(b)=10),\n",
       " Row(a=1, max(b)=2),\n",
       " Row(a=3, max(b)=6)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 区别于\n",
    "sdf.groupBy(\"a\").agg({\"b\":\"max\"}).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+------+\n",
      "|   a|grouping(a)|sum(b)|\n",
      "+----+-----------+------+\n",
      "|null|          1|    60|\n",
      "|   1|          0|    12|\n",
      "|   3|          0|     6|\n",
      "|   5|          0|    10|\n",
      "|   7|          0|    14|\n",
      "|   9|          0|    18|\n",
      "+----+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 12、聚合函数 grouping \n",
    "sdf.cube(\"a\").agg(func.grouping(\"a\"), func.sum(\"b\")).orderBy(\"a\").show() # 看不懂"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13、聚合函数 grouping_id\n",
    "df.cube(\"a\").agg(grouping_id(), sum(\"b\")).orderBy(\"a\").show() # 看不懂"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"MyLearn\")\n",
    "sc = SparkContext(conf = conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helllo张无忌\n",
      "Helllo赵敏\n",
      "Helllo周芷若\n",
      "['Helllo张无忌', 'Helllo赵敏', 'Helllo周芷若']\n"
     ]
    }
   ],
   "source": [
    "# 一、Value类型：\n",
    "# 1、map\n",
    "names = [\"张无忌\", \"赵敏\", \"周芷若\"]\n",
    "listRDD = sc.parallelize(names)\n",
    "\n",
    "temp = listRDD.map(lambda name : \"Helllo\" + name)\n",
    "\n",
    "temp.foreach(lambda strs : print(strs)) # foreach是在Executor中执行，所以在这不会打印\n",
    "for strs in temp.collect():\n",
    "    print(strs)\n",
    "print(temp.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello张无忌\n",
      "Hello赵敏\n",
      "Hello周芷若\n",
      "['Hello张无忌', 'Hello赵敏', 'Hello周芷若']\n"
     ]
    }
   ],
   "source": [
    "# 2、flatMap：\n",
    "names = [\"张无忌\", \"赵敏\", \"周芷若\"]\n",
    "listRDD = sc.parallelize(names)\n",
    "\n",
    "# flatMap会自动将元组打开，元组中的每个元素添加进列表中；\n",
    "# temp = listRDD.flatMap(lambda name : (name, \"Hello\" + name)) \n",
    "\n",
    "temp = listRDD.flatMap(lambda name : [\"Hello\" + name]) # flatMap要求得到的列表类型\n",
    "\n",
    "temp.foreach(lambda strs : print(strs)) # foreach是在Executor中执行\n",
    "for strs in temp.collect():\n",
    "    print(strs)\n",
    "print(temp.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello张无忌\n",
      "Hello赵敏\n",
      "Hello宋青书\n",
      "Hello周芷若\n",
      "Hello刘德华\n",
      "Hello张学友\n",
      "['Hello张无忌', 'Hello赵敏', 'Hello宋青书', 'Hello周芷若', 'Hello刘德华', 'Hello张学友']\n"
     ]
    }
   ],
   "source": [
    "names = (\"张无忌 赵敏\", \"宋青书 周芷若\", \"刘德华\", \"张学友\")\n",
    "listRDD = sc.parallelize(names)\n",
    "\n",
    "temp = listRDD.flatMap(lambda name: name.split(\" \")).map(lambda name: \"Hello\" + name)\n",
    "\n",
    "temp.foreach(lambda strs : print(strs)) # foreach是在Executor中执行\n",
    "for strs in temp.collect():\n",
    "    print(strs)\n",
    "print(temp.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 7]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3、mapPartitions(f, preservesPartitioning=False)\n",
    "# 与map不同，map是对每一个元素用函数作用；而mapPartitions是对每一个分区用一个函数去作用，每一个分区的元素先构成一个迭代器iterator，iterator是一个像列表，但里面的元素又保持分布式特点的一类对象;输入的参数就是这个iterator，然后对iterator进行运算，iterator支持的函数不是太多，sum,count等一些spark定义的基本函数应该都是支持的。但如果要进行更为复杂的一些个性化函数运算，可以就用不了。实践中发生可以通过[x for i in iterator]的方式，将iterator转换为列表，然后就可以进行各种操作。但是这样在分区内部或分组内部就失去了分布式运算的特点。\n",
    "# yield是生成的意思，但是在python中则是作为生成器理解，生成器的用处主要可以迭代，这样简化了很多运算模型。\n",
    "rdd = sc.parallelize([1, 2, 3, 4], 2)\n",
    "def f(iterator): yield sum(iterator)\n",
    "rdd.mapPartitions(f).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello spark']\n"
     ]
    }
   ],
   "source": [
    "# 3、# mapPartitions(f, preservesPartitioning=False)：具体看代码：“merge_data.ipynb” 中 words_df = article_data.rdd.mapPartitions(segmentation).toDF(['article_id', 'channel_id', 'words'])\n",
    "data = [\"hello spark\", \"hello world\", \"hello world\"]\n",
    "rdd = sc.parallelize(data, 2)\n",
    "mapRDD = rdd.mapPartitions(lambda iterator: [i for i in iterator if 'spark' in i]).collect()\n",
    "print(mapRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4、mapPartitionsWithIndex(f, preservesPartitioning=False)\n",
    "# 通过在这个RDD的每个分区上应用一个函数来返回一个新的RDD，同时跟踪原始分区的索引。为对索引进行操作提供可能\n",
    "rdd = sc.parallelize([1, 2, 3, 4], 4)\n",
    "def f(splitIndex, iterator): yield splitIndex\n",
    "rdd.mapPartitionsWithIndex(f).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0_hello spark']\n"
     ]
    }
   ],
   "source": [
    "data = [\"hello spark\", \"hello world\", \"hello world\"]\n",
    "rdd = sc.parallelize(data, 2)\n",
    "mapRDD = rdd.mapPartitionsWithIndex(lambda index,iterator: [str(index) + \"_\" + i for i in iterator if 'spark' in i]).collect()\n",
    "print(mapRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['0_hello spark'], []]\n"
     ]
    }
   ],
   "source": [
    "# 5、glom：将每个分区形成一个数组\n",
    "data = [\"hello spark\", \"hello world\", \"hello world\"]\n",
    "rdd = sc.parallelize(data, 2)\n",
    "mapRDD = rdd.mapPartitionsWithIndex(lambda index,iterator: [str(index) + \"_\" + i for i in iterator if 'spark' in i])\n",
    "\n",
    "# glom()：将每个分区形成一个数组；collect()：将所有数据收集到一个数组中。\n",
    "# 两者结合：glom每个分区形成一个数组，collect在外层再套上一个数组：Array(Array(1,2,3), Array(4,5,6), ...)\n",
    "glomv = mapRDD.glom().collect() # 两个分区，\n",
    "print(glomv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['0_(0, <pyspark.resultiterable.ResultIterable object at 0x0000023D92262F28>)'], ['1_(1, <pyspark.resultiterable.ResultIterable object at 0x000001BAFFB52F28>)'], [], [], []]\n"
     ]
    }
   ],
   "source": [
    "# 6、groupby：\n",
    "array = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]\n",
    "rdd = sc.parallelize(array, 3)\n",
    "result = rdd.groupBy(lambda x: x%2)\n",
    "\n",
    "mapRDD = result.mapPartitionsWithIndex(lambda index,iterator: [str(index) + \"_\" + str(i) for i in iterator])\n",
    "glomv = mapRDD.glom().collect() \n",
    "print(glomv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "张无忌 赵敏\n",
      "张学友\n",
      "['张无忌 赵敏', '张学友']\n"
     ]
    }
   ],
   "source": [
    "# 7、filter：\n",
    "names = (\"张无忌 赵敏\", \"宋青书 周芷若\", \"刘德华\", \"张学友\")\n",
    "listRDD = sc.parallelize(names, 3)\n",
    "\n",
    "temp = listRDD.filter(lambda name: name.startswith(\"张\"))\n",
    "\n",
    "temp.foreach(lambda strs : print(strs)) # foreach是在Executor中执行\n",
    "temp.foreachPartition(lambda iterator : print([i for i in iterator])) # 3个分区分开打印\n",
    "for strs in temp.collect():\n",
    "    print(strs)\n",
    "print(temp.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "周芷若\n",
      "['周芷若']\n"
     ]
    }
   ],
   "source": [
    "# 8、sample：\n",
    "names = (\"张无忌\", \"赵敏\", \"周芷若\", \"张学友\")\n",
    "listRDD = sc.parallelize(names, 3)\n",
    "\n",
    "temp = listRDD.sample(False, 0.33, 99)\n",
    "\n",
    "temp.foreach(lambda strs : print(strs)) # foreach是在Executor中执行\n",
    "temp.foreachPartition(lambda iterator : print([i for i in iterator])) # 3个分区分开打印\n",
    "for strs in temp.collect():\n",
    "    print(strs)\n",
    "print(temp.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "张无忌\n",
      "赵敏\n",
      "周芷若\n",
      "张学友\n",
      "['张无忌', '赵敏', '周芷若', '张学友']\n"
     ]
    }
   ],
   "source": [
    "# 9、distinct：\n",
    "lists = [\"张无忌\", \"赵敏\", \"周芷若\", \"张学友\", \"赵敏\", \"周芷若\"]\n",
    "listRDD = sc.parallelize(lists, 3)\n",
    "nameRDD = listRDD.distinct()\n",
    "\n",
    "nameRDD.foreachPartition(lambda iterator: print([i for i in iterator]))\n",
    "for strs in nameRDD.collect():\n",
    "    print(strs)\n",
    "print(nameRDD.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['0_xuruyun1', '0_xuruyun2', '0_xuruyun3'], ['1_xuruyun4', '1_xuruyun5', '1_xuruyun7'], ['2_xuruyun8', '2_xuruyun9', '2_xuruyun10', '2_xuruyun11', '2_xuruyun12']]\n",
      "------------------------------\n",
      "[[], ['1_0_xuruyun1', '1_0_xuruyun2', '1_0_xuruyun3'], [], [], ['4_2_xuruyun8', '4_2_xuruyun9', '4_2_xuruyun10', '4_2_xuruyun11', '4_2_xuruyun12'], ['5_1_xuruyun4', '5_1_xuruyun5', '5_1_xuruyun7']]\n"
     ]
    }
   ],
   "source": [
    "# 10、Repartition：\n",
    "lists = [\"xuruyun1\", \"xuruyun2\", \"xuruyun3\", \"xuruyun4\", \"xuruyun5\",\n",
    "      \"xuruyun7\", \"xuruyun8\", \"xuruyun9\", \"xuruyun10\", \"xuruyun11\", \"xuruyun12\"]\n",
    "listRDD = sc.parallelize(lists, 3)\n",
    "\n",
    "mapRDD = listRDD.mapPartitionsWithIndex(lambda index,iterator: [str(index) + \"_\" + str(i) for i in iterator])\n",
    "glomv = mapRDD.glom().collect() \n",
    "print(glomv)\n",
    "\n",
    "print(\"-\"*30)\n",
    "\n",
    "mapRDD2 = mapRDD.repartition(6) # 最终是调用了coalesce：coalesce(numPartitions, shuffle = true)\n",
    "mapRDD3 = mapRDD2.mapPartitionsWithIndex(lambda index,iterator: [str(index) + \"_\" + str(i) for i in iterator])\n",
    "glomv = mapRDD3.glom().collect() \n",
    "print(glomv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['0_xuruyun1', '0_xuruyun2', '0_xuruyun3'], ['1_xuruyun4', '1_xuruyun5', '1_xuruyun7'], ['2_xuruyun8', '2_xuruyun9', '2_xuruyun10', '2_xuruyun11', '2_xuruyun12']]\n",
      "------------------------------\n",
      "[[], ['1_0_xuruyun1', '1_0_xuruyun2', '1_0_xuruyun3'], [], [], ['4_2_xuruyun8', '4_2_xuruyun9', '4_2_xuruyun10', '4_2_xuruyun11', '4_2_xuruyun12'], ['5_1_xuruyun4', '5_1_xuruyun5', '5_1_xuruyun7']]\n"
     ]
    }
   ],
   "source": [
    "# 11、Coalesce：\n",
    "lists = [\"xuruyun1\", \"xuruyun2\", \"xuruyun3\", \"xuruyun4\", \"xuruyun5\",\n",
    "      \"xuruyun7\", \"xuruyun8\", \"xuruyun9\", \"xuruyun10\", \"xuruyun11\", \"xuruyun12\"]\n",
    "listRDD = sc.parallelize(lists, 3)\n",
    "\n",
    "mapRDD = listRDD.mapPartitionsWithIndex(lambda index,iterator: [str(index) + \"_\" + str(i) for i in iterator])\n",
    "glomv = mapRDD.glom().collect() \n",
    "print(glomv)\n",
    "\n",
    "print(\"-\"*30)\n",
    "\n",
    "'''\n",
    "1、压缩Partition：\n",
    "RDD.coalesce(x)：默认shuffle=false时，只有Partition在一台机器上的时候才能生效，如果Partition分布在多台机器上不生效。\n",
    "shuffle=true时，Partition所在多台机器进行shuffle代价大。\n",
    "2、扩展Partition：\n",
    "RDD.coalesce(x, shuffle=True)：这时就相当于repartition了，因为repartition最终调用了coalesce(numPartitions, shuffle = true)\n",
    "''''\n",
    "mapRDD2 = mapRDD.coalesce(6, True)\n",
    "mapRDD3 = mapRDD2.mapPartitionsWithIndex(lambda index,iterator: [str(index) + \"_\" + str(i) for i in iterator])\n",
    "glomv = mapRDD3.glom().collect() \n",
    "print(glomv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['0_65:leo', '0_50:tom'], ['1_100:marry', '1_85:jack', '1_66:feiji']]\n",
      "[[(100, 'marry'), (85, 'jack')], [(66, 'feiji'), (65, 'leo'), (50, 'tom')]]\n",
      "[['0_100:marry', '0_85:jack'], ['1_66:feiji', '1_65:leo', '1_50:tom']]\n"
     ]
    }
   ],
   "source": [
    "# 12.1、SortBy：\n",
    "scoreList = ((65, \"leo\"), (50, \"tom\"), (100, \"marry\"), (85, \"jack\"), (66, \"feiji\"))\n",
    "scores = sc.parallelize(scoreList, 2)\n",
    "rddMap = scores.mapPartitionsWithIndex(lambda index,iterator: [str(index) + \"_\" + str(i[0]) + \":\" + str(i[1]) for i in iterator])\n",
    "glomv = rddMap.glom().collect() \n",
    "print(glomv)\n",
    "\n",
    "# 当设置的平行度>1时：sortBy会自动重分区\n",
    "sortedScores = scores.sortBy(lambda x: x[0], False)\n",
    "glomv = sortedScores.glom().collect() \n",
    "print(glomv)\n",
    "\n",
    "rddMap = sortedScores.mapPartitionsWithIndex(lambda index,iterator: [str(index) + \"_\" + str(i[0]) + \":\" + str(i[1]) for i in iterator])\n",
    "glomv = rddMap.glom().collect() \n",
    "print(glomv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['0_65:leo', '0_50:tom'], ['1_100:marry', '1_85:jack', '1_66:feiji']]\n",
      "[[(100, 'marry'), (85, 'jack')], [(66, 'feiji'), (65, 'leo'), (50, 'tom')]]\n",
      "[['0_100:marry', '0_85:jack'], ['1_66:feiji', '1_65:leo', '1_50:tom']]\n"
     ]
    }
   ],
   "source": [
    "# 12.2、SortByKey：\n",
    "scoreList = ((65, \"leo\"), (50, \"tom\"), (100, \"marry\"), (85, \"jack\"), (66, \"feiji\"))\n",
    "scores = sc.parallelize(scoreList, 2)\n",
    "rddMap = scores.mapPartitionsWithIndex(lambda index,iterator: [str(index) + \"_\" + str(i[0]) + \":\" + str(i[1]) for i in iterator])\n",
    "glomv = rddMap.glom().collect() \n",
    "print(glomv)\n",
    "\n",
    "# 当设置的平行度>1时：sortBy会自动重分区\n",
    "sortedScores = scores.sortByKey(False)\n",
    "glomv = sortedScores.glom().collect() \n",
    "print(glomv)\n",
    "\n",
    "rddMap = sortedScores.mapPartitionsWithIndex(lambda index,iterator: [str(index) + \"_\" + str(i[0]) + \":\" + str(i[1]) for i in iterator])\n",
    "glomv = rddMap.glom().collect() \n",
    "print(glomv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13、pipe(command, env=None, checkCode=False)\n",
    "# 通过管道向后面环节输出command处理过的结果，具体功能就体现在command，command为linux命令。 shell脚本，注意：是针对分区的\n",
    "# pipe函数中的'cat'为linux命令，表示打印内容。\n",
    "# sc.parallelize(['1', '2', '', '3']).pipe('cat').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['xuruyun1', 'xuruyun2'], ['xuruyun3', 'xuruyun4'], ['xuruyun5', 'xuruyun7']]\n",
      "[['xuruyun1', 'xuruyun2'], ['xuruyun3', 'xuruyun4'], ['xuruyun5', 'xuruyun7']]\n",
      "[['xuruyun1', 'xuruyun2'], ['xuruyun3', 'xuruyun4'], ['xuruyun5', 'xuruyun7'], ['xuruyun1', 'xuruyun2'], ['xuruyun3', 'xuruyun4'], ['xuruyun5', 'xuruyun7']]\n"
     ]
    }
   ],
   "source": [
    "# 14、union：分区数累加\n",
    "lists = [\"xuruyun1\", \"xuruyun2\", \"xuruyun3\", \"xuruyun4\", \"xuruyun5\",\"xuruyun7\"]\n",
    "listRDD = sc.parallelize(lists, 3)\n",
    "print(listRDD.glom().collect())\n",
    "\n",
    "lists2 = [\"xuruyun1\", \"xuruyun2\", \"xuruyun3\", \"xuruyun4\", \"xuruyun5\",\"xuruyun7\"]\n",
    "listRDD2 = sc.parallelize(lists2, 3)\n",
    "print(listRDD2.glom().collect())\n",
    "\n",
    "glomv = listRDD.union(listRDD2).glom().collect() \n",
    "print(glomv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 3, 4, 5]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = sc.parallelize([1,2,3])\n",
    "b = sc.parallelize([3,4,5])\n",
    "a.union(b).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4, 1, 3]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = sc.parallelize([1, 2, 3])\n",
    "b = sc.parallelize([3, 4, 2])\n",
    "a.union(b).distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['xuruyun1'], ['xuruyun2', 'xuruyun3'], ['xuruyun4'], ['xuruyun5', 'xuruyun7']]\n",
      "[['xuruyun1'], ['xuruyun2', 'xuruyun3'], ['xuruyun4'], ['xuruyun5', 'xuruyun7']]\n",
      "[[], [], [], [], [], [], [], []]\n"
     ]
    }
   ],
   "source": [
    "# 15、subtract：分区数累加\n",
    "lists = [\"xuruyun1\", \"xuruyun2\", \"xuruyun3\", \"xuruyun4\", \"xuruyun5\",\"xuruyun7\"]\n",
    "listRDD = sc.parallelize(lists, 4)\n",
    "print(listRDD.glom().collect())\n",
    "\n",
    "lists2 = [\"xuruyun1\", \"xuruyun2\", \"xuruyun3\", \"xuruyun4\", \"xuruyun5\",\"xuruyun7\"]\n",
    "listRDD2 = sc.parallelize(lists2, 4)\n",
    "print(listRDD2.glom().collect())\n",
    "\n",
    "glomv = listRDD.subtract(listRDD2).glom().collect() \n",
    "print(glomv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['xuruyun1', 'xuruyun2'], ['xuruyun3', 'xuruyun4'], ['xuruyun5', 'xuruyun7']]\n",
      "[['xuruyun1', 'xuruyun2'], ['xuruyun3', 'xuruyun4'], ['xuruyun5', 'xuruyun7']]\n",
      "[[], ['xuruyun2'], ['xuruyun3', 'xuruyun4', 'xuruyun5'], ['xuruyun1'], ['xuruyun7'], []]\n"
     ]
    }
   ],
   "source": [
    "# 16、intersection：分区数累加\n",
    "lists = [\"xuruyun1\", \"xuruyun2\", \"xuruyun3\", \"xuruyun4\", \"xuruyun5\",\"xuruyun7\"]\n",
    "listRDD = sc.parallelize(lists, 3)\n",
    "print(listRDD.glom().collect() )\n",
    "\n",
    "lists2 = [\"xuruyun1\", \"xuruyun2\", \"xuruyun3\", \"xuruyun4\", \"xuruyun5\",\"xuruyun7\"]\n",
    "listRDD2 = sc.parallelize(lists2, 3)\n",
    "print(listRDD2.glom().collect())\n",
    "\n",
    "glomv = listRDD.intersection(listRDD2).glom().collect() \n",
    "print(glomv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['xuruyun1'], ['xuruyun2', 'xuruyun3']]\n",
      "[['zhangxueyou1'], ['zhangxueyou2', 'zhangxueyou3']]\n",
      "[[('xuruyun1', 'zhangxueyou1')], [('xuruyun1', 'zhangxueyou2'), ('xuruyun1', 'zhangxueyou3')], [('xuruyun2', 'zhangxueyou1'), ('xuruyun3', 'zhangxueyou1')], [('xuruyun2', 'zhangxueyou2'), ('xuruyun2', 'zhangxueyou3'), ('xuruyun3', 'zhangxueyou2'), ('xuruyun3', 'zhangxueyou3')]]\n"
     ]
    }
   ],
   "source": [
    "# 17、cartesian：分区数累加\n",
    "lists = [\"xuruyun1\", \"xuruyun2\", \"xuruyun3\"]\n",
    "listRDD = sc.parallelize(lists, 2)\n",
    "print(listRDD.glom().collect() )\n",
    "\n",
    "lists2 = [\"zhangxueyou1\", \"zhangxueyou2\", \"zhangxueyou3\"]\n",
    "listRDD2 = sc.parallelize(lists2, 2)\n",
    "print(listRDD2.glom().collect())\n",
    "\n",
    "glomv = listRDD.cartesian(listRDD2).glom().collect() \n",
    "print(glomv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['xuruyun1'], ['xuruyun2', 'xuruyun3']]\n",
      "[['zhangxueyou1'], ['zhangxueyou2', 'zhangxueyou3']]\n",
      "[[('xuruyun1', 'zhangxueyou1')], [('xuruyun2', 'zhangxueyou2'), ('xuruyun3', 'zhangxueyou3')]]\n"
     ]
    }
   ],
   "source": [
    "# 18、zip：1、分区数相同；2、每个分区中元素个数相同。另：分区数不变。\n",
    "lists = [\"xuruyun1\", \"xuruyun2\", \"xuruyun3\"]\n",
    "listRDD = sc.parallelize(lists, 2)\n",
    "print(listRDD.glom().collect() )\n",
    "\n",
    "lists2 = [\"zhangxueyou1\", \"zhangxueyou2\", \"zhangxueyou3\"]\n",
    "listRDD2 = sc.parallelize(lists2, 2)\n",
    "print(listRDD2.glom().collect())\n",
    "\n",
    "glomv = listRDD.zip(listRDD2).glom().collect() \n",
    "print(glomv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('C', ('c1', 'c2')), ('C', ('c1', 'c3')), ('A', ('a1', 'a2')), ('D', ('d1', None)), ('F', ('f1', None)), ('F', ('f2', None)), ('E', (None, 'e1'))]\n"
     ]
    }
   ],
   "source": [
    "# 19、join：\n",
    "a = sc.parallelize([(\"A\", \"a1\"), (\"C\", \"c1\"), (\"D\", \"d1\"), (\"F\", \"f1\"), (\"F\", \"f2\")])\n",
    "b = sc.parallelize([(\"A\", \"a2\"), (\"C\", \"c2\"), (\"C\", \"c3\"), (\"E\", \"e1\")])\n",
    "print(a.fullOuterJoin(b).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(2, 2), (4, 4), (2, 2), (4, 4)], [(1, 1), (3, 3), (1, 1)]]\n",
      "{(4, 4), (2, 2)} {(1, 1), (3, 3)}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 二、Key-Value类型\n",
    "# 1、partitionBy(numPartitions, partitionFunc=<function portable_hash>)\n",
    "# 返回使用指定的分区器分区的RDD的副本\n",
    "# set().intersection 取交集\n",
    "pairs = sc.parallelize([1, 2, 3, 4, 2, 4, 1]).map(lambda x: (x, x))\n",
    "\n",
    "sets = pairs.partitionBy(2).glom().collect()\n",
    "print(sets)\n",
    "print(set(sets[0]), set(sets[1]))\n",
    "len(set(sets[0]).intersection(set(sets[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('hello', 3), ('spark', 1), ('world', 2)]\n"
     ]
    }
   ],
   "source": [
    "# 2、reduceByKey：\n",
    "data = [\"hello spark\", \"hello world\", \"hello world\"]\n",
    "rdd = sc.parallelize(data)\n",
    "mapRdd = rdd.flatMap(lambda line:line.split(\" \")).map(lambda x:(x,1))\n",
    "reduceByKeyRdd = mapRdd.reduceByKey(lambda a,b:a+b) # 意思是 相同key的value进行操作\n",
    "print(reduceByKeyRdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('hello', <pyspark.resultiterable.ResultIterable object at 0x0000015BFEB201D0>), ('spark', <pyspark.resultiterable.ResultIterable object at 0x0000015BFEB20208>), ('world', <pyspark.resultiterable.ResultIterable object at 0x0000015BFEB208D0>)]\n",
      "[{'hello': [1, 1, 1]}, {'spark': [1]}, {'world': [1, 1]}]\n"
     ]
    }
   ],
   "source": [
    "# 3、groupByKey：\n",
    "data = [\"hello spark\", \"hello world\", \"hello world\"]\n",
    "rdd = sc.parallelize(data)\n",
    "mapRdd = rdd.flatMap(lambda line:line.split(\" \")).map(lambda x:(x,1))\n",
    "groupByRdd = mapRdd.groupByKey()\n",
    "print(groupByRdd.collect())\n",
    "print(groupByRdd.map(lambda x:{x[0]:list(x[1])}).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('world', 2), ('spark', 1), ('hello', 3)]\n",
      "[('hello', 3), ('world', 2), ('spark', 1)]\n"
     ]
    }
   ],
   "source": [
    "# 6、sortByKey：\n",
    "data = [\"hello spark\", \"hello world\", \"hello world\"]\n",
    "rdd = sc.parallelize(data)\n",
    "mapRDD = rdd.flatMap(lambda line: line.split(\" \")).map(lambda x: (x, 1))\n",
    "reduceByKeyRdd = mapRdd.reduceByKey(lambda a, b: a + b)\n",
    "print(reduceByKeyRdd.sortByKey(False).collect())\n",
    "print(reduceByKeyRdd.map(lambda x:(x[1],x[0])).sortByKey(False).map(lambda x:(x[1],x[0])).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

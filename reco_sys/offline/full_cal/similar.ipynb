{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "\n",
    "# 2.7.4、增量更新-文章向量计算\n",
    "# 如果当前代码文件运行测试需要加入修改路径，避免出现后导包问题\n",
    "BASE_DIR = os.path.dirname(os.path.dirname(os.getcwd()))\n",
    "sys.path.insert(0, os.path.join(BASE_DIR))\n",
    "\n",
    "PYSPARK_PYTHON = \"/miniconda2/envs/py365/bin/python\"\n",
    "# 当存在多个版本时，不指定很可能会导致出错\n",
    "os.environ[\"PYSPARK_PYTHON\"] = PYSPARK_PYTHON\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = PYSPARK_PYTHON\n",
    "\n",
    "from offline import SparkSessionBase\n",
    "from setting.default import CHANNEL_INFO\n",
    "from pyspark.ml.feature import Word2Vec\n",
    "\n",
    "\n",
    "\n",
    "class TrainWord2VecModel(SparkSessionBase):\n",
    "\n",
    "    SPARK_APP_NAME = \"Word2Vec\"\n",
    "    SPARK_URL = \"local\"\n",
    "    \n",
    "    ENABLE_HIVE_SUPPORT = True\n",
    "\n",
    "    def __init__(self):\n",
    "        self.spark = self._create_spark_session()\n",
    "\n",
    "\n",
    "w2v = TrainWord2VecModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+------------+--------------------+--------------------+--------------------+\n",
      "|article_id|channel_id|channel_name|               title|             content|            sentence|\n",
      "+----------+----------+------------+--------------------+--------------------+--------------------+\n",
      "|     12237|        18|      python|想学习区块链？那就用 Python...|<div id=\"article_...|python,想学习区块链？那就用...|\n",
      "|     12238|        18|      python|鲜为人知的 Python 语法 使...|<p>所有人（好吧，不是所有人）都...|python,鲜为人知的 Pyth...|\n",
      "|     12243|        18|      python|手把手教你写网络爬虫（4）：Scr...|<div id=\"cnblogs_...|python,手把手教你写网络爬虫...|\n",
      "|     12245|        18|      python|手把手教你写网络爬虫（5）：Pha...|<div id=\"cnblogs_...|python,手把手教你写网络爬虫...|\n",
      "|     12247|        18|      python|用 Plumbum 开发 Pyth...|<div id=\"article_...|python,用 Plumbum ...|\n",
      "+----------+----------+------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 训练一个频道的模型\n",
    "w2v.spark.sql(\"use article\")\n",
    "article_data = w2v.spark.sql(\"select * from article_data where channel_id=18 limit 5\")\n",
    "article_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文章数据进行分词处理,得到分词结果\n",
    "# 分词\n",
    "def segmentation(partition):\n",
    "    import os\n",
    "    import re\n",
    "\n",
    "    import jieba\n",
    "    import jieba.analyse\n",
    "    import jieba.posseg as pseg\n",
    "    import codecs\n",
    "\n",
    "    abspath = \"/root/backup/words\"\n",
    "\n",
    "    # 结巴加载用户词典\n",
    "    userDict_path = os.path.join(abspath, \"ITKeywords.txt\")\n",
    "    jieba.load_userdict(userDict_path)\n",
    "\n",
    "    # 停用词文本\n",
    "    stopwords_path = os.path.join(abspath, \"stopwords.txt\")\n",
    "\n",
    "    def get_stopwords_list():\n",
    "        \"\"\"返回stopwords列表\"\"\"\n",
    "        stopwords_list = [i.strip()\n",
    "                          for i in codecs.open(stopwords_path).readlines()]\n",
    "        return stopwords_list\n",
    "\n",
    "    # 所有的停用词列表\n",
    "    stopwords_list = get_stopwords_list()\n",
    "\n",
    "    # 分词\n",
    "    def cut_sentence(sentence):\n",
    "        \"\"\"对切割之后的词语进行过滤，去除停用词，保留名词，英文和自定义词库中的词，长度大于2的词\"\"\"\n",
    "        # print(sentence,\"*\"*100)\n",
    "        # eg:[pair('今天', 't'), pair('有', 'd'), pair('雾', 'n'), pair('霾', 'g')]\n",
    "        seg_list = pseg.lcut(sentence)\n",
    "        seg_list = [i for i in seg_list if i.flag not in stopwords_list]\n",
    "        filtered_words_list = []\n",
    "        for seg in seg_list:\n",
    "            # print(seg)\n",
    "            if len(seg.word) <= 1:\n",
    "                continue\n",
    "            elif seg.flag == \"eng\":\n",
    "                if len(seg.word) <= 2:\n",
    "                    continue\n",
    "                else:\n",
    "                    filtered_words_list.append(seg.word)\n",
    "            elif seg.flag.startswith(\"n\"):\n",
    "                filtered_words_list.append(seg.word)\n",
    "            elif seg.flag in [\"x\", \"eng\"]:  # 是自定一个词语或者是英文单词\n",
    "                filtered_words_list.append(seg.word)\n",
    "        return filtered_words_list\n",
    "\n",
    "    for row in partition:\n",
    "        sentence = re.sub(\"<.*?>\", \"\", row.sentence)    # 替换掉标签数据\n",
    "        words = cut_sentence(sentence)\n",
    "        yield row.article_id, row.channel_id, words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_df = article_data.rdd.mapPartitions(segmentation).toDF(['article_id', 'channel_id', 'words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+--------------------+\n",
      "|article_id|channel_id|               words|\n",
      "+----------+----------+--------------------+\n",
      "|     12237|        18|[python, 区块链, Pyt...|\n",
      "|     12238|        18|[python, Python, ...|\n",
      "|     12243|        18|[python, 手把手, 网络,...|\n",
      "|     12245|        18|[python, 手把手, 网络,...|\n",
      "|     12247|        18|[python, Plumbum,...|\n",
      "+----------+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "words_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 直接调用word2vec训练\n",
    "w2v_model = Word2Vec(vectorSize=100, inputCol='words', outputCol='model', minCount=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = w2v_model.fit(words_df)\n",
    "model.write().overwrite().save(\"hdfs://hadoop-master:9000/headlines/models/test.word2vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+\n",
      "|        word|              vector|\n",
      "+------------+--------------------+\n",
      "|        函数参数|[0.03575931116938...|\n",
      "|          流程|[0.08695186674594...|\n",
      "|        配置文件|[0.10811857879161...|\n",
      "|   recipient|[-0.1994543075561...|\n",
      "|      enable|[0.05931602790951...|\n",
      "|    register|[-0.0666727498173...|\n",
      "|         fib|[0.01022425200790...|\n",
      "|          函数|[0.15153197944164...|\n",
      "|        read|[0.04327561706304...|\n",
      "|          装饰|[0.10575848817825...|\n",
      "|         数据流|[0.09737470746040...|\n",
      "|QuotesSpider|[0.00672959722578...|\n",
      "|     Request|[0.15434546768665...|\n",
      "|      format|[0.03352830186486...|\n",
      "|         for|[0.16336914896965...|\n",
      "|          对象|[0.16593627631664...|\n",
      "|         Set|[0.06395456194877...|\n",
      "|      encode|[-0.0407770648598...|\n",
      "|     program|[-0.0036022611893...|\n",
      "|     network|[-0.0405408889055...|\n",
      "+------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1、加载某个频道模型，得到每个词的向量\n",
    "from pyspark.ml.feature import Word2VecModel\n",
    "\n",
    "wv = Word2VecModel.load(\"hdfs://hadoop-master:9000/headlines/models/test.word2vec\")\n",
    "vectors = wv.getVectors()\n",
    "\n",
    "# 它的Spark版本是2.2.2，我的是2.1.0计算出来的模型不能通用。\n",
    "# channel_id = 18\n",
    "# channel = \"python\"\n",
    "# wv_model = Word2VecModel.load(\n",
    "#                 \"hdfs://hadoop-master:9000/headlines/models/word2vec_model/channel_%d_%s.word2vec\" % (channel_id, channel))\n",
    "# vectors = wv_model.getVectors()\n",
    "\n",
    "vectors.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+--------------------+--------------------+\n",
      "|article_id|channel_id|            keywords|              topics|\n",
      "+----------+----------+--------------------+--------------------+\n",
      "|     13098|        18|Map(pre -> 0.6040...|[__, object, 属性, ...|\n",
      "|     13248|        18|Map(有限元 -> 5.2929...|[有限元, 代码分析, 案例, z...|\n",
      "|     13401|        18|Map(pre -> 0.2100...|[补码, 字符串, 李白, typ...|\n",
      "|     13723|        18|Map(pre -> 2.1094...|[acc, bstr, 原地, l...|\n",
      "|     14719|        18|Map(pre -> 0.8814...|[__, ctime, cons,...|\n",
      "|     14846|        18|Map(__ -> 2.54674...|[files, __, folde...|\n",
      "|     15173|        18|Map(人人 -> 0.74986...|[cookie, Python爬虫...|\n",
      "|     15194|        18|Map(dif -> 0.7567...|[display, 课程, lis...|\n",
      "|     15237|        18|Map(pre -> 0.5349...|[__, send, sel, c...|\n",
      "|     15322|        18|Map(pre -> 0.5762...|[Pclass, replace,...|\n",
      "+----------+----------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#2、获取频道的文章画像，得到文章画像的关键词(接着之前增量更新的文章article_profile)\n",
    "# 获取这些文章20个关键词名称，对应 关键词名称 找到 词向量\n",
    "w2v.spark.sql(\"use article\")\n",
    "article_profile = w2v.spark.sql(\"select * from article_profile where channel_id=18 limit 10\")\n",
    "article_profile.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+--------+-------------------+\n",
      "|article_id|channel_id| keyword|             weight|\n",
      "+----------+----------+--------+-------------------+\n",
      "|     13098|        18|    repr| 0.6326590117716192|\n",
      "|     13098|        18|      __| 2.5401122038114203|\n",
      "|     13098|        18|      属性|0.23645924932468856|\n",
      "|     13098|        18|     pre| 0.6040062287555379|\n",
      "|     13098|        18|    code| 0.9531379029975557|\n",
      "|     13098|        18|     def| 0.5063435861497416|\n",
      "|     13098|        18|   color| 1.1337936117177925|\n",
      "|     13098|        18|      定义| 0.1554380122061322|\n",
      "|     13098|        18| Student| 0.5033771372284416|\n",
      "|     13098|        18|getPrice| 0.7404427038950527|\n",
      "|     13098|        18|      方法|0.08080845613717194|\n",
      "|     13098|        18|     div| 0.3434819820586186|\n",
      "|     13098|        18|     str|0.35999033790156054|\n",
      "|     13098|        18|      pa| 0.6651385256756351|\n",
      "|     13098|        18|   slots| 0.6992789472129189|\n",
      "|     13098|        18| cnblogs|0.33926586102013295|\n",
      "|     13098|        18|      函数|0.15015578405898256|\n",
      "|     13098|        18|   style| 2.4777013955852873|\n",
      "|     13098|        18|      &#| 0.4911011561534254|\n",
      "|     13098|        18|   class|0.28891320463243075|\n",
      "+----------+----------+--------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#3、文章画像表 的 关键词 进行 爆炸/展开\n",
    "article_profile.registerTempTable('incremental')\n",
    "keyword_weight = w2v.spark.sql(\"select article_id, channel_id, keyword, weight from incremental LATERAL VIEW explode(keywords) AS keyword, weight\")\n",
    "keyword_weight.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-------+-------------------+------+--------------------+\n",
      "|article_id|channel_id|keyword|             weight|  word|              vector|\n",
      "+----------+----------+-------+-------------------+------+--------------------+\n",
      "|     13098|        18|     __| 2.5401122038114203|    __|[-0.0841412693262...|\n",
      "|     13098|        18|     属性|0.23645924932468856|    属性|[0.15526983141899...|\n",
      "|     13098|        18|   code| 0.9531379029975557|  code|[0.08397469669580...|\n",
      "|     13098|        18|    def| 0.5063435861497416|   def|[0.00656610028818...|\n",
      "|     13098|        18|     定义| 0.1554380122061322|    定义|[0.08375884592533...|\n",
      "|     13098|        18|     方法|0.08080845613717194|    方法|[0.15521775186061...|\n",
      "|     13098|        18|    div| 0.3434819820586186|   div|[0.04872748255729...|\n",
      "|     13098|        18|    str|0.35999033790156054|   str|[0.06551016867160...|\n",
      "|     13098|        18|     pa| 0.6651385256756351|    pa|[0.07491271197795...|\n",
      "|     13098|        18|     函数|0.15015578405898256|    函数|[0.15153197944164...|\n",
      "|     13098|        18|     &#| 0.4911011561534254|    &#|[0.00475383969023...|\n",
      "|     13098|        18|  class|0.28891320463243075| class|[-0.0675905793905...|\n",
      "|     13248|        18|   code| 2.3665256806276354|  code|[0.08397469669580...|\n",
      "|     13248|        18|     参数| 0.6498203796828275|    参数|[0.19957543909549...|\n",
      "|     13248|        18|     系统| 0.7199439290823474|    系统|[0.08635176718235...|\n",
      "|     13248|        18|     .a| 0.6408325228873414|    .a|[0.07951622456312...|\n",
      "|     13248|        18| Python| 1.2271370668933776|Python|[-0.1032132878899...|\n",
      "|     13401|        18|   item| 0.1907214431716628|  item|[0.09132987260818...|\n",
      "|     13401|        18|    字符串|0.16086650318453152|   字符串|[0.17387962341308...|\n",
      "|     13401|        18| update|0.18091837445730974|update|[-0.0967768207192...|\n",
      "+----------+----------+-------+-------------------+------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 合并文章关键词与词向量\n",
    "_keywords_vector = keyword_weight.join(vectors, vectors.word==keyword_weight.keyword, 'inner')\n",
    "_keywords_vector.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-------+--------------------+\n",
      "|article_id|channel_id|keyword|     weightingVector|\n",
      "+----------+----------+-------+--------------------+\n",
      "|     13098|        18|     __|[-0.2137282650596...|\n",
      "|     13098|        18|     属性|[0.03671498778010...|\n",
      "|     13098|        18|   code|[0.08003946631349...|\n",
      "|     13098|        18|    def|[0.00332470276693...|\n",
      "|     13098|        18|     定义|[0.01301930851531...|\n",
      "|     13098|        18|     方法|[0.01254290689293...|\n",
      "|     13098|        18|    div|[0.01673701228950...|\n",
      "|     13098|        18|    str|[0.02358302775608...|\n",
      "|     13098|        18|     pa|[0.04982733079938...|\n",
      "|     13098|        18|     函数|[0.02275340318306...|\n",
      "|     13098|        18|     &#|[0.00233461616804...|\n",
      "|     13098|        18|  class|[-0.0195278108946...|\n",
      "|     13248|        18|   code|[0.19872827625353...|\n",
      "|     13248|        18|     参数|[0.12968818760840...|\n",
      "|     13248|        18|     系统|[0.06216843054846...|\n",
      "|     13248|        18|     .a|[0.05095658279726...|\n",
      "|     13248|        18| Python|[-0.1266568513657...|\n",
      "|     13401|        18|   item|[0.01741856510851...|\n",
      "|     13401|        18|    字符串|[0.02797140699350...|\n",
      "|     13401|        18| update|[-0.0175087050896...|\n",
      "+----------+----------+-------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#4、计算得到文章每个词的向量：这里用词的权重 * 词的向量 = weights x vector=new_vector\n",
    "def compute_vector(row):\n",
    "    return row.article_id, row.channel_id, row.keyword, row.weight * row.vector\n",
    "\n",
    "articleKeywordVectors = _keywords_vector.rdd.map(compute_vector).toDF([\"article_id\", \"channel_id\", \"keyword\", \"weightingVector\"])\n",
    "articleKeywordVectors.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+--------------------+\n",
      "|article_id|channel_id|             vectors|\n",
      "+----------+----------+--------------------+\n",
      "|     13098|        18|[[0.0033247027669...|\n",
      "|     13248|        18|[[0.0621684305484...|\n",
      "|     13401|        18|[[0.0137473878736...|\n",
      "|     13723|        18|[[0.0387393960900...|\n",
      "|     14719|        18|[[0.0215666713566...|\n",
      "|     14846|        18|[[0.0224016568366...|\n",
      "|     15173|        18|[[0.0327754618004...|\n",
      "|     15194|        18|[[0.0550583468621...|\n",
      "|     15237|        18|[[-0.027733238125...|\n",
      "|     15322|        18|[[0.0096505359203...|\n",
      "+----------+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#5、计算得到文章的平均词向量 即：文章的向量\n",
    "articleKeywordVectors.registerTempTable('temptable')\n",
    "articleKeywordVectors = w2v.spark.sql(\"select article_id, min(channel_id) channel_id, collect_set(weightingVector) vectors from temptable group by article_id\")\n",
    "articleKeywordVectors.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+--------------------+\n",
      "|article_id|channel_id|             vectors|\n",
      "+----------+----------+--------------------+\n",
      "|     13098|        18|[0.00230172387587...|\n",
      "|     13248|        18|[0.06297692516839...|\n",
      "|     13401|        18|[0.00371605895856...|\n",
      "|     13723|        18|[0.04396727569275...|\n",
      "|     14719|        18|[0.03041110678417...|\n",
      "|     14846|        18|[0.00779903058473...|\n",
      "|     15173|        18|[0.03051927367831...|\n",
      "|     15194|        18|[0.01959675463890...|\n",
      "|     15237|        18|[0.00435888468677...|\n",
      "|     15322|        18|[0.01299606486049...|\n",
      "+----------+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 求平均值 得到 文章向量\n",
    "def compute_avg_vectors(row):\n",
    "    x = 0\n",
    "    for i in row.vectors:\n",
    "        x += i\n",
    "    \n",
    "    # 求平均值\n",
    "    return row.article_id, row.channel_id, x / len(row.vectors)\n",
    "\n",
    "article_vector = articleKeywordVectors.rdd.map(compute_avg_vectors).toDF(['article_id', 'channel_id', 'vectors'])\n",
    "article_vector.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[article_id: bigint, channel_id: bigint, vectors: vector]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看 article_vector 的变量类型：vectors字段为 vector\n",
    "article_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+--------------------+\n",
      "|article_id|channel_id|       articlevector|\n",
      "+----------+----------+--------------------+\n",
      "|     13098|        18|[0.00230172387587...|\n",
      "|     13248|        18|[0.06297692516839...|\n",
      "|     13401|        18|[0.00371605895856...|\n",
      "|     13723|        18|[0.04396727569275...|\n",
      "|     14719|        18|[0.03041110678417...|\n",
      "|     14846|        18|[0.00779903058473...|\n",
      "|     15173|        18|[0.03051927367831...|\n",
      "|     15194|        18|[0.01959675463890...|\n",
      "|     15237|        18|[0.00435888468677...|\n",
      "|     15322|        18|[0.01299606486049...|\n",
      "+----------+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 对计算出的“vectors”列进行处理，该列为Vector类型，不能直接存入HIVE，HIVE不支持Vector数据类型\n",
    "# vectors字段名 代表 Vector数据类型，而 articlevector字段名 代表 数组数据类型。\n",
    "def toArray(row):\n",
    "    return row.article_id, row.channel_id, [float(i) for i in row.vectors.toArray()]\n",
    "\n",
    "article_vector = article_vector.rdd.map(toArray).toDF(['article_id', 'channel_id', 'articlevector'])\n",
    "article_vector.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[article_id: bigint, channel_id: bigint, articlevector: array<double>]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看 article_vector 的变量类型：vectors字段为 array<double>\n",
    "article_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# 最终计算出 18号Python频道 的所有文章向量，保存到固定的表当中\n",
    "# vectors字段名 代表 Vector数据类型，而 articlevector字段名 代表 数组数据类型。\n",
    "\n",
    "# 创建文章向量表（DataFrame的字段名 和 表的字段名 不相同，也可以新增数据）\n",
    "CREATE TABLE article_vector(\n",
    "article_id BIGINT comment \"article_id\",\n",
    "channel_id INT comment \"channel_id\",\n",
    "articlevector ARRAY<DOUBLE> comment \"articlevector\"); \n",
    "\n",
    "# 保存数据到HIVE\n",
    "# article_vector.write.insertInto(\"article_vector\")\n",
    "\n",
    "# 上传计算好的历史文章向量\n",
    "./hadoop dfs -put  /root/backup/backup/article.db/article_vector/ /user/hive/warehouse/article.db/\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# 2.7.5、文章相似度计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+\n",
      "|article_id|       articleVector|\n",
      "+----------+--------------------+\n",
      "|     13098|[0.10339950907039...|\n",
      "|     13248|[0.84907054580879...|\n",
      "|     13401|[0.06157120217893...|\n",
      "|     13723|[0.20708073724961...|\n",
      "|     14719|[-0.0405607722081...|\n",
      "|     14846|[0.17945355257543...|\n",
      "|     15173|[-0.2399774663757...|\n",
      "|     15194|[0.08605245220126...|\n",
      "|     15237|[0.02019666206037...|\n",
      "|     15322|[0.11985676790665...|\n",
      "+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1、拿到18号-Python频道的所有文章数据，10篇测试\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "# 选取部分数据做测试\n",
    "article_vector = w2v.spark.sql(\"select article_id, articlevector from article_vector where channel_id=18 limit 10\")\n",
    "train = article_vector.select(['article_id', 'articleVector'])\n",
    "train.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+\n",
      "|article_id|             vectors|\n",
      "+----------+--------------------+\n",
      "|     13098|[0.10339950907039...|\n",
      "|     13248|[0.84907054580879...|\n",
      "|     13401|[0.06157120217893...|\n",
      "|     13723|[0.20708073724961...|\n",
      "|     14719|[-0.0405607722081...|\n",
      "|     14846|[0.17945355257543...|\n",
      "|     15173|[-0.2399774663757...|\n",
      "|     15194|[0.08605245220126...|\n",
      "|     15237|[0.02019666206037...|\n",
      "|     15322|[0.11985676790665...|\n",
      "+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def toVector(row):\n",
    "    return row.article_id, Vectors.dense(row.articleVector)\n",
    "\n",
    "train = train.rdd.map(toVector).toDF(['article_id', 'vectors'])\n",
    "train.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[article_id: bigint, vectors: vector]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看 train 的变量类型：vectors字段为 vector\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算相似的文章（2.1.0版本还没有这两个类，所以后续没有做了）\n",
    "from pyspark.ml.feature import MinHashLSH, BucketedRandomProjectionLSH\n",
    "\n",
    "brp = BucketedRandomProjectionLSH(inputCol='vectors', outputCol='hashes', numHashTables=4.0, bucketLength=10.0)\n",
    "model = brp.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第一个参数为：新增文章； 第二个参数为：历史库文章； 第三个参数为：欧氏距离名称（并不是指定使用欧氏距离公式，默认欧氏距离公式）\n",
    "similar = model.approxSimilarityJoin(train, train, 2.0, distCol='EuclideanDistance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar.sort(['EuclideanDistance']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 因暂时无法计算，所以不能确定是什么数据类型\n",
    "similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将 相似文章 保存到 HBase 中：\n",
    "# 创建HBASE的article_similar表：create 'article_similar', 'similar'\n",
    "def save_hbase(partitions):\n",
    "    import happybase\n",
    "    pool = happybase.ConnectionPool(size=3, host='hadoop-master')\n",
    "    \n",
    "    with pool.connection() as conn:\n",
    "        article_similar = conn.table('article_similar')\n",
    "        for row in partitions:\n",
    "            if row.datasetA.article_id == row.datasetB.article_id:\n",
    "                pass\n",
    "            else:\n",
    "                article_similar.put(str(row.datasetA.article_id).encode(),\n",
    "                                   {'similar:{}'.format(row.datasetB.article_id).encode(): b'%0.4f' % (row.EuclideanDistance)})\n",
    "\n",
    "similar.foreachPartition(save_hbase)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
